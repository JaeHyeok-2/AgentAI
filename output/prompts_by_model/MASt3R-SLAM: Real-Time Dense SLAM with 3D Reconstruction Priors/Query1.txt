You are an AI scientist.

A user has asked the following question:
"While moving around with a handheld camera, it built a full 3D map of the room in real time. How can it reconstruct and localize everything so quickly?"

Based on the following recommended models, explain:

1. What task the user is trying to perform.
2. How the model(s) would work in a CNAPS AI-like workflow (input → model → output).
3. List relevant papers and tools (with GitHub or ArXiv links) that support your answer.

Use **only the provided models and papers**. Do not refer to outside sources.

### Recommended AI Models:
Model: MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors
Paper: https://arxiv.org/pdf/2412.12392
GitHub: https://github.com/rmurai0610/MASt3R-SLAM

### Related Papers:
Model: A survey on real-time 3D scene reconstruction with SLAM methods in embedded systems
Paper: http://arxiv.org/pdf/2309.05349v1.pdf
Summary: The 3D reconstruction of simultaneous localization and mapping (SLAM) is an important topic in the field for transport systems such as drones, service robots and mobile AR/VR devices. Compared to a point cloud representation, the 3D reconstruction based on meshes and voxels is particularly useful for high-level functions, like obstacle avoidance or interaction with the physical environment. This article reviews the implementation of a visual-based 3D scene reconstruction pipeline on resource-constrained hardware platforms. Real-time performances, memory management and low power consumption are critical for embedded systems. A conventional SLAM pipeline from sensors to 3D reconstruction is described, including the potential use of deep learning. The implementation of advanced functions with limited resources is detailed. Recent systems propose the embedded implementation of 3D reconstruction methods with different granularities. The trade-off between required accuracy and resource consumption for real-time localization and reconstruction is one of the open research questions identified and discussed in this paper.

Model: Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile Mapping
Paper: http://arxiv.org/pdf/2304.14301v2.pdf
Summary: This work represents a large step into modern ways of fast 3D reconstruction based on RGB camera images. Utilizing a Microsoft HoloLens 2 as a multisensor platform that includes an RGB camera and an inertial measurement unit for SLAM-based camera-pose determination, we train a Neural Radiance Field (NeRF) as a neural scene representation in real-time with the acquired data from the HoloLens. The HoloLens is connected via Wifi to a high-performance PC that is responsible for the training and 3D reconstruction. After the data stream ends, the training is stopped and the 3D reconstruction is initiated, which extracts a point cloud of the scene. With our specialized inference algorithm, five million scene points can be extracted within 1 second. In addition, the point cloud also includes radiometry per point. Our method of 3D reconstruction outperforms grid point sampling with NeRFs by multiple orders of magnitude and can be regarded as a complete real-time 3D reconstruction method in a mobile mapping setup.

Model: 3D reconstruction using Structure for Motion
Paper: http://arxiv.org/pdf/2306.06360v1.pdf
GitHub: https://github.com/kshitijkarnawat/structure-from-motion
Summary: We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.


Answer:
