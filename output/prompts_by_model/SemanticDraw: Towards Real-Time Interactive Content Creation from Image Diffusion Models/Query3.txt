You are an AI scientist.

A user has asked the following question:
"Just by dragging and labeling regions—like “sky,” “grass,” or “building”—I watched a full image come to life on the canvas. What kind of model allows that level of interactive generation?"

Based on the following recommended models, explain:

1. What task the user is trying to perform.
2. How the model(s) would work in a CNAPS AI-like workflow (input → model → output).
3. List relevant papers and tools (with GitHub or ArXiv links) that support your answer.

Use **only the provided models and papers**. Do not refer to outside sources.

### Recommended AI Models:
Model: SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models
Paper: https://arxiv.org/pdf/2403.09055
GitHub: https://github.com/ironjr/semantic-draw

### Related Papers:
Model: WorldSmith: Iterative and Expressive Prompting for World Building with a Generative AI
Paper: http://arxiv.org/pdf/2308.13355v1.pdf
Summary: Crafting a rich and unique environment is crucial for fictional world-building, but can be difficult to achieve since illustrating a world from scratch requires time and significant skill. We investigate the use of recent multi-modal image generation systems to enable users iteratively visualize and modify elements of their fictional world using a combination of text input, sketching, and region-based filling. WorldSmith enables novice world builders to quickly visualize a fictional world with layered edits and hierarchical compositions. Through a formative study (4 participants) and first-use study (13 participants) we demonstrate that WorldSmith offers more expressive interactions with prompt-based models. With this work, we explore how creatives can be empowered to leverage prompt-based generative AI as a tool in their creative process, beyond current "click-once" prompting UI paradigms.

Model: Visions Of Destruction: Exploring Human Impact on Nature by Navigating the Latent Space of a Diffusion Model via Gaze
Paper: http://arxiv.org/pdf/2401.06361v1.pdf
Summary: This paper discusses the artwork "Visions of Destruction", with a primary conceptual focus on the Anthropocene, which is communicated through audience interaction and generative AI as artistic research methods. Gaze-based interaction transitions the audience from mere observers to agents of landscape transformation, fostering a profound, on-the-edge engagement with pressing issues such as climate change and planetary destruction. The paper looks into early references of interactive art history that deploy eye-tracking as a method for audience interaction, and presents recent AI-aided artworks that demonstrate interactive latent space navigation.

Model: MUSES: 3D-Controllable Image Generation via Multi-Modal Agent Collaboration
Paper: http://arxiv.org/pdf/2408.10605v5.pdf
GitHub: https://github.com/DINGYANB/MUSES
Summary: Despite recent advancements in text-to-image generation, most existing methods struggle to create images with multiple objects and complex spatial relationships in the 3D world. To tackle this limitation, we introduce a generic AI system, namely MUSES, for 3D-controllable image generation from user queries. Specifically, our MUSES addresses this challenging task by developing a progressive workflow with three key components, including (1) Layout Manager for 2D-to-3D layout lifting, (2) Model Engineer for 3D object acquisition and calibration, (3) Image Artist for 3D-to-2D image rendering. By mimicking the collaboration of human professionals, this multi-modal agent pipeline facilitates the effective and automatic creation of images with 3D-controllable objects, through an explainable integration of top-down planning and bottom-up generation. Additionally, we find that existing benchmarks lack detailed descriptions of complex 3D spatial relationships of multiple objects. To fill this gap, we further construct a new benchmark of T2I-3DisBench (3D image scene), which describes diverse 3D image scenes with 50 detailed prompts. Extensive experiments show the state-of-the-art performance of MUSES on both T2I-CompBench and T2I-3DisBench, outperforming recent strong competitors such as DALL-E 3 and Stable Diffusion 3. These results demonstrate a significant step of MUSES forward in bridging natural language, 2D image generation, and 3D world. Our codes are available at the following link: https://github.com/DINGYANB/MUSES.


Answer:
