You are an AI scientist.

A user has asked the following question:
"I used it on a street photo, and it could tell how far the cars and people were—without any special sensors. How is that even possible?"

Based on the following recommended models, explain:

1. What task the user is trying to perform.
2. How the model(s) would work in a CNAPS AI-like workflow (input → model → output).
3. List relevant papers and tools (with GitHub or ArXiv links) that support your answer.

Use **only the provided models and papers**. Do not refer to outside sources.

### Recommended AI Models:
Model: UniK3D: Universal Camera Monocular 3D Estimation
Paper: https://arxiv.org/pdf/2503.16591
GitHub: https://github.com/lpiccinelli-eth/UniK3D

### Related Papers:
Model: CalTag: Robust calibration of mmWave Radar and LiDAR using backscatter tags
Paper: http://arxiv.org/pdf/2408.16867v2.pdf
Summary: The rise of automation in robotics necessitates the use of high-quality perception systems, often through the use of multiple sensors. A crucial aspect of a successfully deployed multi-sensor system is the calibration with a known object typically named fiducial. In this work, we propose a novel fiducial system for millimeter wave radars, termed as CalTag. CalTag addresses the limitations of traditional corner reflector-based calibration methods in extremely cluttered environments. CalTag leverages millimeter wave backscatter technology to achieve more reliable calibration than corner reflectors, enhancing the overall performance of multi-sensor perception systems. We compare the performance in several real-world environments and show the improvement achieved by using CalTag as the radar fiducial over a corner reflector.

Model: Sensor Equivariance by LiDAR Projection Images
Paper: http://arxiv.org/pdf/2305.00221v1.pdf
Summary: In this work, we propose an extension of conventional image data by an additional channel in which the associated projection properties are encoded. This addresses the issue of sensor-dependent object representation in projection-based sensors, such as LiDAR, which can lead to distorted physical and geometric properties due to variations in sensor resolution and field of view. To that end, we propose an architecture for processing this data in an instance segmentation framework. We focus specifically on LiDAR as a key sensor modality for machine vision tasks and highly automated driving (HAD). Through an experimental setup in a controlled synthetic environment, we identify a bias on sensor resolution and field of view and demonstrate that our proposed method can reduce said bias for the task of LiDAR instance segmentation. Furthermore, we define our method such that it can be applied to other projection-based sensors, such as cameras. To promote transparency, we make our code and dataset publicly available. This method shows the potential to improve performance and robustness in various machine vision tasks that utilize projection-based sensors.

Model: RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving
Paper: http://arxiv.org/pdf/2401.07322v2.pdf
GitHub: https://github.com/hasibzunair/rsud20k
Summary: Road scene understanding is crucial in autonomous driving, enabling machines to perceive the visual environment. However, recent object detectors tailored for learning on datasets collected from certain geographical locations struggle to generalize across different locations. In this paper, we present RSUD20K, a new dataset for road scene understanding, comprised of over 20K high-resolution images from the driving perspective on Bangladesh roads, and includes 130K bounding box annotations for 13 objects. This challenging dataset encompasses diverse road scenes, narrow streets and highways, featuring objects from different viewpoints and scenes from crowded environments with densely cluttered objects and various weather conditions. Our work significantly improves upon previous efforts, providing detailed annotations and increased object complexity. We thoroughly examine the dataset, benchmarking various state-of-the-art object detectors and exploring large vision models as image annotators.


Answer:
