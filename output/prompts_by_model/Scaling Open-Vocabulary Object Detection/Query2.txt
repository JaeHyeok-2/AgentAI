You are an AI scientist.

A user has asked the following question:
"I tried it on a messy room photo and told it to find “wireless earbuds,” and it picked them out perfectly. How can it understand open-ended text like that?"

Based on the following recommended models, explain:

1. What task the user is trying to perform.
2. How the model(s) would work in a CNAPS AI-like workflow (input → model → output).
3. List relevant papers and tools (with GitHub or ArXiv links) that support your answer.

Use **only the provided models and papers**. Do not refer to outside sources.

### Recommended AI Models:
Model: BGE: One-Stop Retrieval Toolkit For Search and RAG
Paper: https://arxiv.org/pdf/2412.14475
GitHub: https://github.com/FlagOpen/FlagEmbedding/tree/master?tab=readme-ov-file

### Related Papers:
Model: A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld
Paper: http://arxiv.org/pdf/2302.05244v5.pdf
Summary: Building open-ended agents that can autonomously discover a diversity of behaviours is one of the long-standing goals of artificial intelligence. This challenge can be studied in the framework of autotelic RL agents, i.e. agents that learn by selecting and pursuing their own goals, self-organizing a learning curriculum. Recent work identified language as a key dimension of autotelic learning, in particular because it enables abstract goal sampling and guidance from social peers for hindsight relabelling. Within this perspective, we study the following open scientific questions: What is the impact of hindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can the agent learn from very rare language goal examples in its experience replay? How can multiple forms of exploration be combined, and take advantage of easier goals as stepping stones to reach harder ones? To address these questions, we use ScienceWorld, a textual environment with rich abstract and combinatorial physics. We show the importance of selectivity from the social peer's feedback; that experience replay needs to over-sample examples of rare goals; and that following self-generated goal sequences where the agent's competence is intermediate leads to significant improvements in final performance.

Model: Automatic Creative Selection with Cross-Modal Matching
Paper: http://arxiv.org/pdf/2405.00029v1.pdf
Summary: Application developers advertise their Apps by creating product pages with App images, and bidding on search terms. It is then crucial for App images to be highly relevant with the search terms. Solutions to this problem require an image-text matching model to predict the quality of the match between the chosen image and the search terms. In this work, we present a novel approach to matching an App image to search terms based on fine-tuning a pre-trained LXMERT model. We show that compared to the CLIP model and a baseline using a Transformer model for search terms, and a ResNet model for images, we significantly improve the matching accuracy. We evaluate our approach using two sets of labels: advertiser associated (image, search term) pairs for a given application, and human ratings for the relevance between (image, search term) pairs. Our approach achieves 0.96 AUC score for advertiser associated ground truth, outperforming the transformer+ResNet baseline and the fine-tuned CLIP model by 8% and 14%. For human labeled ground truth, our approach achieves 0.95 AUC score, outperforming the transformer+ResNet baseline and the fine-tuned CLIP model by 16% and 17%.

Model: Sequential annotations for naturally-occurring HRI: first insights
Paper: http://arxiv.org/pdf/2308.15097v1.pdf
Summary: We explain the methodology we developed for improving the interactions accomplished by an embedded conversational agent, drawing from Conversation Analytic sequential and multimodal analysis. The use case is a Pepper robot that is expected to inform and orient users in a library. In order to propose and learn better interactive schema, we are creating a corpus of naturally-occurring interactions that will be made available to the community. To do so, we propose an annotation practice based on some theoretical underpinnings about the use of language and multimodal resources in human-robot interaction. CCS CONCEPTS $\bullet$ Computing methodologies $\rightarrow$ Discourse, dialogue and pragmatics; $\bullet$ Human-centered computing $\rightarrow$ Text input; HCI theory, concepts and models; Field studies.


Answer:
