You are an AI scientist.

A user has asked the following question:
"Even indoor scenes with clutter and odd lighting turned into clean 3D geometry. What allows it to stay accurate in such messy conditions?"

Based on the following recommended models, explain:

1. What task the user is trying to perform.
2. How the model(s) would work in a CNAPS AI-like workflow (input → model → output).
3. List relevant papers and tools (with GitHub or ArXiv links) that support your answer.

Use **only the provided models and papers**. Do not refer to outside sources.

### Recommended AI Models:
Model: UniK3D: Universal Camera Monocular 3D Estimation
Paper: https://arxiv.org/pdf/2503.16591
GitHub: https://github.com/lpiccinelli-eth/UniK3D

### Related Papers:
Model: On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks
Paper: http://arxiv.org/pdf/2303.14840v1.pdf
GitHub: https://github.com/junggy/hammer-dataset
Summary: Learning-based methods to solve dense 3D vision problems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are intricate to measure with existing hardware. Training on inaccurate or corrupt data induces model bias and hampers generalisation capabilities. These effects remain unnoticed if the sensor measurement is considered as ground truth during the evaluation. This paper investigates the effect of sensor errors for the dense 3D vision tasks of depth estimation and reconstruction. We rigorously show the significant impact of sensor characteristics on the learned predictions and notice generalisation issues arising from various technologies in everyday household environments. For evaluation, we introduce a carefully designed dataset\footnote{dataset available at https://github.com/Junggy/HAMMER-dataset} comprising measurements from commodity sensors, namely D-ToF, I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the considerable sensor noise impact and paves the way to improved dense vision estimates and targeted data fusion.


Answer:
