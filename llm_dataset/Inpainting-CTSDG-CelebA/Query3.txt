You are an AI scientist.

A user has asked the following question:
"Even after removing a large portion of the hair and background in a selfie, the AI completed it with smooth textures and sharp contours. What kind of dual modeling lets it keep both structure and texture aligned?"

Based on the following recommended models, explain:

1. What task the user is trying to perform.
2. How the model(s) would work in a CNAPS AI-like workflow (input → model → output).
3. List relevant papers and tools (with GitHub or ArXiv links) that support your answer.

Use **only the provided models and papers**. Do not refer to outside sources.

### Recommended AI Models:
Model: Inpainting-CTSDG-CelebA
Paper: https://arxiv.org/pdf/2108.09760
GitHub: https://github.com/xiefan-guo/ctsdg

### Related Papers:
Model: Doppelgangers: Learning to Disambiguate Images of Similar Structures
Paper: http://arxiv.org/pdf/2309.02420v1.pdf
GitHub: https://github.com/RuojinCai/Doppelgangers
Summary: We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building). Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results. We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs. To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels. We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues. Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions. See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/.

Model: SurfaceAug: Closing the Gap in Multimodal Ground Truth Sampling
Paper: http://arxiv.org/pdf/2312.03808v1.pdf
Summary: Despite recent advances in both model architectures and data augmentation, multimodal object detectors still barely outperform their LiDAR-only counterparts. This shortcoming has been attributed to a lack of sufficiently powerful multimodal data augmentation. To address this, we present SurfaceAug, a novel ground truth sampling algorithm. SurfaceAug pastes objects by resampling both images and point clouds, enabling object-level transformations in both modalities. We evaluate our algorithm by training a multimodal detector on KITTI and compare its performance to previous works. We show experimentally that SurfaceAug outperforms existing methods on car detection tasks and establishes a new state of the art for multimodal ground truth sampling.

Model: Point Cloud Data Simulation and Modelling with Aize Workspace
Paper: http://arxiv.org/pdf/2301.07947v1.pdf
Summary: This work takes a look at data models often used in digital twins and presents preliminary results specifically from surface reconstruction and semantic segmentation models trained using simulated data. This work is expected to serve as a ground work for future endeavours in data contextualisation inside a digital twin.


Answer:
