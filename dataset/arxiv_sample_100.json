[
  {
    "Model Unique Name":"Bunting and the ghost runner: a causal inference approach",
    "Category":"Multi-modal",
    "Detailed Category":"VQA",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.06587v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.06587,
    "Summary":"In this paper, we investigate the effectiveness of the home team bunting in extra innings of Major League Baseball games when the game is tied in the bottom of the inning. Using methods rooted in causal inference, we show that teams choose not to bunt when the statistical evidence clearly suggests it is in their advantage to do so. The reluctance to bunt in this situation is likely tied to the general decline in bunting, which is considered less effective than swinging away in most situations. In the 2021-22 seasons, the home team's first batter bunted in only 21% of tied extra innings. When they bunted, the home team went on to win the game in 74% of those innings, compared to 57% when they swung away (i.e. did not bunt), for an odds ratio for winning comparing bunting to swinging away of 2.13 (95% C.I.: 1.13, 4.30). However, the bunters had more experience bunting and the nonbunters were stronger hitters, thus the odds ratio above is a confounded estimate of the effect of bunting. Using inverse probability weighting to adjust for confounding, we estimate an odds ratio of 1.86 (95% C.I. 1.07, 3.27). This means that a typical team would expect to win 2 more games each season if they bunted in such situations which is worth millions in player salary."
  },
  {
    "Model Unique Name":"Robust Multi-Modal Image Stitching for Improved Scene Understanding",
    "Category":"Multi-modal",
    "Detailed Category":"Multi-modal",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2312.17010v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2312.1701,
    "Summary":"Multi-modal image stitching can be a difficult feat. That's why, in this paper, we've devised a unique and comprehensive image-stitching pipeline that taps into OpenCV's stitching module. Our approach integrates feature-based matching, transformation estimation, and blending techniques to bring about panoramic views that are of top-tier quality - irrespective of lighting, scale or orientation differences between images. We've put our pipeline to the test with a varied dataset and found that it's very effective in enhancing scene understanding and finding real-world applications."
  },
  {
    "Model Unique Name":"PointCFormer: a Relation-based Progressive Feature Extraction Network for Point Cloud Completion",
    "Category":"3D Vision",
    "Detailed Category":"Point Cloud Completion",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2412.08421v2.pdf",
    "GitHub":"https:\/\/github.com\/zyyyyy0926\/pointcformer_plus_pytorch",
    "HuggingFace":null,
    "arxiv_id":2412.08421,
    "Summary":"Point cloud completion aims to reconstruct the complete 3D shape from incomplete point clouds, and it is crucial for tasks such as 3D object detection and segmentation. Despite the continuous advances in point cloud analysis techniques, feature extraction methods are still confronted with apparent limitations. The sparse sampling of point clouds, used as inputs in most methods, often results in a certain loss of global structure information. Meanwhile, traditional local feature extraction methods usually struggle to capture the intricate geometric details. To overcome these drawbacks, we introduce PointCFormer, a transformer framework optimized for robust global retention and precise local detail capture in point cloud completion. This framework embraces several key advantages. First, we propose a relation-based local feature extraction method to perceive local delicate geometry characteristics. This approach establishes a fine-grained relationship metric between the target point and its k-nearest neighbors, quantifying each neighboring point's contribution to the target point's local features. Secondly, we introduce a progressive feature extractor that integrates our local feature perception method with self-attention. Starting with a denser sampling of points as input, it iteratively queries long-distance global dependencies and local neighborhood relationships. This extractor maintains enhanced global structure and refined local details, without generating substantial computational overhead. Additionally, we develop a correction module after generating point proxies in the latent space to reintroduce denser information from the input points, enhancing the representation capability of the point proxies. PointCFormer demonstrates state-of-the-art performance on several widely used benchmarks. Our code is available at https:\/\/github.com\/Zyyyyy0926\/PointCFormer_Plus_Pytorch."
  },
  {
    "Model Unique Name":"Moments and saddles of heavy CFT correlators",
    "Category":"Transformer",
    "Detailed Category":"Transformer",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2501.00092v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2501.00092,
    "Summary":"We study the operator product expansion (OPE) of identical scalars in a conformal four-point correlator as a Stieltjes moment problem, and use Riemann-Liouville type fractional differential operators to generate classical moments from the correlation function. We use crossing symmetry to derive leading and subleading relations between moments in $\\Delta$ and $J_2 \\equiv \\ell(\\ell+d-2)$ in the ``heavy\" limit of large external scaling dimension, and combine them with constraints from unitarity to derive two-sided bounds on moment sequences in $\\Delta$ and the covariance between $\\Delta$ and $J_2$. The moment sequences which saturate these bounds produce ``saddle point\" solutions to the crossing equations which we identify as particular limits of correlators in a generalized free field (GFF) theory. This motivates us to study perturbations of heavy GFF four-point correlators by way of saddle point analysis, and we show that saddles in the OPE arise from contributions of fixed-length operator families encoded by a decomposition into higher-spin conformal blocks. To apply our techniques, we consider holographic correlators of four identical single scalar fields perturbed by a bulk interaction, and use their first few moments to derive Gaussian weight-interpolating functions that predict the OPE coefficients of interacting double-twist operators in the heavy limit."
  },
  {
    "Model Unique Name":"FHGE: A Fast Heterogeneous Graph Embedding with Ad-hoc Meta-paths",
    "Category":"Graph Neural Network",
    "Detailed Category":"Graph Neural Network",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2502.16281v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2502.16281,
    "Summary":"Graph neural networks (GNNs) have emerged as the state of the art for a variety of graph-related tasks and have been widely used in Heterogeneous Graphs (HetGs), where meta-paths help encode specific semantics between various node types. Despite the revolutionary representation capabilities of existing heterogeneous GNNs (HGNNs) due to their focus on improving the effectiveness of heterogeneity capturing, the huge training costs hinder their practical deployment in real-world scenarios that frequently require handling ad-hoc queries with user-defined meta-paths. To address this, we propose FHGE, a Fast Heterogeneous Graph Embedding designed for efficient, retraining-free generation of meta-path-guided graph embeddings. The key design of the proposed framework is two-fold: segmentation and reconstruction modules. It employs Meta-Path Units (MPUs) to segment the graph into local and global components, enabling swift integration of node embeddings from relevant MPUs during reconstruction and allowing quick adaptation to specific meta-paths. In addition, a dual attention mechanism is applied to enhance semantics capturing. Extensive experiments across diverse datasets demonstrate the effectiveness and efficiency of FHGE in generating meta-path-guided graph embeddings and downstream tasks, such as link prediction and node classification, highlighting its significant advantages for real-time graph analysis in ad-hoc queries."
  },
  {
    "Model Unique Name":"GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism",
    "Category":"Graph Neural Network",
    "Detailed Category":"Graph Neural Network",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.06911v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.06911,
    "Summary":"Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks. However, bridging the modality gap between graph structures and text remains a significant challenge. Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs. In this work, we propose a novel graph-guided self-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs. Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million."
  },
  {
    "Model Unique Name":"Semi-supervised classification of dental conditions in panoramic radiographs using large language model and instance segmentation: A real-world dataset evaluation",
    "Category":"Computer Vision",
    "Detailed Category":"Instance Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2406.17915v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2406.17915,
    "Summary":"Dental panoramic radiographs offer vast diagnostic opportunities, but training supervised deep learning networks for automatic analysis of those radiology images is hampered by a shortage of labeled data. Here, a different perspective on this problem is introduced. A semi-supervised learning framework is proposed to classify thirteen dental conditions on panoramic radiographs, with a particular emphasis on teeth. Large language models were explored to annotate the most common dental conditions based on dental reports. Additionally, a masked autoencoder was employed to pre-train the classification neural network, and a Vision Transformer was used to leverage the unlabeled data. The analyses were validated using two of the most extensive datasets in the literature, comprising 8,795 panoramic radiographs and 8,029 paired reports and images. Encouragingly, the results consistently met or surpassed the baseline metrics for the Matthews correlation coefficient. A comparison of the proposed solution with human practitioners, supported by statistical analysis, highlighted its effectiveness and performance limitations; based on the degree of agreement among specialists, the solution demonstrated an accuracy level comparable to that of a junior specialist."
  },
  {
    "Model Unique Name":"PROFIT: A Specialized Optimizer for Deep Fine Tuning",
    "Category":"Computer Vision",
    "Detailed Category":"Image Classification",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2412.01930v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2412.0193,
    "Summary":"Fine-tuning pre-trained models has become invaluable in computer vision and robotics. Recent fine-tuning approaches focus on improving efficiency rather than accuracy by using a mixture of smaller learning rates or frozen backbones. To return the spotlight to model accuracy, we present PROFIT (Proximally Restricted Optimizer For Iterative Training), one of the first optimizers specifically designed for incrementally fine-tuning converged models on new tasks or datasets. Unlike traditional optimizers such as SGD or Adam, which make minimal assumptions due to random initialization, PROFIT leverages the structure of a converged model to regularize the optimization process, leading to improved results. By employing a simple temporal gradient orthogonalization process, PROFIT outperforms traditional fine-tuning methods across various tasks: image classification, representation learning, and large-scale motion prediction. Moreover, PROFIT is encapsulated within the optimizer logic, making it easily integrated into any training pipeline with minimal engineering effort. A new class of fine-tuning optimizers like PROFIT can drive advancements as fine-tuning and incremental training become increasingly prevalent, reducing reliance on costly model training from scratch."
  },
  {
    "Model Unique Name":"Universal Graph Continual Learning",
    "Category":"Graph Neural Network",
    "Detailed Category":"Graph Neural Network",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2308.13982v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2308.13982,
    "Summary":"We address catastrophic forgetting issues in graph learning as incoming data transits from one to another graph distribution. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We propose a novel method that enables graph neural networks to excel in this universal setting. Our approach perseveres knowledge about past tasks through a rehearsal mechanism that maintains local and global structure consistency across the graphs. We benchmark our method against various continual learning baselines in real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks."
  },
  {
    "Model Unique Name":"Irrational rotations and 2-filling rays",
    "Category":"Transformer",
    "Detailed Category":"Transformer",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2403.00097v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2403.00097,
    "Summary":"We study a skew product transformation associated to an irrational rotation of the circle [0,1]\/~. This skew product keeps track of the number of times an orbit of the rotation lands in the two complementary intervals of {0,1\/2} in the circle. We show that under certain conditions on the continued fraction expansion of the irrational number defining the rotation, the skew product transformation has certain dense orbits. This is in spite of the presence of numerous non-dense orbits. We use this to construct laminations on infinite type surfaces with exotic properties. In particular, we show that for every infinite type surface with an isolated planar end, there is an infinite clique of 2-filling rays based at that end. These 2-filling rays are relevant to Bavard--Walker's loop graphs."
  },
  {
    "Model Unique Name":"SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis",
    "Category":"Computer Vision",
    "Detailed Category":"Object Detection",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2501.03836v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2501.03836,
    "Summary":"Brain tumors can lead to neurological dysfunction, cognitive and psychological changes, increased intracranial pressure, and seizures, posing significant risks to health. The You Only Look Once (YOLO) series has shown superior accuracy in medical imaging object detection. This paper presents a novel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The SCConv module optimizes convolutional efficiency by reducing spatial and channel redundancy, enhancing image feature learning. We examine the effects of different attention mechanisms with YOLOv9 for brain tumor detection using the Br35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate that SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our custom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art performance in brain tumor detection."
  },
  {
    "Model Unique Name":"Reinforcement Learning based Autonomous Multi-Rotor Landing on Moving Platforms",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2302.13192v3.pdf",
    "GitHub":"https:\/\/github.com\/robot-perception-group\/rl_multi_rotor_landing",
    "HuggingFace":null,
    "arxiv_id":2302.13192,
    "Summary":"Multi-rotor UAVs suffer from a restricted range and flight duration due to limited battery capacity. Autonomous landing on a 2D moving platform offers the possibility to replenish batteries and offload data, thus increasing the utility of the vehicle. Classical approaches rely on accurate, complex and difficult-to-derive models of the vehicle and the environment. Reinforcement learning (RL) provides an attractive alternative due to its ability to learn a suitable control policy exclusively from data during a training procedure. However, current methods require several hours to train, have limited success rates and depend on hyperparameters that need to be tuned by trial-and-error. We address all these issues in this work. First, we decompose the landing procedure into a sequence of simpler, but similar learning tasks. This is enabled by applying two instances of the same RL based controller trained for 1D motion for controlling the multi-rotor's movement in both the longitudinal and the lateral directions. Second, we introduce a powerful state space discretization technique that is based on i) kinematic modeling of the moving platform to derive information about the state space topology and ii) structuring the training as a sequential curriculum using transfer learning. Third, we leverage the kinematics model of the moving platform to also derive interpretable hyperparameters for the training process that ensure sufficient maneuverability of the multi-rotor vehicle. The training is performed using the tabular RL method Double Q-Learning. Through extensive simulations we show that the presented method significantly increases the rate of successful landings, while requiring less training time compared to other deep RL approaches. Finally, we deploy and demonstrate our algorithm on real hardware. For all evaluation scenarios we provide statistics on the agent's performance."
  },
  {
    "Model Unique Name":"Long-tail Detection with Effective Class-Margins",
    "Category":"Computer Vision",
    "Detailed Category":"Instance Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2301.09724v1.pdf",
    "GitHub":"https:\/\/github.com\/janghyuncho\/ecm-loss",
    "HuggingFace":null,
    "arxiv_id":2301.09724,
    "Summary":"Large-scale object detection and instance segmentation face a severe data imbalance. The finer-grained object classes become, the less frequent they appear in our datasets. However, at test-time, we expect a detector that performs well for all classes and not just the most frequent ones. In this paper, we provide a theoretical understanding of the long-trail detection problem. We show how the commonly used mean average precision evaluation metric on an unknown test set is bound by a margin-based binary classification error on a long-tailed object detection training set. We optimize margin-based binary classification error with a novel surrogate objective called \\textbf{Effective Class-Margin Loss} (ECM). The ECM loss is simple, theoretically well-motivated, and outperforms other heuristic counterparts on LVIS v1 benchmark over a wide range of architecture and detectors. Code is available at \\url{https:\/\/github.com\/janghyuncho\/ECM-Loss}."
  },
  {
    "Model Unique Name":"Voice-Enabled AI Agents can Perform Common Scams",
    "Category":"Multi-modal",
    "Detailed Category":"Multi-modal",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2410.15650v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2410.1565,
    "Summary":"Recent advances in multi-modal, highly capable LLMs have enabled voice-enabled AI agents. These agents are enabling new applications, such as voice-enabled autonomous customer service. However, with all AI capabilities, these new capabilities have the potential for dual use.   In this work, we show that voice-enabled AI agents can perform the actions necessary to perform common scams. To do so, we select a list of common scams collected by the government and construct voice-enabled agents with directions to perform these scams. We conduct experiments on our voice-enabled agents and show that they can indeed perform the actions necessary to autonomously perform such scams. Our results raise questions around the widespread deployment of voice-enabled AI agents."
  },
  {
    "Model Unique Name":"Message Passing Meets Graph Neural Networks: A New Paradigm for Massive MIMO Systems",
    "Category":"Graph Neural Network",
    "Detailed Category":"Graph Neural Network",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2302.06896v2.pdf",
    "GitHub":"https:\/\/github.com\/hehengtao\/amp_gnn",
    "HuggingFace":null,
    "arxiv_id":2302.06896,
    "Summary":"As one of the core technologies for 5G systems, massive multiple-input multiple-output (MIMO) introduces dramatic capacity improvements along with very high beamforming and spatial multiplexing gains. When developing efficient physical layer algorithms for massive MIMO systems, message passing is one promising candidate owing to the superior performance. However, as their computational complexity increases dramatically with the problem size, the state-of-the-art message passing algorithms cannot be directly applied to future 6G systems, where an exceedingly large number of antennas are expected to be deployed. To address this issue, we propose a model-driven deep learning (DL) framework, namely the AMP-GNN for massive MIMO transceiver design, by considering the low complexity of the AMP algorithm and adaptability of GNNs. Specifically, the structure of the AMP-GNN network is customized by unfolding the approximate message passing (AMP) algorithm and introducing a graph neural network (GNN) module into it. The permutation equivariance property of AMP-GNN is proved, which enables the AMP-GNN to learn more efficiently and to adapt to different numbers of users. We also reveal the underlying reason why GNNs improve the AMP algorithm from the perspective of expectation propagation, which motivates us to amalgamate various GNNs with different message passing algorithms. In the simulation, we take the massive MIMO detection to exemplify that the proposed AMP-GNN significantly improves the performance of the AMP detector, achieves comparable performance as the state-of-the-art DL-based MIMO detectors, and presents strong robustness to various mismatches."
  },
  {
    "Model Unique Name":"A Light-weight Transformer-based Self-supervised Matching Network for Heterogeneous Images",
    "Category":"Transformer",
    "Detailed Category":"Transformer",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.19311v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.19311,
    "Summary":"Matching visible and near-infrared (NIR) images remains a significant challenge in remote sensing image fusion. The nonlinear radiometric differences between heterogeneous remote sensing images make the image matching task even more difficult. Deep learning has gained substantial attention in computer vision tasks in recent years. However, many methods rely on supervised learning and necessitate large amounts of annotated data. Nevertheless, annotated data is frequently limited in the field of remote sensing image matching. To address this challenge, this paper proposes a novel keypoint descriptor approach that obtains robust feature descriptors via a self-supervised matching network. A light-weight transformer network, termed as LTFormer, is designed to generate deep-level feature descriptors. Furthermore, we implement an innovative triplet loss function, LT Loss, to enhance the matching performance further. Our approach outperforms conventional hand-crafted local feature descriptors and proves equally competitive compared to state-of-the-art deep learning-based methods, even amidst the shortage of annotated data."
  },
  {
    "Model Unique Name":"Fuse after Align: Improving Face-Voice Association Learning via Multimodal Encoder",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.09509v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.09509,
    "Summary":"Today, there have been many achievements in learning the association between voice and face. However, most previous work models rely on cosine similarity or L2 distance to evaluate the likeness of voices and faces following contrastive learning, subsequently applied to retrieval and matching tasks. This method only considers the embeddings as high-dimensional vectors, utilizing a minimal scope of available information. This paper introduces a novel framework within an unsupervised setting for learning voice-face associations. By employing a multimodal encoder after contrastive learning and addressing the problem through binary classification, we can learn the implicit information within the embeddings in a more effective and varied manner. Furthermore, by introducing an effective pair selection method, we enhance the learning outcomes of both contrastive learning and the matching task. Empirical evidence demonstrates that our framework achieves state-of-the-art results in voice-face matching, verification, and retrieval tasks, improving verification by approximately 3%, matching by about 2.5%, and retrieval by around 1.3%."
  },
  {
    "Model Unique Name":"Humans as Checkerboards: Calibrating Camera Motion Scale for World-Coordinate Human Mesh Recovery",
    "Category":"Robotics",
    "Detailed Category":"SLAM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2407.00574v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2407.00574,
    "Summary":"Accurate camera motion estimation is essential for recovering global human motion in world coordinates from RGB video inputs. SLAM is widely used for estimating camera trajectory and point cloud, but monocular SLAM does so only up to an unknown scale factor. Previous works estimate the scale factor through optimization, but this is unreliable and time-consuming. This paper presents an optimization-free scale calibration framework, Human as Checkerboard (HAC). HAC innovatively leverages the human body predicted by human mesh recovery model as a calibration reference. Specifically, it uses the absolute depth of human-scene contact joints as references to calibrate the corresponding relative scene depth from SLAM. HAC benefits from geometric priors encoded in human mesh recovery models to estimate the SLAM scale and achieves precise global human motion estimation. Simple yet powerful, our method sets a new state-of-the-art performance for global human mesh estimation tasks, reducing motion errors by 50% over prior local-to-global methods while using 100$\\times$ less inference time than optimization-based methods. Project page: https:\/\/martayang.github.io\/HAC."
  },
  {
    "Model Unique Name":"Generalized Few-Shot Point Cloud Segmentation Via Geometric Words",
    "Category":"3D Vision",
    "Detailed Category":"Point Cloud Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2309.11222v1.pdf",
    "GitHub":"https:\/\/github.com\/Pixie8888\/GFS-3DSeg_GWs",
    "HuggingFace":null,
    "arxiv_id":2309.11222,
    "Summary":"Existing fully-supervised point cloud segmentation methods suffer in the dynamic testing environment with emerging new classes. Few-shot point cloud segmentation algorithms address this problem by learning to adapt to new classes at the sacrifice of segmentation accuracy for the base classes, which severely impedes its practicality. This largely motivates us to present the first attempt at a more practical paradigm of generalized few-shot point cloud segmentation, which requires the model to generalize to new categories with only a few support point clouds and simultaneously retain the capability to segment base classes. We propose the geometric words to represent geometric components shared between the base and novel classes, and incorporate them into a novel geometric-aware semantic representation to facilitate better generalization to the new classes without forgetting the old ones. Moreover, we introduce geometric prototypes to guide the segmentation with geometric prior knowledge. Extensive experiments on S3DIS and ScanNet consistently illustrate the superior performance of our method over baseline methods. Our code is available at: https:\/\/github.com\/Pixie8888\/GFS-3DSeg_GWs."
  },
  {
    "Model Unique Name":"Self-Supervised Learning for Building Robust Pediatric Chest X-ray Classification Models",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2409.00231v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2409.00231,
    "Summary":"Recent advancements in deep learning for Medical Artificial Intelligence have demonstrated that models can match the diagnostic performance of clinical experts in adult chest X-ray (CXR) interpretation. However, their application in the pediatric context remains limited due to the scarcity of large annotated pediatric image datasets. Additionally, significant challenges arise from the substantial variability in pediatric CXR images across different hospitals and the diverse age range of patients from 0 to 18 years. To address these challenges, we propose SCC, a novel approach that combines transfer learning with self-supervised contrastive learning, augmented by an unsupervised contrast enhancement technique. Transfer learning from a well-trained adult CXR model mitigates issues related to the scarcity of pediatric training data. Contrastive learning with contrast enhancement focuses on the lungs, reducing the impact of image variations and producing high-quality embeddings across diverse pediatric CXR images. We train SCC on one pediatric CXR dataset and evaluate its performance on two other pediatric datasets from different sources. Our results show that SCC's out-of-distribution (zero-shot) performance exceeds regular transfer learning in terms of AUC by 13.6% and 34.6% on the two test datasets. Moreover, with few-shot learning using 10 times fewer labeled images, SCC matches the performance of regular transfer learning trained on the entire labeled dataset. To test the generality of the framework, we verify its performance on three benchmark breast cancer datasets. Starting from a model trained on natural images and fine-tuned on one breast dataset, SCC outperforms the fully supervised learning baseline on the other two datasets in terms of AUC by 3.6% and 5.5% in zero-shot learning."
  },
  {
    "Model Unique Name":"A Comparative Analysis Of Latent Regressor Losses For Singing Voice Conversion",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2302.13678v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2302.13678,
    "Summary":"Previous research has shown that established techniques for spoken voice conversion (VC) do not perform as well when applied to singing voice conversion (SVC). We propose an alternative loss component in a loss function that is otherwise well-established among VC tasks, which has been shown to improve our model's SVC performance. We first trained a singer identity embedding (SIE) network on mel-spectrograms of singer recordings to produce singer-specific variance encodings using contrastive learning. We subsequently trained a well-known autoencoder framework (AutoVC) conditioned on these SIEs, and measured differences in SVC performance when using different latent regressor loss components. We found that using this loss w.r.t. SIEs leads to better performance than w.r.t. bottleneck embeddings, where converted audio is more natural and specific towards target singers. The inclusion of this loss component has the advantage of explicitly forcing the network to reconstruct with timbral similarity, and also negates the effect of poor disentanglement in AutoVC's bottleneck embeddings. We demonstrate peculiar diversity between computational and human evaluations on singer-converted audio clips, which highlights the necessity of both. We also propose a pitch-matching mechanism between source and target singers to ensure these evaluations are not influenced by differences in pitch register."
  },
  {
    "Model Unique Name":"MAINS: A Magnetic Field Aided Inertial Navigation System for Indoor Positioning",
    "Category":"Robotics",
    "Detailed Category":"SLAM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2312.02599v3.pdf",
    "GitHub":"https:\/\/github.com\/huang-chuan\/mainsvsmagekf",
    "HuggingFace":null,
    "arxiv_id":2312.02599,
    "Summary":"A Magnetic field Aided Inertial Navigation System (MAINS) for indoor navigation is proposed in this paper. MAINS leverages an array of magnetometers to measure spatial variations in the magnetic field, which are then used to estimate the displacement and orientation changes of the system, thereby aiding the inertial navigation system (INS). Experiments show that MAINS significantly outperforms the stand-alone INS, demonstrating a remarkable two orders of magnitude reduction in position error. Furthermore, when compared to the state-of-the-art magnetic-field-aided navigation approach, the proposed method exhibits slightly improved horizontal position accuracy. On the other hand, it has noticeably larger vertical error on datasets with large magnetic field variations. However, one of the main advantages of MAINS compared to the state-of-the-art is that it enables flexible sensor configurations. The experimental results show that the position error after 2 minutes of navigation in most cases is less than 3 meters when using an array of 30 magnetometers. Thus, the proposed navigation solution has the potential to solve one of the key challenges faced with current magnetic-field simultaneous localization and mapping (SLAM) solutions: the very limited allowable length of the exploration phase during which unvisited areas are mapped."
  },
  {
    "Model Unique Name":"MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction",
    "Category":"Multi-modal",
    "Detailed Category":"Multi-modal",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2402.12238v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2402.12238,
    "Summary":"To predict future trajectories, the normalizing flow with a standard Gaussian prior suffers from weak diversity. The ineffectiveness comes from the conflict between the fact of asymmetric and multi-modal distribution of likely outcomes and symmetric and single-modal original distribution and supervision losses. Instead, we propose constructing a mixed Gaussian prior for a normalizing flow model for trajectory prediction. The prior is constructed by analyzing the trajectory patterns in the training samples without requiring extra annotations while showing better expressiveness and being multi-modal and asymmetric. Besides diversity, it also provides better controllability for probabilistic trajectory generation. We name our method Mixed Gaussian Flow (MGF). It achieves state-of-the-art performance in the evaluation of both trajectory alignment and diversity on the popular UCY\/ETH and SDD datasets. Code is available at https:\/\/github.com\/mulplue\/MGF."
  },
  {
    "Model Unique Name":"Embodied Supervision: Haptic Display of Automation Command to Improve Supervisory Performance",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2402.18707v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2402.18707,
    "Summary":"A human operator using a manual control interface has ready access to their own command signal, both by efference copy and proprioception. In contrast, a human supervisor typically relies on visual information alone. We propose supplying a supervisor with a copy of the operators command signal, hypothesizing improved performance, especially when that copy is provided through haptic display. We experimentally compared haptic with visual access to the command signal, quantifying the performance of N equals 10 participants attempting to determine which of three reference signals was being tracked by an operator. Results indicate an improved accuracy in identifying the tracked target when haptic display was available relative to visual display alone. We conjecture the benefit follows from the relationship of haptics to the supervisor's own experience, perhaps muscle memory, as an operator."
  },
  {
    "Model Unique Name":"Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards",
    "Category":"Reinforcement Learning",
    "Detailed Category":"Reinforcement Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2408.12112v3.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2408.12112,
    "Summary":"LLMs are increasingly used to design reward functions based on human preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards for Restless Multi-Armed Bandits, a framework for allocating limited resources among agents. In applications such as public health, this approach empowers grassroots health workers to tailor automated allocation decisions to community needs. In the presence of multiple agents, altering the reward function based on human preferences can impact subpopulations very differently, leading to complex tradeoffs and a multi-objective resource allocation problem. We are the first to present a principled method termed Social Choice Language Model for dealing with these tradeoffs for LLM-designed rewards for multiagent planners in general and restless bandits in particular. The novel part of our model is a transparent and configurable selection component, called an adjudicator, external to the LLM that controls complex tradeoffs via a user-selected social welfare function. Our experiments demonstrate that our model reliably selects more effective, aligned, and balanced reward functions compared to purely LLM-based approaches."
  },
  {
    "Model Unique Name":"Leveraging Semantic Cues from Foundation Vision Models for Enhanced Local Feature Correspondence",
    "Category":"Robotics",
    "Detailed Category":"SFM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2410.09533v1.pdf",
    "GitHub":"https:\/\/github.com\/verlab\/DescriptorReasoning_ACCV_2024",
    "HuggingFace":null,
    "arxiv_id":2410.09533,
    "Summary":"Visual correspondence is a crucial step in key computer vision tasks, including camera localization, image registration, and structure from motion. The most effective techniques for matching keypoints currently involve using learned sparse or dense matchers, which need pairs of images. These neural networks have a good general understanding of features from both images, but they often struggle to match points from different semantic areas. This paper presents a new method that uses semantic cues from foundation vision model features (like DINOv2) to enhance local feature matching by incorporating semantic reasoning into existing descriptors. Therefore, the learned descriptors do not require image pairs at inference time, allowing feature caching and fast matching using similarity search, unlike learned matchers. We present adapted versions of six existing descriptors, with an average increase in performance of 29% in camera localization, with comparable accuracy to existing matchers as LightGlue and LoFTR in two existing benchmarks. Both code and trained models are available at https:\/\/www.verlab.dcc.ufmg.br\/descriptors\/reasoning_accv24"
  },
  {
    "Model Unique Name":"TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection",
    "Category":"Multi-modal",
    "Detailed Category":"Multi-modal",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.09421v1.pdf",
    "GitHub":"https:\/\/github.com\/uestc-gqj\/tiefake",
    "HuggingFace":null,
    "arxiv_id":2304.09421,
    "Summary":"Fake news detection aims to detect fake news widely spreading on social media platforms, which can negatively influence the public and the government. Many approaches have been developed to exploit relevant information from news images, text, or videos. However, these methods may suffer from the following limitations: (1) ignore the inherent emotional information of the news, which could be beneficial since it contains the subjective intentions of the authors; (2) pay little attention to the relation (similarity) between the title and textual information in news articles, which often use irrelevant title to attract reader' attention. To this end, we propose a novel Title-Text similarity and emotion-aware Fake news detection (TieFake) method by jointly modeling the multi-modal context information and the author sentiment in a unified framework. Specifically, we respectively employ BERT and ResNeSt to learn the representations for text and images, and utilize publisher emotion extractor to capture the author's subjective emotion in the news content. We also propose a scale-dot product attention mechanism to capture the similarity between title features and textual features. Experiments are conducted on two publicly available multi-modal datasets, and the results demonstrate that our proposed method can significantly improve the performance of fake news detection. Our code is available at https:\/\/github.com\/UESTC-GQJ\/TieFake."
  },
  {
    "Model Unique Name":"Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction",
    "Category":"Computer Vision",
    "Detailed Category":"Semantic Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2308.15427v3.pdf",
    "GitHub":"https:\/\/github.com\/xjtu-cs-gao\/satforhdmap",
    "HuggingFace":null,
    "arxiv_id":2308.15427,
    "Summary":"High-definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time using vehicle onboard sensors. Due to the inherent limitations of onboard sensors, which include sensitivity to detection range and susceptibility to occlusion by nearby vehicles, the performance of these methods significantly declines in complex scenarios and long-range detection tasks. In this paper, we explore a new perspective that boosts HD map construction through the use of satellite maps to complement onboard sensors. We initially generate the satellite map tiles for each sample in nuScenes and release a complementary dataset for further research. To enable better integration of satellite maps with existing methods, we propose a hierarchical fusion module, which includes feature-level fusion and BEV-level fusion. The feature-level fusion, composed of a mask generator and a masked cross-attention mechanism, is used to refine the features from onboard sensors. The BEV-level fusion mitigates the coordinate differences between features obtained from onboard sensors and satellite maps through an alignment module. The experimental results on the augmented nuScenes showcase the seamless integration of our module into three existing HD map construction methods. The satellite maps and our proposed module notably enhance their performance in both HD map semantic segmentation and instance detection tasks."
  },
  {
    "Model Unique Name":"DeformableFormer: Classification of Endoscopic Ultrasound Guided Fine Needle Biopsy in Pancreatic Diseases",
    "Category":"Computer Vision",
    "Detailed Category":"Image Classification",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.10791v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2304.10791,
    "Summary":"Endoscopic Ultrasound-Fine Needle Aspiration (EUS-FNA) is used to examine pancreatic cancer. EUS-FNA is an examination using EUS to insert a thin needle into the tumor and collect pancreatic tissue fragments. Then collected pancreatic tissue fragments are then stained to classify whether they are pancreatic cancer. However, staining and visual inspection are time consuming. In addition, if the pancreatic tissue fragment cannot be examined after staining, the collection must be done again on the other day. Therefore, our purpose is to classify from an unstained image whether it is available for examination or not, and to exceed the accuracy of visual classification by specialist physicians. Image classification before staining can reduce the time required for staining and the burden of patients. However, the images of pancreatic tissue fragments used in this study cannot be successfully classified by processing the entire image because the pancreatic tissue fragments are only a part of the image. Therefore, we propose a DeformableFormer that uses Deformable Convolution in MetaFormer framework. The architecture consists of a generalized model of the Vision Transformer, and we use Deformable Convolution in the TokenMixer part. In contrast to existing approaches, our proposed DeformableFormer is possible to perform feature extraction more locally and dynamically by Deformable Convolution. Therefore, it is possible to perform suitable feature extraction for classifying target. To evaluate our method, we classify two categories of pancreatic tissue fragments; available and unavailable for examination. We demonstrated that our method outperformed the accuracy by specialist physicians and conventional methods."
  },
  {
    "Model Unique Name":"AARK: An Open Toolkit for Autonomous Racing Research",
    "Category":"Computer Vision",
    "Detailed Category":"Semantic Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2410.00358v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2410.00358,
    "Summary":"Autonomous racing demands safe control of vehicles at their physical limits for extended periods of time, providing insights into advanced vehicle safety systems which increasingly rely on intervention provided by vehicle autonomy. Participation in this field carries with it a high barrier to entry. Physical platforms and their associated sensor suites require large capital outlays before any demonstrable progress can be made. Simulators allow researches to develop soft autonomous systems without purchasing a platform. However, currently available simulators lack visual and dynamic fidelity, can still be expensive to buy, lack customisation, and are difficult to use. AARK provides three packages, ACI, ACDG, and ACMPC. These packages enable research into autonomous control systems in the demanding environment of racing to bring more people into the field and improve reproducibility: ACI provides researchers with a computer vision-friendly interface to Assetto Corsa for convenient comparison and evaluation of autonomous control solutions; ACDG enables generation of depth, normal and semantic segmentation data for training computer vision models to use in perception systems; and ACMPC gives newcomers to the field a modular full-stack autonomous control solution, capable of controlling vehicles to build from. AARK aims to unify and democratise research into a field critical to providing safer roads and trusted autonomous systems."
  },
  {
    "Model Unique Name":"KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection",
    "Category":"Computer Vision",
    "Detailed Category":"Object Detection",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2307.07942v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2307.07942,
    "Summary":"Achieving a reliable LiDAR-based object detector in autonomous driving is paramount, but its success hinges on obtaining large amounts of precise 3D annotations. Active learning (AL) seeks to mitigate the annotation burden through algorithms that use fewer labels and can attain performance comparable to fully supervised learning. Although AL has shown promise, current approaches prioritize the selection of unlabeled point clouds with high uncertainty and\/or diversity, leading to the selection of more instances for labeling and reduced computational efficiency. In this paper, we resort to a novel kernel coding rate maximization (KECOR) strategy which aims to identify the most informative point clouds to acquire labels through the lens of information theory. Greedy search is applied to seek desired point clouds that can maximize the minimal number of bits required to encode the latent features. To determine the uniqueness and informativeness of the selected samples from the model perspective, we construct a proxy network of the 3D detector head and compute the outer product of Jacobians from all proxy layers to form the empirical neural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e., SECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate the classification entropy maximization and well trade-off between detection performance and the total number of bounding boxes selected for annotation. Extensive experiments conducted on two 3D benchmarks and a 2D detection dataset evidence the superiority and versatility of the proposed approach. Our results show that approximately 44% box-level annotation costs and 26% computational time are reduced compared to the state-of-the-art AL method, without compromising detection performance."
  },
  {
    "Model Unique Name":"Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera",
    "Category":"Robotics",
    "Detailed Category":"SLAM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2412.12861v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2412.12861,
    "Summary":"We propose Dyn-HaMR, to the best of our knowledge, the first approach to reconstruct 4D global hand motion from monocular videos recorded by dynamic cameras in the wild. Reconstructing accurate 3D hand meshes from monocular videos is a crucial task for understanding human behaviour, with significant applications in augmented and virtual reality (AR\/VR). However, existing methods for monocular hand reconstruction typically rely on a weak perspective camera model, which simulates hand motion within a limited camera frustum. As a result, these approaches struggle to recover the full 3D global trajectory and often produce noisy or incorrect depth estimations, particularly when the video is captured by dynamic or moving cameras, which is common in egocentric scenarios. Our Dyn-HaMR consists of a multi-stage, multi-objective optimization pipeline, that factors in (i) simultaneous localization and mapping (SLAM) to robustly estimate relative camera motion, (ii) an interacting-hand prior for generative infilling and to refine the interaction dynamics, ensuring plausible recovery under (self-)occlusions, and (iii) hierarchical initialization through a combination of state-of-the-art hand tracking methods. Through extensive evaluations on both in-the-wild and indoor datasets, we show that our approach significantly outperforms state-of-the-art methods in terms of 4D global mesh recovery. This establishes a new benchmark for hand motion reconstruction from monocular video with moving cameras. Our project page is at https:\/\/dyn-hamr.github.io\/."
  },
  {
    "Model Unique Name":"Human Motion Prediction, Reconstruction, and Generation",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2502.15956v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2502.15956,
    "Summary":"This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields."
  },
  {
    "Model Unique Name":"Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2502.17219v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2502.17219,
    "Summary":"Humans possess delicate dynamic balance mechanisms that enable them to maintain stability across diverse terrains and under extreme conditions. However, despite significant advances recently, existing locomotion algorithms for humanoid robots are still struggle to traverse extreme environments, especially in cases that lack external perception (e.g., vision or LiDAR). This is because current methods often rely on gait-based or perception-condition rewards, lacking effective mechanisms to handle unobservable obstacles and sudden balance loss. To address this challenge, we propose a novel whole-body locomotion algorithm based on dynamic balance and Reinforcement Learning (RL) that enables humanoid robots to traverse extreme terrains, particularly narrow pathways and unexpected obstacles, using only proprioception. Specifically, we introduce a dynamic balance mechanism by leveraging an extended measure of Zero-Moment Point (ZMP)-driven rewards and task-driven rewards in a whole-body actor-critic framework, aiming to achieve coordinated actions of the upper and lower limbs for robust locomotion. Experiments conducted on a full-sized Unitree H1-2 robot verify the ability of our method to maintain balance on extremely narrow terrains and under external disturbances, demonstrating its effectiveness in enhancing the robot's adaptability to complex environments. The videos are given at https:\/\/whole-body-loco.github.io."
  },
  {
    "Model Unique Name":"MCoCo: Multi-level Consistency Collaborative Multi-view Clustering",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2302.13339v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2302.13339,
    "Summary":"Multi-view clustering can explore consistent information from different views to guide clustering. Most existing works focus on pursuing shallow consistency in the feature space and integrating the information of multiple views into a unified representation for clustering. These methods did not fully consider and explore the consistency in the semantic space. To address this issue, we proposed a novel Multi-level Consistency Collaborative learning framework (MCoCo) for multi-view clustering. Specifically, MCoCo jointly learns cluster assignments of multiple views in feature space and aligns semantic labels of different views in semantic space by contrastive learning. Further, we designed a multi-level consistency collaboration strategy, which utilizes the consistent information of semantic space as a self-supervised signal to collaborate with the cluster assignments in feature space. Thus, different levels of spaces collaborate with each other while achieving their own consistency goals, which makes MCoCo fully mine the consistent information of different views without fusion. Compared with state-of-the-art methods, extensive experiments demonstrate the effectiveness and superiority of our method."
  },
  {
    "Model Unique Name":"Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient Sparse Matrix Multiplication",
    "Category":"Graph Neural Network",
    "Detailed Category":"Graph Neural Network",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2402.19364v2.pdf",
    "GitHub":"https:\/\/github.com\/spcl\/arrow-matrix",
    "HuggingFace":null,
    "arxiv_id":2402.19364,
    "Summary":"We propose a novel approach to iterated sparse matrix dense matrix multiplication, a fundamental computational kernel in scientific computing and graph neural network training. In cases where matrix sizes exceed the memory of a single compute node, data transfer becomes a bottleneck. An approach based on dense matrix multiplication algorithms leads to suboptimal scalability and fails to exploit the sparsity in the problem. To address these challenges, we propose decomposing the sparse matrix into a small number of highly structured matrices called arrow matrices, which are connected by permutations. Our approach enables communication-avoiding multiplications, achieving a polynomial reduction in communication volume per iteration for matrices corresponding to planar graphs and other minor-excluded families of graphs. Our evaluation demonstrates that our approach outperforms a state-of-the-art method for sparse matrix multiplication on matrices with hundreds of millions of rows, offering near-linear strong and weak scaling."
  },
  {
    "Model Unique Name":"Many-body quantum register for a spin qubit",
    "Category":"Transformer",
    "Detailed Category":"Transformer",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.19680v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.1968,
    "Summary":"Quantum networks require quantum nodes with coherent optical interfaces and multiple stationary qubits. In terms of optical properties, semiconductor quantum dots are highly compelling, but their adoption as quantum nodes has been impaired by the lack of auxiliary qubits. Here, we demonstrate a functional quantum register in a semiconductor quantum dot leveraging the dense, always-present nuclear spin ensemble. We prepare 13,000 host nuclear spins into a single many-body dark state to operate as the register logic state $|0\\rangle$. The logic state $|1\\rangle$ is defined as a single nuclear magnon excitation, enabling controlled quantum-state transfer between the electron spin qubit and the nuclear magnonic register. Using 130-ns SWAP gates, we implement a full write-store-retrieve-readout protocol with 68.6(4)% raw overall fidelity and a storage time of 130(16) $\\mu$s in the absence of dynamical decoupling. Our work establishes how many-body physics can add step-change functionality to quantum devices, in this case transforming quantum dots into multi-qubit quantum nodes with deterministic registers."
  },
  {
    "Model Unique Name":"Underwater Acoustic Source Seeking Using Time-Difference-of-Arrival Measurements",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2402.17405v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2402.17405,
    "Summary":"The research presented in this paper is aimed at developing a control algorithm for an autonomous surface system carrying a two-sensor array consisting of two acoustic receivers, capable of measuring the time-difference-of-arrival (TDOA) of a quasiperiodic underwater acoustic signal and utilizing this value to steer the system toward the acoustic source in the horizontal plane. Stability properties of the proposed algorithm are analyzed using the Lie bracket approximation technique. Furthermore, simulation results are presented, where particular attention is given to the relationship between the time difference of arrival measurement noise and the sensor baseline - the distance between the two acoustic receivers. Also, the influence of a constant disturbance caused by sea currents is considered. Finally, experimental results in which the algorithm was deployed on two autonomous surface vehicles, each equipped with a single acoustic receiver, are presented. The algorithm successfully steers the vehicle formation toward the acoustic source, despite the measurement noise and intermittent measurements, thus showing the feasibility of the proposed algorithm in real-life conditions."
  },
  {
    "Model Unique Name":"NUMSnet: Nested-U Multi-class Segmentation network for 3D Medical Image Stacks",
    "Category":"Computer Vision",
    "Detailed Category":"Semantic Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.02713v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2304.02713,
    "Summary":"Semantic segmentation for medical 3D image stacks enables accurate volumetric reconstructions, computer-aided diagnostics and follow up treatment planning. In this work, we present a novel variant of the Unet model called the NUMSnet that transmits pixel neighborhood features across scans through nested layers to achieve accurate multi-class semantic segmentations with minimal training data. We analyze the semantic segmentation performance of the NUMSnet model in comparison with several Unet model variants to segment 3-7 regions of interest using only 10% of images for training per Lung-CT and Heart-CT volumetric image stacks. The proposed NUMSnet model achieves up to 20% improvement in segmentation recall with 4-9% improvement in Dice scores for Lung-CT stacks and 2.5-10% improvement in Dice scores for Heart-CT stacks when compared to the Unet++ model. The NUMSnet model needs to be trained by ordered images around the central scan of each volumetric stack. Propagation of image feature information from the 6 nested layers of the Unet++ model are found to have better computation and segmentation performances than propagation of all up-sampling layers in a Unet++ model. The NUMSnet model achieves comparable segmentation performances to existing works, while being trained on as low as 5\\% of the training images. Also, transfer learning allows faster convergence of the NUMSnet model for multi-class semantic segmentation from pathology in Lung-CT images to cardiac segmentations in Heart-CT stacks. Thus, the proposed model can standardize multi-class semantic segmentation on a variety of volumetric image stacks with minimal training dataset. This can significantly reduce the cost, time and inter-observer variabilities associated with computer-aided detections and treatment."
  },
  {
    "Model Unique Name":"QueryCAD: Grounded Question Answering for CAD Models",
    "Category":"Computer Vision",
    "Detailed Category":"Instance Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2409.08704v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2409.08704,
    "Summary":"CAD models are widely used in industry and are essential for robotic automation processes. However, these models are rarely considered in novel AI-based approaches, such as the automatic synthesis of robot programs, as there are no readily available methods that would allow CAD models to be incorporated for the analysis, interpretation, or extraction of information. To address these limitations, we propose QueryCAD, the first system designed for CAD question answering, enabling the extraction of precise information from CAD models using natural language queries. QueryCAD incorporates SegCAD, an open-vocabulary instance segmentation model we developed to identify and select specific parts of the CAD model based on part descriptions. We further propose a CAD question answering benchmark to evaluate QueryCAD and establish a foundation for future research. Lastly, we integrate QueryCAD within an automatic robot program synthesis framework, validating its ability to enhance deep-learning solutions for robotics by enabling them to process CAD models (https:\/\/claudius-kienle.github.com\/querycad)."
  },
  {
    "Model Unique Name":"Leveraging Multimodal Features and Item-level User Feedback for Bundle Construction",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2310.18770v1.pdf",
    "GitHub":"https:\/\/github.com\/xiaohao-liu\/clhe",
    "HuggingFace":null,
    "arxiv_id":2310.1877,
    "Summary":"Automatic bundle construction is a crucial prerequisite step in various bundle-aware online services. Previous approaches are mostly designed to model the bundling strategy of existing bundles. However, it is hard to acquire large-scale well-curated bundle dataset, especially for those platforms that have not offered bundle services before. Even for platforms with mature bundle services, there are still many items that are included in few or even zero bundles, which give rise to sparsity and cold-start challenges in the bundle construction models. To tackle these issues, we target at leveraging multimodal features, item-level user feedback signals, and the bundle composition information, to achieve a comprehensive formulation of bundle construction. Nevertheless, such formulation poses two new technical challenges: 1) how to learn effective representations by optimally unifying multiple features, and 2) how to address the problems of modality missing, noise, and sparsity problems induced by the incomplete query bundles. In this work, to address these technical challenges, we propose a Contrastive Learning-enhanced Hierarchical Encoder method (CLHE). Specifically, we use self-attention modules to combine the multimodal and multi-item features, and then leverage both item- and bundle-level contrastive learning to enhance the representation learning, thus to counter the modality missing, noise, and sparsity problems. Extensive experiments on four datasets in two application domains demonstrate that our method outperforms a list of SOTA methods. The code and dataset are available at https:\/\/github.com\/Xiaohao-Liu\/CLHE."
  },
  {
    "Model Unique Name":"Look Around and Learn: Self-Training Object Detection by Exploration",
    "Category":"Computer Vision",
    "Detailed Category":"Object Detection",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2302.03566v4.pdf",
    "GitHub":"https:\/\/github.com\/iit-pavis\/look_around_and_learn",
    "HuggingFace":null,
    "arxiv_id":2302.03566,
    "Summary":"When an object detector is deployed in a novel setting it often experiences a drop in performance. This paper studies how an embodied agent can automatically fine-tune a pre-existing object detector while exploring and acquiring images in a new environment without relying on human intervention, i.e., a fully self-supervised approach. In our setting, an agent initially learns to explore the environment using a pre-trained off-the-shelf detector to locate objects and associate pseudo-labels. By assuming that pseudo-labels for the same object must be consistent across different views, we learn the exploration policy Look Around to mine hard samples, and we devise a novel mechanism called Disagreement Reconciliation for producing refined pseudo-labels from the consensus among observations. We implement a unified benchmark of the current state-of-the-art and compare our approach with pre-existing exploration policies and perception mechanisms. Our method is shown to outperform existing approaches, improving the object detector by 6.2% in a simulated scenario, a 3.59% advancement over other state-of-the-art methods, and by 9.97% in the real robotic test without relying on ground-truth. Code for the proposed approach and baselines are available at https:\/\/iit-pavis.github.io\/Look_Around_And_Learn\/."
  },
  {
    "Model Unique Name":"Language Models for Code Completion: A Practical Evaluation",
    "Category":"Large-Language Model",
    "Detailed Category":"Large-Language Model",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2402.16197v1.pdf",
    "GitHub":"https:\/\/github.com\/aise-tudelft\/code4meevaluation",
    "HuggingFace":null,
    "arxiv_id":2402.16197,
    "Summary":"Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the model's predictions, we found that 66.3% of failures were due to the models' limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability."
  },
  {
    "Model Unique Name":"Transformer Based Implementation for Automatic Book Summarization",
    "Category":"Large-Language Model",
    "Detailed Category":"Large-Language Model",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2301.07057v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2301.07057,
    "Summary":"Document Summarization is the procedure of generating a meaningful and concise summary of a given document with the inclusion of relevant and topic-important points. There are two approaches: one is picking up the most relevant statements from the document itself and adding it to the Summary known as Extractive and the other is generating sentences for the Summary known as Abstractive Summarization. Training a machine learning model to perform tasks that are time-consuming or very difficult for humans to evaluate is a major challenge. Book Abstract generation is one of such complex tasks. Traditional machine learning models are getting modified with pre-trained transformers. Transformer based language models trained in a self-supervised fashion are gaining a lot of attention; when fine-tuned for Natural Language Processing(NLP) downstream task like text summarization. This work is an attempt to use Transformer based techniques for Abstract generation."
  },
  {
    "Model Unique Name":"FedSSP: Federated Graph Learning with Spectral Knowledge and Personalized Preference",
    "Category":"Graph Neural Network",
    "Detailed Category":"Graph Neural Network",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2410.20105v1.pdf",
    "GitHub":"https:\/\/github.com\/oakleytan\/fedssp",
    "HuggingFace":null,
    "arxiv_id":2410.20105,
    "Summary":"Personalized Federated Graph Learning (pFGL) facilitates the decentralized training of Graph Neural Networks (GNNs) without compromising privacy while accommodating personalized requirements for non-IID participants. In cross-domain scenarios, structural heterogeneity poses significant challenges for pFGL. Nevertheless, previous pFGL methods incorrectly share non-generic knowledge globally and fail to tailor personalized solutions locally under domain structural shift. We innovatively reveal that the spectral nature of graphs can well reflect inherent domain structural shifts. Correspondingly, our method overcomes it by sharing generic spectral knowledge. Moreover, we indicate the biased message-passing schemes for graph structures and propose the personalized preference module. Combining both strategies, we propose our pFGL framework FedSSP which Shares generic Spectral knowledge while satisfying graph Preferences. Furthermore, We perform extensive experiments on cross-dataset and cross-domain settings to demonstrate the superiority of our framework. The code is available at https:\/\/github.com\/OakleyTan\/FedSSP."
  },
  {
    "Model Unique Name":"Towards Explaining Uncertainty Estimates in Point Cloud Registration",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2412.20612v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2412.20612,
    "Summary":"Iterative Closest Point (ICP) is a commonly used algorithm to estimate transformation between two point clouds. The key idea of this work is to leverage recent advances in explainable AI for probabilistic ICP methods that provide uncertainty estimates. Concretely, we propose a method that can explain why a probabilistic ICP method produced a particular output. Our method is based on kernel SHAP (SHapley Additive exPlanations). With this, we assign an importance value to common sources of uncertainty in ICP such as sensor noise, occlusion, and ambiguous environments. The results of the experiment show that this explanation method can reasonably explain the uncertainty sources, providing a step towards robots that know when and why they failed in a human interpretable manner"
  },
  {
    "Model Unique Name":"Age-minimal Multicast by Graph Attention Reinforcement Learning",
    "Category":"Reinforcement Learning",
    "Detailed Category":"Reinforcement Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.18084v4.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.18084,
    "Summary":"Multicast routing is essential for real-time group applications, such as video streaming, virtual reality, and metaverse platforms, where the Age of Information (AoI) acts as a crucial metric to assess information timeliness. This paper studies dynamic multicast networks with the objective of minimizing the expected average Age of Information (AoI) by jointly optimizing multicast routing and scheduling. The main challenges stem from the intricate coupling between routing and scheduling decisions, the inherent complexity of multicast operations, and the graph representation. We first decompose the original problem into two subtasks amenable to hierarchical reinforcement learning (RL) methods. We propose the first RL framework to address the multicast routing problem, also known as the Steiner Tree problem, by incorporating graph embedding and the successive addition of nodes and links. For graph embedding, we propose the Normalized Graph Attention mechanism (NGAT) framework with a proven contraction mapping property, enabling effective graph information capture and superior generalization within the hierarchical RL framework. We validate our framework through experiments on four datasets, including the real-world AS-733 dataset. The results demonstrate that our proposed scheme can be up to 9.85 times more computationally efficient than traditional multicast routing algorithms, achieving approximation ratios of 1.1-1.3 that are not only comparable to state-of-the-art (SOTA) methods but also highlight its superior generalization capabilities, performing effectively on unseen and more complex tasks. Additionally, our age-optimal TGMS algorithm reduces the average weighted Age of Information (AoI) by 25.6% and the weighted peak age by 29.2% under low-energy scenarios."
  },
  {
    "Model Unique Name":"ViMoE: An Empirical Study of Designing Vision Mixture-of-Experts",
    "Category":"Computer Vision",
    "Detailed Category":"Semantic Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2410.15732v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2410.15732,
    "Summary":"Mixture-of-Experts (MoE) models embody the divide-and-conquer concept and are a promising approach for increasing model capacity, demonstrating excellent scalability across multiple domains. In this paper, we integrate the MoE structure into the classic Vision Transformer (ViT), naming it ViMoE, and explore the potential of applying MoE to vision through a comprehensive study on image classification and semantic segmentation. However, we observe that the performance is sensitive to the configuration of MoE layers, making it challenging to obtain optimal results without careful design. The underlying cause is that inappropriate MoE layers lead to unreliable routing and hinder experts from effectively acquiring helpful information. To address this, we introduce a shared expert to learn and capture common knowledge, serving as an effective way to construct stable ViMoE. Furthermore, we demonstrate how to analyze expert routing behavior, revealing which MoE layers are capable of specializing in handling specific information and which are not. This provides guidance for retaining the critical layers while removing redundancies, thereby advancing ViMoE to be more efficient without sacrificing accuracy. We aspire for this work to offer new insights into the design of vision MoE models and provide valuable empirical guidance for future research."
  },
  {
    "Model Unique Name":"CoReFace: Sample-Guided Contrastive Regularization for Deep Face Recognition",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.11668v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2304.11668,
    "Summary":"The discriminability of feature representation is the key to open-set face recognition. Previous methods rely on the learnable weights of the classification layer that represent the identities. However, the evaluation process learns no identity representation and drops the classifier from training. This inconsistency could confuse the feature encoder in understanding the evaluation goal and hinder the effect of identity-based methods. To alleviate the above problem, we propose a novel approach namely Contrastive Regularization for Face recognition (CoReFace) to apply image-level regularization in feature representation learning. Specifically, we employ sample-guided contrastive learning to regularize the training with the image-image relationship directly, which is consistent with the evaluation process. To integrate contrastive learning into face recognition, we augment embeddings instead of images to avoid the image quality degradation. Then, we propose a novel contrastive loss for the representation distribution by incorporating an adaptive margin and a supervised contrastive mask to generate steady loss values and avoid the collision with the classification supervision signal. Finally, we discover and solve the semantically repetitive signal problem in contrastive learning by exploring new pair coupling protocols. Extensive experiments demonstrate the efficacy and efficiency of our CoReFace which is highly competitive with the state-of-the-art approaches."
  },
  {
    "Model Unique Name":"Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
    "Category":"Reinforcement Learning",
    "Detailed Category":"Reinforcement Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2410.18076v3.pdf",
    "GitHub":"https:\/\/github.com\/rail-berkeley\/supe",
    "HuggingFace":null,
    "arxiv_id":2410.18076,
    "Summary":"Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled offline trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-labels unlabeled trajectories with optimistic rewards and high-level action labels, transforming prior data into high-level, task-relevant examples that encourage novelty-seeking behavior. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. In our experiments, SUPE consistently outperforms prior strategies across a suite of 42 long-horizon, sparse-reward tasks. Code: https:\/\/github.com\/rail-berkeley\/supe."
  },
  {
    "Model Unique Name":"A priori compression of convolutional neural networks for wave simulators",
    "Category":"Computer Vision",
    "Detailed Category":"Image Classification",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.04964v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2304.04964,
    "Summary":"Convolutional neural networks are now seeing widespread use in a variety of fields, including image classification, facial and object recognition, medical imaging analysis, and many more. In addition, there are applications such as physics-informed simulators in which accurate forecasts in real time with a minimal lag are required. The present neural network designs include millions of parameters, which makes it difficult to install such complex models on devices that have limited memory. Compression techniques might be able to resolve these issues by decreasing the size of CNN models that are created by reducing the number of parameters that contribute to the complexity of the models. We propose a compressed tensor format of convolutional layer, a priori, before the training of the neural network. 3-way kernels or 2-way kernels in convolutional layers are replaced by one-way fiters. The overfitting phenomena will be reduced also. The time needed to make predictions or time required for training using the original Convolutional Neural Networks model would be cut significantly if there were fewer parameters to deal with. In this paper we present a method of a priori compressing convolutional neural networks for finite element (FE) predictions of physical data. Afterwards we validate our a priori compressed models on physical data from a FE model solving a 2D wave equation. We show that the proposed convolutinal compression technique achieves equivalent performance as classical convolutional layers with fewer trainable parameters and lower memory footprint."
  },
  {
    "Model Unique Name":"Preferential Multi-Target Search in Indoor Environments using Semantic SLAM",
    "Category":"Robotics",
    "Detailed Category":"SLAM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2309.14063v3.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2309.14063,
    "Summary":"In recent years, the demand for service robots capable of executing tasks beyond autonomous navigation has grown. In the future, service robots will be expected to perform complex tasks like 'Set table for dinner'. High-level tasks like these, require, among other capabilities, the ability to retrieve multiple targets. This paper delves into the challenge of locating multiple targets in an environment, termed 'Find my Objects.' We present a novel heuristic designed to facilitate robots in conducting a preferential search for multiple targets in indoor spaces. Our approach involves a Semantic SLAM framework that combines semantic object recognition with geometric data to generate a multi-layered map. We fuse the semantic maps with probabilistic priors for efficient inferencing. Recognizing the challenges introduced by obstacles that might obscure a navigation goal and render standard point-to-point navigation strategies less viable, our methodology offers resilience to such factors. Importantly, our method is adaptable to various object detectors, RGB-D SLAM techniques, and local navigation planners. We demonstrate the 'Find my Objects' task in real-world indoor environments, yielding quantitative results that attest to the effectiveness of our methodology. This strategy can be applied in scenarios where service robots need to locate, grasp, and transport objects, taking into account user preferences. For a brief summary, please refer to our video: https:\/\/tinyurl.com\/PrefTargetSearch"
  },
  {
    "Model Unique Name":"Representing Long Volumetric Video with Temporal Gaussian Hierarchy",
    "Category":"3D Vision",
    "Detailed Category":"Point Cloud",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2412.09608v1.pdf",
    "GitHub":"https:\/\/github.com\/dendenxu\/fast-gaussian-rasterization",
    "HuggingFace":null,
    "arxiv_id":2412.09608,
    "Summary":"This paper aims to address the challenge of reconstructing long volumetric videos from multi-view RGB videos. Recent dynamic view synthesis methods leverage powerful 4D representations, like feature grids or point cloud sequences, to achieve high-quality rendering results. However, they are typically limited to short (1~2s) video clips and often suffer from large memory footprints when dealing with longer videos. To solve this issue, we propose a novel 4D representation, named Temporal Gaussian Hierarchy, to compactly model long volumetric videos. Our key observation is that there are generally various degrees of temporal redundancy in dynamic scenes, which consist of areas changing at different speeds. Motivated by this, our approach builds a multi-level hierarchy of 4D Gaussian primitives, where each level separately describes scene regions with different degrees of content change, and adaptively shares Gaussian primitives to represent unchanged scene content over different temporal segments, thus effectively reducing the number of Gaussian primitives. In addition, the tree-like structure of the Gaussian hierarchy allows us to efficiently represent the scene at a particular moment with a subset of Gaussian primitives, leading to nearly constant GPU memory usage during the training or rendering regardless of the video length. Extensive experimental results demonstrate the superiority of our method over alternative methods in terms of training cost, rendering speed, and storage usage. To our knowledge, this work is the first approach capable of efficiently handling minutes of volumetric video data while maintaining state-of-the-art rendering quality. Our project page is available at: https:\/\/zju3dv.github.io\/longvolcap."
  },
  {
    "Model Unique Name":"SCoDA: Domain Adaptive Shape Completion for Real Scans",
    "Category":"3D Vision",
    "Detailed Category":"Point Cloud",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.10179v2.pdf",
    "GitHub":"https:\/\/github.com\/snap-research\/r2l",
    "HuggingFace":null,
    "arxiv_id":2304.10179,
    "Summary":"3D shape completion from point clouds is a challenging task, especially from scans of real-world objects. Considering the paucity of 3D shape ground truths for real scans, existing works mainly focus on benchmarking this task on synthetic data, e.g. 3D computer-aided design models. However, the domain gap between synthetic and real data limits the generalizability of these methods. Thus, we propose a new task, SCoDA, for the domain adaptation of real scan shape completion from synthetic data. A new dataset, ScanSalon, is contributed with a bunch of elaborate 3D models created by skillful artists according to scans. To address this new task, we propose a novel cross-domain feature fusion method for knowledge transfer and a novel volume-consistent self-training framework for robust learning from real data. Extensive experiments prove our method is effective to bring an improvement of 6%~7% mIoU."
  },
  {
    "Model Unique Name":"TUM-FAADE: Reviewing and enriching point cloud benchmarks for faade segmentation",
    "Category":"3D Vision",
    "Detailed Category":"Point Cloud",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.07140v1.pdf",
    "GitHub":"https:\/\/github.com\/oloocki\/tum-facade",
    "HuggingFace":null,
    "arxiv_id":2304.0714,
    "Summary":"Point clouds are widely regarded as one of the best dataset types for urban mapping purposes. Hence, point cloud datasets are commonly investigated as benchmark types for various urban interpretation methods. Yet, few researchers have addressed the use of point cloud benchmarks for fa\\c{c}ade segmentation. Robust fa\\c{c}ade segmentation is becoming a key factor in various applications ranging from simulating autonomous driving functions to preserving cultural heritage. In this work, we present a method of enriching existing point cloud datasets with fa\\c{c}ade-related classes that have been designed to facilitate fa\\c{c}ade segmentation testing. We propose how to efficiently extend existing datasets and comprehensively assess their potential for fa\\c{c}ade segmentation. We use the method to create the TUM-FA\\c{C}ADE dataset, which extends the capabilities of TUM-MLS-2016. Not only can TUM-FA\\c{C}ADE facilitate the development of point-cloud-based fa\\c{c}ade segmentation tasks, but our procedure can also be applied to enrich further datasets."
  },
  {
    "Model Unique Name":"NeuroMoCo: A Neuromorphic Momentum Contrast Learning Method for Spiking Neural Networks",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2406.06305v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2406.06305,
    "Summary":"Recently, brain-inspired spiking neural networks (SNNs) have attracted great research attention owing to their inherent bio-interpretability, event-triggered properties and powerful perception of spatiotemporal information, which is beneficial to handling event-based neuromorphic datasets. In contrast to conventional static image datasets, event-based neuromorphic datasets present heightened complexity in feature extraction due to their distinctive time series and sparsity characteristics, which influences their classification accuracy. To overcome this challenge, a novel approach termed Neuromorphic Momentum Contrast Learning (NeuroMoCo) for SNNs is introduced in this paper by extending the benefits of self-supervised pre-training to SNNs to effectively stimulate their potential. This is the first time that self-supervised learning (SSL) based on momentum contrastive learning is realized in SNNs. In addition, we devise a novel loss function named MixInfoNCE tailored to their temporal characteristics to further increase the classification accuracy of neuromorphic datasets, which is verified through rigorous ablation experiments. Finally, experiments on DVS-CIFAR10, DVS128Gesture and N-Caltech101 have shown that NeuroMoCo of this paper establishes new state-of-the-art (SOTA) benchmarks: 83.6% (Spikformer-2-256), 98.62% (Spikformer-2-256), and 84.4% (SEW-ResNet-18), respectively."
  },
  {
    "Model Unique Name":"Direct Visual Servoing Based on Discrete Orthogonal Moments",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.14012v5.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2304.14012,
    "Summary":"This paper proposes a new approach to achieve direct visual servoing (DVS) based on discrete orthogonal moments (DOMs). DVS is performed in such a way that the extraction of geometric primitives, matching, and tracking steps in the conventional feature-based visual servoing pipeline can be bypassed. Although DVS enables highly precise positioning, it suffers from a limited convergence domain and poor robustness due to the extreme nonlinearity of the cost function to be minimized and the presence of redundant data between visual features. To tackle these issues, we propose a generic and augmented framework that considers DOMs as visual features. By using the Tchebichef, Krawtchouk, and Hahn moments as examples, we not only present the strategies for adaptively tuning the parameters and order of the visual features but also exhibit an analytical formulation of the associated interaction matrix. Simulations demonstrate the robustness and accuracy of our approach, as well as its advantages over the state-of-the-art. Real-world experiments have also been performed to validate the effectiveness of our approach."
  },
  {
    "Model Unique Name":"Instance Consistency Regularization for Semi-Supervised 3D Instance Segmentation",
    "Category":"Computer Vision",
    "Detailed Category":"Instance Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2406.16776v1.pdf",
    "GitHub":"https:\/\/github.com\/w1zheng\/insteacher3d",
    "HuggingFace":null,
    "arxiv_id":2406.16776,
    "Summary":"Large-scale datasets with point-wise semantic and instance labels are crucial to 3D instance segmentation but also expensive. To leverage unlabeled data, previous semi-supervised 3D instance segmentation approaches have explored self-training frameworks, which rely on high-quality pseudo labels for consistency regularization. They intuitively utilize both instance and semantic pseudo labels in a joint learning manner. However, semantic pseudo labels contain numerous noise derived from the imbalanced category distribution and natural confusion of similar but distinct categories, which leads to severe collapses in self-training. Motivated by the observation that 3D instances are non-overlapping and spatially separable, we ask whether we can solely rely on instance consistency regularization for improved semi-supervised segmentation. To this end, we propose a novel self-training network InsTeacher3D to explore and exploit pure instance knowledge from unlabeled data. We first build a parallel base 3D instance segmentation model DKNet, which distinguishes each instance from the others via discriminative instance kernels without reliance on semantic segmentation. Based on DKNet, we further design a novel instance consistency regularization framework to generate and leverage high-quality instance pseudo labels. Experimental results on multiple large-scale datasets show that the InsTeacher3D significantly outperforms prior state-of-the-art semi-supervised approaches. Code is available: https:\/\/github.com\/W1zheng\/InsTeacher3D."
  },
  {
    "Model Unique Name":"Expanding Training Data for Endoscopic Phenotyping of Eosinophilic Esophagitis",
    "Category":"Computer Vision",
    "Detailed Category":"Image Classification",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2502.04199v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2502.04199,
    "Summary":"Eosinophilic esophagitis (EoE) is a chronic esophageal disorder marked by eosinophil-dominated inflammation. Diagnosing EoE usually involves endoscopic inspection of the esophageal mucosa and obtaining esophageal biopsies for histologic confirmation. Recent advances have seen AI-assisted endoscopic imaging, guided by the EREFS system, emerge as a potential alternative to reduce reliance on invasive histological assessments. Despite these advancements, significant challenges persist due to the limited availability of data for training AI models - a common issue even in the development of AI for more prevalent diseases. This study seeks to improve the performance of deep learning-based EoE phenotype classification by augmenting our training data with a diverse set of images from online platforms, public datasets, and electronic textbooks increasing our dataset from 435 to 7050 images. We utilized the Data-efficient Image Transformer for image classification and incorporated attention map visualizations to boost interpretability. The findings show that our expanded dataset and model enhancements improved diagnostic accuracy, robustness, and comprehensive analysis, enhancing patient outcomes."
  },
  {
    "Model Unique Name":"ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model",
    "Category":"Transformer",
    "Detailed Category":"Transformer",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2408.16767v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2408.16767,
    "Summary":"Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability."
  },
  {
    "Model Unique Name":"Uncertainty Quantification for Bird's Eye View Semantic Segmentation: Methods and Benchmarks",
    "Category":"Computer Vision",
    "Detailed Category":"Semantic Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2405.20986v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2405.20986,
    "Summary":"The fusion of raw sensor data to create a Bird's Eye View (BEV) representation is critical for autonomous vehicle planning and control. Despite the growing interest in using deep learning models for BEV semantic segmentation, anticipating segmentation errors and enhancing the explainability of these models remain underexplored. This paper introduces a comprehensive benchmark for predictive uncertainty quantification in BEV segmentation, evaluating multiple uncertainty quantification methods across three popular datasets with three representative network architectures. Our study focuses on the effectiveness of quantified uncertainty in detecting misclassified and out-of-distribution (OOD) pixels while also improving model calibration. Through empirical analysis, we uncover challenges in existing uncertainty quantification methods and demonstrate the potential of evidential deep learning techniques, which capture both aleatoric and epistemic uncertainty. To address these challenges, we propose a novel loss function, Uncertainty-Focal-Cross-Entropy (UFCE), specifically designed for highly imbalanced data, along with a simple uncertainty-scaling regularization term that improves both uncertainty quantification and model calibration for BEV segmentation."
  },
  {
    "Model Unique Name":"Linking data separation, visual separation, and classifier performance using pseudo-labeling by contrastive learning",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2302.02663v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2302.02663,
    "Summary":"Lacking supervised data is an issue while training deep neural networks (DNNs), mainly when considering medical and biological data where supervision is expensive. Recently, Embedded Pseudo-Labeling (EPL) addressed this problem by using a non-linear projection (t-SNE) from a feature space of the DNN to a 2D space, followed by semi-supervised label propagation using a connectivity-based method (OPFSemi). We argue that the performance of the final classifier depends on the data separation present in the latent space and visual separation present in the projection. We address this by first proposing to use contrastive learning to produce the latent space for EPL by two methods (SimCLR and SupCon) and by their combination, and secondly by showing, via an extensive set of experiments, the aforementioned correlations between data separation, visual separation, and classifier performance. We demonstrate our results by the classification of five real-world challenging image datasets of human intestinal parasites with only 1% supervised samples."
  },
  {
    "Model Unique Name":"CORI: CJKV Benchmark with Romanization Integration -- A step towards Cross-lingual Transfer Beyond Textual Scripts",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.12618v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.12618,
    "Summary":"Naively assuming English as a source language may hinder cross-lingual transfer for many languages by failing to consider the importance of language contact. Some languages are more well-connected than others, and target languages can benefit from transferring from closely related languages; for many languages, the set of closely related languages does not include English. In this work, we study the impact of source language for cross-lingual transfer, demonstrating the importance of selecting source languages that have high contact with the target language. We also construct a novel benchmark dataset for close contact Chinese-Japanese-Korean-Vietnamese (CJKV) languages to further encourage in-depth studies of language contact. To comprehensively capture contact between these languages, we propose to integrate Romanized transcription beyond textual scripts via Contrastive Learning objectives, leading to enhanced cross-lingual representations and effective zero-shot cross-lingual transfer."
  },
  {
    "Model Unique Name":"Intrinsic Gaussian Vector Fields on Manifolds",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2310.18824v2.pdf",
    "GitHub":"https:\/\/github.com\/danielrobertnicoud\/imv-gps",
    "HuggingFace":null,
    "arxiv_id":2310.18824,
    "Summary":"Various applications ranging from robotics to climate science require modeling signals on non-Euclidean domains, such as the sphere. Gaussian process models on manifolds have recently been proposed for such tasks, in particular when uncertainty quantification is needed. In the manifold setting, vector-valued signals can behave very differently from scalar-valued ones, with much of the progress so far focused on modeling the latter. The former, however, are crucial for many applications, such as modeling wind speeds or force fields of unknown dynamical systems. In this paper, we propose novel Gaussian process models for vector-valued signals on manifolds that are intrinsically defined and account for the geometry of the space in consideration. We provide computational primitives needed to deploy the resulting Hodge-Mat\\'ern Gaussian vector fields on the two-dimensional sphere and the hypertori. Further, we highlight two generalization directions: discrete two-dimensional meshes and \"ideal\" manifolds like hyperspheres, Lie groups, and homogeneous spaces. Finally, we show that our Gaussian vector fields constitute considerably more refined inductive biases than the extrinsic fields proposed before."
  },
  {
    "Model Unique Name":"3D GANs and Latent Space: A comprehensive survey",
    "Category":"3D Vision",
    "Detailed Category":"Point Cloud",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.03932v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2304.03932,
    "Summary":"Generative Adversarial Networks (GANs) have emerged as a significant player in generative modeling by mapping lower-dimensional random noise to higher-dimensional spaces. These networks have been used to generate high-resolution images and 3D objects. The efficient modeling of 3D objects and human faces is crucial in the development process of 3D graphical environments such as games or simulations. 3D GANs are a new type of generative model used for 3D reconstruction, point cloud reconstruction, and 3D semantic scene completion. The choice of distribution for noise is critical as it represents the latent space. Understanding a GAN's latent space is essential for fine-tuning the generated samples, as demonstrated by the morphing of semantically meaningful parts of images. In this work, we explore the latent space and 3D GANs, examine several GAN variants and training methods to gain insights into improving 3D GAN training, and suggest potential future directions for further research."
  },
  {
    "Model Unique Name":"General Method for Solving Four Types of SAT Problems",
    "Category":"Reinforcement Learning",
    "Detailed Category":"Reinforcement Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2312.16423v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2312.16423,
    "Summary":"Existing methods provide varying algorithms for different types of Boolean satisfiability problems (SAT), lacking a general solution framework. Accordingly, this study proposes a unified framework DCSAT based on integer programming and reinforcement learning (RL) algorithm to solve different types of SAT problems such as MaxSAT, Weighted MaxSAT, PMS, WPMS. Specifically, we first construct a consolidated integer programming representation for four types of SAT problems by adjusting objective function coefficients. Secondly, we construct an appropriate reinforcement learning models based on the 0-1 integer programming for SAT problems. Based on the binary tree search structure, we apply the Monte Carlo tree search (MCTS) method on SAT problems. Finally, we prove that this method can find all optimal Boolean assignments based on Wiener-khinchin law of large Numbers. We experimentally verify that this paradigm can prune the unnecessary search space to find the optimal Boolean assignments for the problem. Furthermore, the proposed method can provide diverse labels for supervised learning methods for SAT problems."
  },
  {
    "Model Unique Name":"Towards Open-ended Visual Quality Comparison",
    "Category":"Multi-modal",
    "Detailed Category":"Multi-modal",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2402.16641v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2402.16641,
    "Summary":"Comparative settings (e.g. pairwise choice, listwise ranking) have been adopted by a wide range of subjective studies for image quality assessment (IQA), as it inherently standardizes the evaluation criteria across different observers and offer more clear-cut responses. In this work, we extend the edge of emerging large multi-modality models (LMMs) to further advance visual quality comparison into open-ended settings, that 1) can respond to open-range questions on quality comparison; 2) can provide detailed reasonings beyond direct answers. To this end, we propose the Co-Instruct. To train this first-of-its-kind open-source open-ended visual quality comparer, we collect the Co-Instruct-562K dataset, from two sources: (a) LLM-merged single image quality description, (b) GPT-4V \"teacher\" responses on unlabeled data. Furthermore, to better evaluate this setting, we propose the MICBench, the first benchmark on multi-image comparison for LMMs. We demonstrate that Co-Instruct not only achieves in average 30% higher accuracy than state-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher), on both existing related benchmarks and the proposed MICBench. Our model is published at https:\/\/huggingface.co\/q-future\/co-instruct."
  },
  {
    "Model Unique Name":"TGNN: A Joint Semi-supervised Framework for Graph-level Classification",
    "Category":"Graph Neural Network",
    "Detailed Category":"Graph Neural Network",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.11688v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2304.11688,
    "Summary":"This paper studies semi-supervised graph classification, a crucial task with a wide range of applications in social network analysis and bioinformatics. Recent works typically adopt graph neural networks to learn graph-level representations for classification, failing to explicitly leverage features derived from graph topology (e.g., paths). Moreover, when labeled data is scarce, these methods are far from satisfactory due to their insufficient topology exploration of unlabeled data. We address the challenge by proposing a novel semi-supervised framework called Twin Graph Neural Network (TGNN). To explore graph structural information from complementary views, our TGNN has a message passing module and a graph kernel module. To fully utilize unlabeled data, for each module, we calculate the similarity of each unlabeled graph to other labeled graphs in the memory bank and our consistency loss encourages consistency between two similarity distributions in different embedding spaces. The two twin modules collaborate with each other by exchanging instance similarity knowledge to fully explore the structure information of both labeled and unlabeled data. We evaluate our TGNN on various public datasets and show that it achieves strong performance."
  },
  {
    "Model Unique Name":"DPMix: Mixture of Depth and Point Cloud Video Experts for 4D Action Segmentation",
    "Category":"3D Vision",
    "Detailed Category":"Point Cloud",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2307.16803v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2307.16803,
    "Summary":"In this technical report, we present our findings from the research conducted on the Human-Object Interaction 4D (HOI4D) dataset for egocentric action segmentation task. As a relatively novel research area, point cloud video methods might not be good at temporal modeling, especially for long point cloud videos (\\eg, 150 frames). In contrast, traditional video understanding methods have been well developed. Their effectiveness on temporal modeling has been widely verified on many large scale video datasets. Therefore, we convert point cloud videos into depth videos and employ traditional video modeling methods to improve 4D action segmentation. By ensembling depth and point cloud video methods, the accuracy is significantly improved. The proposed method, named Mixture of Depth and Point cloud video experts (DPMix), achieved the first place in the 4D Action Segmentation Track of the HOI4D Challenge 2023."
  },
  {
    "Model Unique Name":"OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data",
    "Category":"Multi-modal",
    "Detailed Category":"Multi-modal",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2310.13398v1.pdf",
    "GitHub":"https:\/\/github.com\/fudan-projecttitan\/openannotate3d",
    "HuggingFace":null,
    "arxiv_id":2310.13398,
    "Summary":"In the era of big data and large models, automatic annotating functions for multi-modal data are of great significance for real-world AI-driven applications, such as autonomous driving and embodied AI. Unlike traditional closed-set annotation, open-vocabulary annotation is essential to achieve human-level cognition capability. However, there are few open-vocabulary auto-labeling systems for multi-modal 3D data. In this paper, we introduce OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can automatically generate 2D masks, 3D masks, and 3D bounding box annotations for vision and point cloud data. Our system integrates the chain-of-thought capabilities of Large Language Models (LLMs) and the cross-modality capabilities of vision-language models (VLMs). To the best of our knowledge, OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal 3D auto-labeling. We conduct comprehensive evaluations on both public and in-house real-world datasets, which demonstrate that the system significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results."
  },
  {
    "Model Unique Name":"Lessons Learned: The Evolution of an Undergraduate Robotics Course in Computer Science",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.18012v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.18012,
    "Summary":"Seven years ago (2016), we began integrating Robotics into our Computer Science curriculum. This paper explores the mission, initial goals and objectives, specific choices we made along the way, and why and outcomes. Of course, we were not the first to do so. Our contribution in this paper is to describe a seven-year experience in the hope that others going down this road will benefit, perhaps avoiding some missteps and dead-ends. We offer our answers to many questions that anyone undertaking bootstrapping a new robotics program may have to deal with. At the end of the paper, we discuss a set of lessons learned, including striking the right balance between depth and breadth in syllabus design and material organization, the significance of utilizing physical robots and criteria for selecting a suitable robotics platform, insights into the scope and design of a robotics lab, the necessity of standardizing hardware and software configurations, along with implementation methods, and strategies for preparing students for the steep learning curve."
  },
  {
    "Model Unique Name":"Uncertainty-boosted Robust Video Activity Anticipation",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.18648v1.pdf",
    "GitHub":"https:\/\/github.com\/qzhb\/ubrv2a",
    "HuggingFace":null,
    "arxiv_id":2404.18648,
    "Summary":"Video activity anticipation aims to predict what will happen in the future, embracing a broad application prospect ranging from robot vision and autonomous driving. Despite the recent progress, the data uncertainty issue, reflected as the content evolution process and dynamic correlation in event labels, has been somehow ignored. This reduces the model generalization ability and deep understanding on video content, leading to serious error accumulation and degraded performance. In this paper, we address the uncertainty learning problem and propose an uncertainty-boosted robust video activity anticipation framework, which generates uncertainty values to indicate the credibility of the anticipation results. The uncertainty value is used to derive a temperature parameter in the softmax function to modulate the predicted target activity distribution. To guarantee the distribution adjustment, we construct a reasonable target activity label representation by incorporating the activity evolution from the temporal class correlation and the semantic relationship. Moreover, we quantify the uncertainty into relative values by comparing the uncertainty among sample pairs and their temporal-lengths. This relative strategy provides a more accessible way in uncertainty modeling than quantifying the absolute uncertainty values on the whole dataset. Experiments on multiple backbones and benchmarks show our framework achieves promising performance and better robustness\/interpretability. Source codes are available at https:\/\/github.com\/qzhb\/UbRV2A."
  },
  {
    "Model Unique Name":"Debiased Contrastive Representation Learning for Mitigating Dual Biases in Recommender Systems",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2408.09646v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2408.09646,
    "Summary":"In recommender systems, popularity and conformity biases undermine recommender effectiveness by disproportionately favouring popular items, leading to their over-representation in recommendation lists and causing an unbalanced distribution of user-item historical data. We construct a causal graph to address both biases and describe the abstract data generation mechanism. Then, we use it as a guide to develop a novel Debiased Contrastive Learning framework for Mitigating Dual Biases, called DCLMDB. In DCLMDB, both popularity bias and conformity bias are handled in the model training process by contrastive learning to ensure that user choices and recommended items are not unduly influenced by conformity and popularity. Extensive experiments on two real-world datasets, Movielens-10M and Netflix, show that DCLMDB can effectively reduce the dual biases, as well as significantly enhance the accuracy and diversity of recommendations."
  },
  {
    "Model Unique Name":"Visual Mamba: A Survey and New Outlooks",
    "Category":"Multi-modal",
    "Detailed Category":"Multi-modal",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.18861v3.pdf",
    "GitHub":"https:\/\/github.com\/ruixxxx\/awesome-vision-mamba-models",
    "HuggingFace":null,
    "arxiv_id":2404.18861,
    "Summary":"Mamba, a recent selective structured state space model, excels in long sequence modeling, which is vital in the large model era. Long sequence modeling poses significant challenges, including capturing long-range dependencies within the data and handling the computational demands caused by their extensive length. Mamba addresses these challenges by overcoming the local perception limitations of convolutional neural networks and the quadratic computational complexity of Transformers. Given its advantages over these mainstream foundation architectures, Mamba exhibits great potential to be a visual foundation architecture. Since January 2024, Mamba has been actively applied to diverse computer vision tasks, yielding numerous contributions. To help keep pace with the rapid advancements, this paper reviews visual Mamba approaches, analyzing over 200 papers. This paper begins by delineating the formulation of the original Mamba model. Subsequently, it delves into representative backbone networks, and applications categorized using different modalities, including image, video, point cloud, and multi-modal data. Particularly, we identify scanning techniques as critical for adapting Mamba to vision tasks, and decouple these scanning techniques to clarify their functionality and enhance their flexibility across various applications. Finally, we discuss the challenges and future directions, providing insights into new outlooks in this fast evolving area. A comprehensive list of visual Mamba models reviewed in this work is available at https:\/\/github.com\/Ruixxxx\/Awesome-Vision-Mamba-Models."
  },
  {
    "Model Unique Name":"CLFT: Camera-LiDAR Fusion Transformer for Semantic Segmentation in Autonomous Driving",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.17793v3.pdf",
    "GitHub":"https:\/\/github.com\/claud1234\/fcn_transformer_object_segmentation",
    "HuggingFace":null,
    "arxiv_id":2404.17793,
    "Summary":"Critical research about camera-and-LiDAR-based semantic object segmentation for autonomous driving significantly benefited from the recent development of deep learning. Specifically, the vision transformer is the novel ground-breaker that successfully brought the multi-head-attention mechanism to computer vision applications. Therefore, we propose a vision-transformer-based network to carry out camera-LiDAR fusion for semantic segmentation applied to autonomous driving. Our proposal uses the novel progressive-assemble strategy of vision transformers on a double-direction network and then integrates the results in a cross-fusion strategy over the transformer decoder layers. Unlike other works in the literature, our camera-LiDAR fusion transformers have been evaluated in challenging conditions like rain and low illumination, showing robust performance. The paper reports the segmentation results over the vehicle and human classes in different modalities: camera-only, LiDAR-only, and camera-LiDAR fusion. We perform coherent controlled benchmark experiments of CLFT against other networks that are also designed for semantic segmentation. The experiments aim to evaluate the performance of CLFT independently from two perspectives: multimodal sensor fusion and backbone architectures. The quantitative assessments show our CLFT networks yield an improvement of up to 10% for challenging dark-wet conditions when comparing with Fully-Convolutional-Neural-Network-based (FCN) camera-LiDAR fusion neural network. Contrasting to the network with transformer backbone but using single modality input, the all-around improvement is 5-10%."
  },
  {
    "Model Unique Name":"BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling",
    "Category":"Robotics",
    "Detailed Category":"SFM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2403.04926v2.pdf",
    "GitHub":"https:\/\/github.com\/snldmt\/bags",
    "HuggingFace":null,
    "arxiv_id":2403.04926,
    "Summary":"Recent efforts in using 3D Gaussians for scene reconstruction and novel view synthesis can achieve impressive results on curated benchmarks; however, images captured in real life are often blurry. In this work, we analyze the robustness of Gaussian-Splatting-based methods against various image blur, such as motion blur, defocus blur, downscaling blur, \\etc. Under these degradations, Gaussian-Splatting-based methods tend to overfit and produce worse results than Neural-Radiance-Field-based methods. To address this issue, we propose Blur Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling capacities such that a 3D-consistent and high quality scene can be reconstructed despite image-wise blur. Specifically, we model blur by estimating per-pixel convolution kernels from a Blur Proposal Network (BPN). BPN is designed to consider spatial, color, and depth variations of the scene to maximize modeling capacity. Additionally, BPN also proposes a quality-assessing mask, which indicates regions where blur occur. Finally, we introduce a coarse-to-fine kernel optimization scheme; this optimization scheme is fast and avoids sub-optimal solutions due to a sparse point cloud initialization, which often occurs when we apply Structure-from-Motion on blurry images. We demonstrate that BAGS achieves photorealistic renderings under various challenging blur conditions and imaging geometry, while significantly improving upon existing approaches."
  },
  {
    "Model Unique Name":"An Overview of Machine Learning-Driven Resource Allocation in IoT Networks",
    "Category":"Reinforcement Learning",
    "Detailed Category":"Reinforcement Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2412.19478v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2412.19478,
    "Summary":"In the wake of disruptive IoT technologies generating massive amounts of diverse data, Machine Learning (ML) will play a crucial role in bringing intelligence to Internet of Things (IoT) networks. This paper provides a comprehensive analysis of the current state of resource allocation within IoT networks, focusing specifically on two key categories: Low-Power IoT Networks and Mobile IoT Networks. We delve into the resource allocation strategies that are crucial for optimizing network performance and energy efficiency in these environments. Furthermore, the paper explores the transformative role of Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in enhancing IoT functionalities. We highlight a range of applications and use cases where these advanced technologies can significantly improve decision-making and optimization processes. In addition to the opportunities presented by ML, DL, and RL, we also address the potential challenges that organizations may face when implementing these technologies in IoT settings. These challenges include crucial accuracy, low flexibility and adaptability, and high computational cost, etc. Finally, the paper identifies promising avenues for future research, emphasizing the need for innovative solutions to overcome existing hurdles and improve the integration of ML, DL, and RL into IoT networks. By providing this holistic perspective, we aim to contribute to the ongoing discourse on resource allocation strategies and the application of intelligent technologies in the IoT landscape."
  },
  {
    "Model Unique Name":"Initialization Matters for Adversarial Transfer Learning",
    "Category":"Computer Vision",
    "Detailed Category":"Image Classification",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2312.05716v2.pdf",
    "GitHub":"https:\/\/github.com\/dongxzz\/roli",
    "HuggingFace":null,
    "arxiv_id":2312.05716,
    "Summary":"With the prevalence of the Pretraining-Finetuning paradigm in transfer learning, the robustness of downstream tasks has become a critical concern. In this work, we delve into adversarial robustness in transfer learning and reveal the critical role of initialization, including both the pretrained model and the linear head. First, we discover the necessity of an adversarially robust pretrained model. Specifically, we reveal that with a standard pretrained model, Parameter-Efficient Finetuning (PEFT) methods either fail to be adversarially robust or continue to exhibit significantly degraded adversarial robustness on downstream tasks, even with adversarial training during finetuning. Leveraging a robust pretrained model, surprisingly, we observe that a simple linear probing can outperform full finetuning and other PEFT methods with random initialization on certain datasets. We further identify that linear probing excels in preserving robustness from the robust pretraining. Based on this, we propose Robust Linear Initialization (RoLI) for adversarial finetuning, which initializes the linear head with the weights obtained by adversarial linear probing to maximally inherit the robustness from pretraining. Across five different image classification datasets, we demonstrate the effectiveness of RoLI and achieve new state-of-the-art results. Our code is available at \\url{https:\/\/github.com\/DongXzz\/RoLI}."
  },
  {
    "Model Unique Name":"Iterative Reward Shaping using Human Feedback for Correcting Reward Misspecification",
    "Category":"Reinforcement Learning",
    "Detailed Category":"Reinforcement Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2308.15969v1.pdf",
    "GitHub":"https:\/\/github.com\/anonymous902109\/iters",
    "HuggingFace":null,
    "arxiv_id":2308.15969,
    "Summary":"A well-defined reward function is crucial for successful training of an reinforcement learning (RL) agent. However, defining a suitable reward function is a notoriously challenging task, especially in complex, multi-objective environments. Developers often have to resort to starting with an initial, potentially misspecified reward function, and iteratively adjusting its parameters, based on observed learned behavior. In this work, we aim to automate this process by proposing ITERS, an iterative reward shaping approach using human feedback for mitigating the effects of a misspecified reward function. Our approach allows the user to provide trajectory-level feedback on agent's behavior during training, which can be integrated as a reward shaping signal in the following training iteration. We also allow the user to provide explanations of their feedback, which are used to augment the feedback and reduce user effort and feedback frequency. We evaluate ITERS in three environments and show that it can successfully correct misspecified reward functions."
  },
  {
    "Model Unique Name":"SoccerNet 2023 Challenges Results",
    "Category":"Multi-modal",
    "Detailed Category":"Image Caption",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2309.06006v1.pdf",
    "GitHub":"https:\/\/github.com\/lRomul\/ball-action-spotting",
    "HuggingFace":null,
    "arxiv_id":2309.06006,
    "Summary":"The SoccerNet 2023 challenges were the third annual video understanding challenges organized by the SoccerNet team. For this third edition, the challenges were composed of seven vision-based tasks split into three main themes. The first theme, broadcast video understanding, is composed of three high-level tasks related to describing events occurring in the video broadcasts: (1) action spotting, focusing on retrieving all timestamps related to global actions in soccer, (2) ball action spotting, focusing on retrieving all timestamps related to the soccer ball change of state, and (3) dense video captioning, focusing on describing the broadcast with natural language and anchored timestamps. The second theme, field understanding, relates to the single task of (4) camera calibration, focusing on retrieving the intrinsic and extrinsic camera parameters from images. The third and last theme, player understanding, is composed of three low-level tasks related to extracting information about the players: (5) re-identification, focusing on retrieving the same players across multiple views, (6) multiple object tracking, focusing on tracking players and the ball through unedited video streams, and (7) jersey number recognition, focusing on recognizing the jersey number of players from tracklets. Compared to the previous editions of the SoccerNet challenges, tasks (2-3-7) are novel, including new annotations and data, task (4) was enhanced with more data and annotations, and task (6) now focuses on end-to-end approaches. More information on the tasks, challenges, and leaderboards are available on https:\/\/www.soccer-net.org. Baselines and development kits can be found on https:\/\/github.com\/SoccerNet."
  },
  {
    "Model Unique Name":"Maritime Vessel Tank Inspection using Aerial Robots: Experience from the field and dataset release",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2404.19045v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2404.19045,
    "Summary":"This paper presents field results and lessons learned from the deployment of aerial robots inside ship ballast tanks. Vessel tanks including ballast tanks and cargo holds present dark, dusty environments having simultaneously very narrow openings and wide open spaces that create several challenges for autonomous navigation and inspection operations. We present a system for vessel tank inspection using an aerial robot along with its autonomy modules. We show the results of autonomous exploration and visual inspection in 3 ships spanning across 7 distinct types of sections of the ballast tanks. Additionally, we comment on the lessons learned from the field and possible directions for future work. Finally, we release a dataset consisting of the data from these missions along with data collected with a handheld sensor stick."
  },
  {
    "Model Unique Name":"Structured Spatial Reasoning with Open Vocabulary Object Detectors",
    "Category":"Computer Vision",
    "Detailed Category":"Object Detection",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2410.07394v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2410.07394,
    "Summary":"Reasoning about spatial relationships between objects is essential for many real-world robotic tasks, such as fetch-and-delivery, object rearrangement, and object search. The ability to detect and disambiguate different objects and identify their location is key to successful completion of these tasks. Several recent works have used powerful Vision and Language Models (VLMs) to unlock this capability in robotic agents. In this paper we introduce a structured probabilistic approach that integrates rich 3D geometric features with state-of-the-art open-vocabulary object detectors to enhance spatial reasoning for robotic perception. The approach is evaluated and compared against zero-shot performance of the state-of-the-art Vision and Language Models (VLMs) on spatial reasoning tasks. To enable this comparison, we annotate spatial clauses in real-world RGB-D Active Vision Dataset [1] and conduct experiments on this and the synthetic Semantic Abstraction [2] dataset. Results demonstrate the effectiveness of the proposed method, showing superior performance of grounding spatial relations over state of the art open-source VLMs by more than 20%."
  },
  {
    "Model Unique Name":"ConCSE: Unified Contrastive Learning and Augmentation for Code-Switched Embeddings",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2409.00120v2.pdf",
    "GitHub":"https:\/\/github.com\/jjy961228\/ConCSE",
    "HuggingFace":null,
    "arxiv_id":2409.0012,
    "Summary":"This paper examines the Code-Switching (CS) phenomenon where two languages intertwine within a single utterance. There exists a noticeable need for research on the CS between English and Korean. We highlight that the current Equivalence Constraint (EC) theory for CS in other languages may only partially capture English-Korean CS complexities due to the intrinsic grammatical differences between the languages. We introduce a novel Koglish dataset tailored for English-Korean CS scenarios to mitigate such challenges. First, we constructed the Koglish-GLUE dataset to demonstrate the importance and need for CS datasets in various tasks. We found the differential outcomes of various foundation multilingual language models when trained on a monolingual versus a CS dataset. Motivated by this, we hypothesized that SimCSE, which has shown strengths in monolingual sentence embedding, would have limitations in CS scenarios. We construct a novel Koglish-NLI (Natural Language Inference) dataset using a CS augmentation-based approach to verify this. From this CS-augmented dataset Koglish-NLI, we propose a unified contrastive learning and augmentation method for code-switched embeddings, ConCSE, highlighting the semantics of CS sentences. Experimental results validate the proposed ConCSE with an average performance enhancement of 1.77\\% on the Koglish-STS(Semantic Textual Similarity) tasks."
  },
  {
    "Model Unique Name":"KITE: Keypoint-Conditioned Policies for Semantic Manipulation",
    "Category":"Robotics",
    "Detailed Category":"Robotics",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2306.16605v4.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2306.16605,
    "Summary":"While natural language offers a convenient shared interface for humans and robots, enabling robots to interpret and follow language commands remains a longstanding challenge in manipulation. A crucial step to realizing a performant instruction-following robot is achieving semantic manipulation, where a robot interprets language at different specificities, from high-level instructions like \"Pick up the stuffed animal\" to more detailed inputs like \"Grab the left ear of the elephant.\" To tackle this, we propose Keypoints + Instructions to Execution (KITE), a two-step framework for semantic manipulation which attends to both scene semantics (distinguishing between different objects in a visual scene) and object semantics (precisely localizing different parts within an object instance). KITE first grounds an input instruction in a visual scene through 2D image keypoints, providing a highly accurate object-centric bias for downstream action inference. Provided an RGB-D scene observation, KITE then executes a learned keypoint-conditioned skill to carry out the instruction. The combined precision of keypoints and parameterized skills enables fine-grained manipulation with generalization to scene and object variations. Empirically, we demonstrate KITE in 3 real-world environments: long-horizon 6-DoF tabletop manipulation, semantic grasping, and a high-precision coffee-making task. In these settings, KITE achieves a 75%, 70%, and 71% overall success rate for instruction-following, respectively. KITE outperforms frameworks that opt for pre-trained visual language models over keypoint-based grounding, or omit skills in favor of end-to-end visuomotor control, all while being trained from fewer or comparable amounts of demonstrations. Supplementary material, datasets, code, and videos can be found on our website: http:\/\/tinyurl.com\/kite-site."
  },
  {
    "Model Unique Name":"ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model",
    "Category":"Computer Vision",
    "Detailed Category":"Object Tracking",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2411.01756v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2411.01756,
    "Summary":"Visual object tracking aims to locate a targeted object in a video sequence based on an initial bounding box. Recently, Vision-Language~(VL) trackers have proposed to utilize additional natural language descriptions to enhance versatility in various applications. However, VL trackers are still inferior to State-of-The-Art (SoTA) visual trackers in terms of tracking performance. We found that this inferiority primarily results from their heavy reliance on manual textual annotations, which include the frequent provision of ambiguous language descriptions. In this paper, we propose ChatTracker to leverage the wealth of world knowledge in the Multimodal Large Language Model (MLLM) to generate high-quality language descriptions and enhance tracking performance. To this end, we propose a novel reflection-based prompt optimization module to iteratively refine the ambiguous and inaccurate descriptions of the target with tracking feedback. To further utilize semantic information produced by MLLM, a simple yet effective VL tracking framework is proposed and can be easily integrated as a plug-and-play module to boost the performance of both VL and visual trackers. Experimental results show that our proposed ChatTracker achieves a performance comparable to existing methods."
  },
  {
    "Model Unique Name":"Crossover Designs in Software Engineering Experiments: Review of the State of Analysis",
    "Category":"Multi-modal",
    "Detailed Category":"VQA",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2408.07594v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2408.07594,
    "Summary":"Experimentation is an essential method for causal inference in any empirical discipline. Crossover-design experiments are common in Software Engineering (SE) research. In these, subjects apply more than one treatment in different orders. This design increases the amount of obtained data and deals with subject variability but introduces threats to internal validity like the learning and carryover effect. Vegas et al. reviewed the state of practice for crossover designs in SE research and provided guidelines on how to address its threats during data analysis while still harnessing its benefits. In this paper, we reflect on the impact of these guidelines and review the state of analysis of crossover design experiments in SE publications between 2015 and March 2024. To this end, by conducting a forward snowballing of the guidelines, we survey 136 publications reporting 67 crossover-design experiments and evaluate their data analysis against the provided guidelines. The results show that the validity of data analyses has improved compared to the original state of analysis. Still, despite the explicit guidelines, only 29.5% of all threats to validity were addressed properly. While the maturation and the optimal sequence threats are properly addressed in 35.8% and 38.8% of all studies in our sample respectively, the carryover threat is only modeled in about 3% of the observed cases. The lack of adherence to the analysis guidelines threatens the validity of the conclusions drawn from crossover design experiments"
  },
  {
    "Model Unique Name":"Delving into Shape-aware Zero-shot Semantic Segmentation",
    "Category":"Computer Vision",
    "Detailed Category":"Semantic Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2304.08491v1.pdf",
    "GitHub":"https:\/\/github.com\/liuxinyv\/sazs",
    "HuggingFace":null,
    "arxiv_id":2304.08491,
    "Summary":"Thanks to the impressive progress of large-scale vision-language pretraining, recent recognition models can classify arbitrary objects in a zero-shot and open-set manner, with a surprisingly high accuracy. However, translating this success to semantic segmentation is not trivial, because this dense prediction task requires not only accurate semantic understanding but also fine shape delineation and existing vision-language models are trained with image-level language descriptions. To bridge this gap, we pursue \\textbf{shape-aware} zero-shot semantic segmentation in this study. Inspired by classical spectral methods in the image segmentation literature, we propose to leverage the eigen vectors of Laplacian matrices constructed with self-supervised pixel-wise features to promote shape-awareness. Despite that this simple and effective technique does not make use of the masks of seen classes at all, we demonstrate that it out-performs a state-of-the-art shape-aware formulation that aligns ground truth and predicted edges during training. We also delve into the performance gains achieved on different datasets using different backbones and draw several interesting and conclusive observations: the benefits of promoting shape-awareness highly relates to mask compactness and language embedding locality. Finally, our method sets new state-of-the-art performance for zero-shot semantic segmentation on both Pascal and COCO, with significant margins. Code and models will be accessed at https:\/\/github.com\/Liuxinyv\/SAZS."
  },
  {
    "Model Unique Name":"Variable Radiance Field for Real-World Category-Specific Reconstruction from Single Image",
    "Category":"Contrastive Learning",
    "Detailed Category":"Contrastive Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2306.05145v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2306.05145,
    "Summary":"Reconstructing category-specific objects using Neural Radiance Field (NeRF) from a single image is a promising yet challenging task. Existing approaches predominantly rely on projection-based feature retrieval to associate 3D points in the radiance field with local image features from the reference image. However, this process is computationally expensive, dependent on known camera intrinsics, and susceptible to occlusions. To address these limitations, we propose Variable Radiance Field (VRF), a novel framework capable of efficiently reconstructing category-specific objects without requiring known camera intrinsics and demonstrating robustness against occlusions. First, we replace the local feature retrieval with global latent representations, generated through a single feed-forward pass, which improves efficiency and eliminates reliance on camera intrinsics. Second, to tackle coordinate inconsistencies inherent in real-world dataset, we define a canonical space by introducing a learnable, category-specific shape template and explicitly aligning each training object to this template using a learnable 3D transformation. This approach also reduces the complexity of geometry prediction to modeling deformations from the template to individual instances. Finally, we employ a hyper-network-based method for efficient NeRF creation and enhance the reconstruction performance through a contrastive learning-based pretraining strategy. Evaluations on the CO3D dataset demonstrate that VRF achieves state-of-the-art performance in both reconstruction quality and computational efficiency."
  },
  {
    "Model Unique Name":"Lie-Poisson Neural Networks (LPNets): Data-Based Computing of Hamiltonian Systems with Symmetries",
    "Category":"Transformer",
    "Detailed Category":"Transformer",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2308.15349v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2308.15349,
    "Summary":"An accurate data-based prediction of the long-term evolution of Hamiltonian systems requires a network that preserves the appropriate structure under each time step. Every Hamiltonian system contains two essential ingredients: the Poisson bracket and the Hamiltonian. Hamiltonian systems with symmetries, whose paradigm examples are the Lie-Poisson systems, have been shown to describe a broad category of physical phenomena, from satellite motion to underwater vehicles, fluids, geophysical applications, complex fluids, and plasma physics. The Poisson bracket in these systems comes from the symmetries, while the Hamiltonian comes from the underlying physics. We view the symmetry of the system as primary, hence the Lie-Poisson bracket is known exactly, whereas the Hamiltonian is regarded as coming from physics and is considered not known, or known approximately. Using this approach, we develop a network based on transformations that exactly preserve the Poisson bracket and the special functions of the Lie-Poisson systems (Casimirs) to machine precision. We present two flavors of such systems: one, where the parameters of transformations are computed from data using a dense neural network (LPNets), and another, where the composition of transformations is used as building blocks (G-LPNets). We also show how to adapt these methods to a larger class of Poisson brackets. We apply the resulting methods to several examples, such as rigid body (satellite) motion, underwater vehicles, a particle in a magnetic field, and others. The methods developed in this paper are important for the construction of accurate data-based methods for simulating the long-term dynamics of physical systems."
  },
  {
    "Model Unique Name":"VR-SLAM: A Visual-Range Simultaneous Localization and Mapping System using Monocular Camera and Ultra-wideband Sensors",
    "Category":"Robotics",
    "Detailed Category":"SLAM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2303.10903v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2303.10903,
    "Summary":"In this work, we propose a simultaneous localization and mapping (SLAM) system using a monocular camera and Ultra-wideband (UWB) sensors. Our system, referred to as VRSLAM, is a multi-stage framework that leverages the strengths and compensates for the weaknesses of each sensor. Firstly, we introduce a UWB-aided 7 degree-of-freedom (scale factor, 3D position, and 3D orientation) global alignment module to initialize the visual odometry (VO) system in the world frame defined by the UWB anchors. This module loosely fuses up-to-scale VO and ranging data using either a quadratically constrained quadratic programming (QCQP) or nonlinear least squares (NLS) algorithm based on whether a good initial guess is available. Secondly, we provide an accompanied theoretical analysis that includes the derivation and interpretation of the Fisher Information Matrix (FIM) and its determinant. Thirdly, we present UWBaided bundle adjustment (UBA) and UWB-aided pose graph optimization (UPGO) modules to improve short-term odometry accuracy, reduce long-term drift as well as correct any alignment and scale errors. Extensive simulations and experiments show that our solution outperforms UWB\/camera-only and previous approaches, can quickly recover from tracking failure without relying on visual relocalization, and can effortlessly obtain a global map even if there are no loop closures."
  },
  {
    "Model Unique Name":"Robust Incremental Structure-from-Motion with Hybrid Features",
    "Category":"Robotics",
    "Detailed Category":"SFM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2409.19811v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2409.19811,
    "Summary":"Structure-from-Motion (SfM) has become a ubiquitous tool for camera calibration and scene reconstruction with many downstream applications in computer vision and beyond. While the state-of-the-art SfM pipelines have reached a high level of maturity in well-textured and well-configured scenes over the last decades, they still fall short of robustly solving the SfM problem in challenging scenarios. In particular, weakly textured scenes and poorly constrained configurations oftentimes cause catastrophic failures or large errors for the primarily keypoint-based pipelines. In these scenarios, line segments are often abundant and can offer complementary geometric constraints. Their large spatial extent and typically structured configurations lead to stronger geometric constraints as compared to traditional keypoint-based methods. In this work, we introduce an incremental SfM system that, in addition to points, leverages lines and their structured geometric relations. Our technical contributions span the entire pipeline (mapping, triangulation, registration) and we integrate these into a comprehensive end-to-end SfM system that we share as an open-source software with the community. We also present the first analytical method to propagate uncertainties for 3D optimized lines via sensitivity analysis. Experiments show that our system is consistently more robust and accurate compared to the widely used point-based state of the art in SfM -- achieving richer maps and more precise camera registrations, especially under challenging conditions. In addition, our uncertainty-aware localization module alone is able to consistently improve over the state of the art under both point-alone and hybrid setups."
  },
  {
    "Model Unique Name":"Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification",
    "Category":"Computer Vision",
    "Detailed Category":"Image Classification",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2306.04037v2.pdf",
    "GitHub":"https:\/\/github.com\/peeples-lab\/xai_analysis",
    "HuggingFace":null,
    "arxiv_id":2306.04037,
    "Summary":"We present a comprehensive analysis of quantitatively evaluating explainable artificial intelligence (XAI) techniques for remote sensing image classification. Our approach leverages state-of-the-art machine learning approaches to perform remote sensing image classification across multiple modalities. We investigate the results of the models qualitatively through XAI methods. Additionally, we compare the XAI methods quantitatively through various categories of desired properties. Through our analysis, we offer insights and recommendations for selecting the most appropriate XAI method(s) to gain a deeper understanding of the models' decision-making processes. The code for this work is publicly available."
  },
  {
    "Model Unique Name":"FPCD: An Open Aerial VHR Dataset for Farm Pond Change Detection",
    "Category":"Computer Vision",
    "Detailed Category":"Instance Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2302.14554v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2302.14554,
    "Summary":"Change detection for aerial imagery involves locating and identifying changes associated with the areas of interest between co-registered bi-temporal or multi-temporal images of a geographical location. Farm ponds are man-made structures belonging to the category of minor irrigation structures used to collect surface run-off water for future irrigation purposes. Detection of farm ponds from aerial imagery and their evolution over time helps in land surveying to analyze the agricultural shifts, policy implementation, seasonal effects and climate changes. In this paper, we introduce a publicly available object detection and instance segmentation (OD\/IS) dataset for localizing farm ponds from aerial imagery. We also collected and annotated the bi-temporal data over a time-span of 14 years across 17 villages, resulting in a binary change detection dataset called \\textbf{F}arm \\textbf{P}ond \\textbf{C}hange \\textbf{D}etection Dataset (\\textbf{FPCD}). We have benchmarked and analyzed the performance of various object detection and instance segmentation methods on our OD\/IS dataset and the change detection methods over the FPCD dataset. The datasets are publicly accessible at this page: \\textit{\\url{https:\/\/huggingface.co\/datasets\/ctundia\/FPCD}}"
  },
  {
    "Model Unique Name":"Multicam-SLAM: Non-overlapping Multi-camera SLAM for Indirect Visual Localization and Navigation",
    "Category":"Robotics",
    "Detailed Category":"SLAM",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2406.06374v2.pdf",
    "GitHub":"https:\/\/github.com\/alterpang\/multi_orb_slam",
    "HuggingFace":null,
    "arxiv_id":2406.06374,
    "Summary":"This paper presents a novel approach to visual simultaneous localization and mapping (SLAM) using multiple RGB-D cameras. The proposed method, Multicam-SLAM, significantly enhances the robustness and accuracy of SLAM systems by capturing more comprehensive spatial information from various perspectives. This method enables the accurate determination of pose relationships among multiple cameras without the need for overlapping fields of view. The proposed Muticam-SLAM includes a unique multi-camera model, a multi-keyframes structure, and several parallel SLAM threads. The multi-camera model allows for the integration of data from multiple cameras, while the multi-keyframes and parallel SLAM threads ensure efficient and accurate pose estimation and mapping. Extensive experiments in various environments demonstrate the superior accuracy and robustness of the proposed method compared to conventional single-camera SLAM systems. The results highlight the potential of the proposed Multicam-SLAM for more complex and challenging applications. Code is available at \\url{https:\/\/github.com\/AlterPang\/Multi_ORB_SLAM}."
  },
  {
    "Model Unique Name":"Artificial Intelligence to Assess Dental Findings from Panoramic Radiographs -- A Multinational Study",
    "Category":"Computer Vision",
    "Detailed Category":"Semantic Segmentation",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2502.10277v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2502.10277,
    "Summary":"Dental panoramic radiographs (DPRs) are widely used in clinical practice for comprehensive oral assessment but present challenges due to overlapping structures and time constraints in interpretation.   This study aimed to establish a solid baseline for the AI-automated assessment of findings in DPRs by developing, evaluating an AI system, and comparing its performance with that of human readers across multinational data sets.   We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and Taiwan), focusing on 8 types of dental findings. The AI system combined object detection and semantic segmentation techniques for per-tooth finding identification. Performance metrics included sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC). AI generalizability was tested across data sets, and performance was compared with human dental practitioners.   The AI system demonstrated comparable or superior performance to human readers, particularly +67.9% (95% CI: 54.0%-81.9%; p < .001) sensitivity for identifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008) sensitivity for identifying missing teeth. The AI achieved a macro-averaged AUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with the reference were comparable to inter-human agreements in 7 of 8 findings except for caries (p = .024). The AI system demonstrated robust generalization across diverse imaging and demographic settings and processed images 79 times faster (95% CI: 75-82) than human readers.   The AI system effectively assessed findings in DPRs, achieving performance on par with or better than human experts while significantly reducing interpretation time. These results highlight the potential for integrating AI into clinical workflows to improve diagnostic efficiency and accuracy, and patient management."
  },
  {
    "Model Unique Name":"Foundation Model-Powered 3D Few-Shot Class Incremental Learning via Training-free Adaptor",
    "Category":"3D Vision",
    "Detailed Category":"Point Cloud",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2410.09237v1.pdf",
    "GitHub":"https:\/\/github.com\/ahmadisahar\/accv_fcil3d",
    "HuggingFace":null,
    "arxiv_id":2410.09237,
    "Summary":"Recent advances in deep learning for processing point clouds hold increased interest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision. This paper introduces a new method to tackle the Few-Shot Continual Incremental Learning (FSCIL) problem in 3D point cloud environments. We leverage a foundational 3D model trained extensively on point cloud data. Drawing from recent improvements in foundation models, known for their ability to work well across different tasks, we propose a novel strategy that does not require additional training to adapt to new tasks. Our approach uses a dual cache system: first, it uses previous test samples based on how confident the model was in its predictions to prevent forgetting, and second, it includes a small number of new task samples to prevent overfitting. This dynamic adaptation ensures strong performance across different learning tasks without needing lots of fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet, ScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and demonstrating its effectiveness and versatility. The code is available at \\url{https:\/\/github.com\/ahmadisahar\/ACCV_FCIL3D}."
  },
  {
    "Model Unique Name":"Maximize Your Diffusion: A Study into Reward Maximization and Alignment for Diffusion-based Control",
    "Category":"Reinforcement Learning",
    "Detailed Category":"Reinforcement Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2502.12198v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2502.12198,
    "Summary":"Diffusion-based planning, learning, and control methods present a promising branch of powerful and expressive decision-making solutions. Given the growing interest, such methods have undergone numerous refinements over the past years. However, despite these advancements, existing methods are limited in their investigations regarding general methods for reward maximization within the decision-making process. In this work, we study extensions of fine-tuning approaches for control applications. Specifically, we explore extensions and various design choices for four fine-tuning approaches: reward alignment through reinforcement learning, direct preference optimization, supervised fine-tuning, and cascading diffusion. We optimize their usage to merge these independent efforts into one unified paradigm. We show the utility of such propositions in offline RL settings and demonstrate empirical improvements over a rich array of control tasks."
  },
  {
    "Model Unique Name":"Incorporating granularity bias as the margin into contrastive loss for video captioning",
    "Category":"Multi-modal",
    "Detailed Category":"Image Caption",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2311.14977v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2311.14977,
    "Summary":"Video captioning models easily suffer from long-tail distribution of phrases, which makes captioning models prone to generate vague sentences instead of accurate ones. However, existing debiasing strategies tend to export external knowledge to build dependency trees of words or refine frequency distribution by complex losses and extra input features, which lack interpretability and are hard to train. To mitigate the impact of granularity bias on the model, we introduced a statistical-based bias extractor. This extractor quantifies the information content within sentences and videos, providing an estimate of the likelihood that a video-sentence pair is affected by granularity bias. Furthermore, with the growing trend of integrating contrastive learning methods into video captioning tasks, we use a bidirectional triplet loss to get more negative samples in a batch. Subsequently, we incorporate the margin score into the contrastive learning loss, establishing distinct training objectives for head and tail sentences. This approach facilitates the model's training effectiveness on tail samples. Our simple yet effective loss, incorporating Granularity bias, is referred to as the Margin-Contrastive Loss (GMC Loss). The proposed model demonstrates state-of-the-art performance on MSRVTT with a CIDEr of 57.17, and MSVD, where CIDEr reaches up to 138.68."
  },
  {
    "Model Unique Name":"Few-Shot Object Detection via Synthetic Features with Optimal Transport",
    "Category":"Transformer",
    "Detailed Category":"Transformer",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2308.15005v2.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2308.15005,
    "Summary":"Few-shot object detection aims to simultaneously localize and classify the objects in an image with limited training samples. However, most existing few-shot object detection methods focus on extracting the features of a few samples of novel classes that lack diversity. Hence, they may not be sufficient to capture the data distribution. To address that limitation, in this paper, we propose a novel approach in which we train a generator to generate synthetic data for novel classes. Still, directly training a generator on the novel class is not effective due to the lack of novel data. To overcome that issue, we leverage the large-scale dataset of base classes. Our overarching goal is to train a generator that captures the data variations of the base dataset. We then transform the captured variations into novel classes by generating synthetic data with the trained generator. To encourage the generator to capture data variations on base classes, we propose to train the generator with an optimal transport loss that minimizes the optimal transport distance between the distributions of real and synthetic data. Extensive experiments on two benchmark datasets demonstrate that the proposed method outperforms the state of the art. Source code will be available."
  },
  {
    "Model Unique Name":"An Extremely Data-efficient and Generative LLM-based Reinforcement Learning Agent for Recommenders",
    "Category":"Reinforcement Learning",
    "Detailed Category":"Reinforcement Learning",
    "Dataset":null,
    "Paper":"http:\/\/arxiv.org\/pdf\/2408.16032v1.pdf",
    "GitHub":null,
    "HuggingFace":null,
    "arxiv_id":2408.16032,
    "Summary":"Recent advancements in large language models (LLMs) have enabled understanding webpage contexts, product details, and human instructions. Utilizing LLMs as the foundational architecture for either reward models or policies in reinforcement learning has gained popularity -- a notable achievement is the success of InstructGPT. RL algorithms have been instrumental in maximizing long-term customer satisfaction and avoiding short-term, myopic goals in industrial recommender systems, which often rely on deep learning models to predict immediate clicks or purchases.   In this project, several RL methods are implemented and evaluated using the WebShop benchmark environment, data, simulator, and pre-trained model checkpoints. The goal is to train an RL agent to maximize the purchase reward given a detailed human instruction describing a desired product. The RL agents are developed by fine-tuning a pre-trained BERT model with various objectives, learning from preferences without a reward model, and employing contemporary training techniques such as Proximal Policy Optimization (PPO) as used in InstructGPT, and Direct Preference Optimization (DPO). This report also evaluates the RL agents trained using generative trajectories. Evaluations were conducted using Thompson sampling in the WebShop simulator environment.   The simulated online experiments demonstrate that agents trained on generated trajectories exhibited comparable task performance to those trained using human trajectories. This has demonstrated an example of an extremely low-cost data-efficient way of training reinforcement learning agents. Also, with limited training time (<2hours), without utilizing any images, a DPO agent achieved a 19% success rate after approximately 3000 steps or 30 minutes of training on T4 GPUs, compared to a PPO agent, which reached a 15% success rate."
  }
]