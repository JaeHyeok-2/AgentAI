[
  {
    "Model Unique Name": "Classification-HuggingFace-falconsai-nsfw_image_detection",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-21k",
    "Paper": "https://arxiv.org/pdf/2010.11929",
    "Github": null,
    "HuggingFace": "https://huggingface.co/Falconsai/nsfw_image_detection",
    "Summary": "This paper proposes Vision Transformer (ViT), which applies a pure Transformer architecture directly to image classification by treating image patches as tokens. The model is pre-trained on large-scale image datasets and then fine-tuned on smaller benchmarks. The ViT model demonstrates superior performance compared to traditional CNN-based architectures when trained on sufficiently large datasets, outperforming them with substantially lower computational resources. Key experiments show that ViT achieves state-of-the-art results on several standard image classification benchmarks."
  },
  {
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-18",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "Github": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-18",
    "Summary": "This paper introduces a novel deep residual learning framework aimed at addressing the optimization difficulties encountered in training very deep neural networks. Instead of fitting stacked layers to approximate a direct mapping, the authors propose learning residual mappings via identity shortcut connections. The proposed architecture, ResNet, significantly mitigates the degradation problem, allowing the successful training of networks with substantially increased depth. Experiments on ImageNet demonstrate that ResNet models significantly outperform previous architectures, achieving state-of-the-art results with reduced complexity"
  },
  {
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-50",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "Github": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-50",
    "Summary": "This paper introduces a novel deep residual learning framework aimed at addressing the optimization difficulties encountered in training very deep neural networks. Instead of fitting stacked layers to approximate a direct mapping, the authors propose learning residual mappings via identity shortcut connections. The proposed architecture, ResNet, significantly mitigates the degradation problem, allowing the successful training of networks with substantially increased depth. Experiments on ImageNet demonstrate that ResNet models significantly outperform previous architectures, achieving state-of-the-art results with reduced complexity"
  },
  {
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-101",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "Github": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-101",
    "Summary": "This paper introduces a novel deep residual learning framework aimed at addressing the optimization difficulties encountered in training very deep neural networks. Instead of fitting stacked layers to approximate a direct mapping, the authors propose learning residual mappings via identity shortcut connections. The proposed architecture, ResNet, significantly mitigates the degradation problem, allowing the successful training of networks with substantially increased depth. Experiments on ImageNet demonstrate that ResNet models significantly outperform previous architectures, achieving state-of-the-art results with reduced complexity"
  },
  {
    "Model Unique Name": "Classification-HuggingFace-NTQAI-pedestrian_gender_recognition",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "PETA",
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/NTQAI/pedestrian_gender_recognition",
    "Summary": "This model card describes a Vision Transformer (microsoft/beit-base-patch16-224-pt22k-ft22k) fine-tuned for 5 epochs on the PETA dataset to recognise pedestrian gender. The resulting checkpoint (≈ 86 M parameters) is released in PyTorch, Safetensors, and ONNX formats under the Apache-2.0 license. Reported validation accuracy reaches 0.9107, with a final loss of 0.2170. Training used a learning rate of 2 × 10⁻⁵, batch size 8, the Adam optimizer, and a linear LR schedule. Usage is demonstrated with a simple pipeline(\"image-classification\") snippet."
  },
  {
    "Model Unique Name": "Inpainting-LatentDiffusion",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "LAION-400M",
    "Paper": "https://arxiv.org/pdf/2112.10752",
    "Github": "https://github.com/CompVis/latent-diffusion",
    "HuggingFace": null,
    "Summary": "The authors introduce Latent Diffusion Models (LDMs), a two-stage framework that first learns a perceptual autoencoder to compress images into a lower-dimensional latent space and then trains a diffusion model in that space. Operating on latents cuts both training and inference cost while preserving detail, enabling megapixel synthesis. Cross-attention layers connect arbitrary conditioning inputs—such as text, semantic maps or bounding boxes—to the UNet backbone, making the generator highly flexible. LDMs set new state-of-the-art scores for image inpainting and class-conditional synthesis, and achieve competitive results on text-to-image, unconditional generation and super-resolution, all with markedly fewer GPU resources than pixel-based diffusion models"
  },
  {
    "Model Unique Name": "Colorization-DISCO-c0_2",
    "Category": "Img2Img",
    "Detailed Category": "Colorization",
    "Dataset": "ImageNet,COCO",
    "Paper": "https://menghanxia.github.io/projects/disco/disco_main.pdf",
    "Github": "https://github.com/MenghanXia/DisentangledColorization",
    "HuggingFace": null,
    "Summary": "The authors build a dual-branch framework in which a small set of globally located colour “anchors” captures the multimodal colour distribution of an image, while a separate generator propagates those anchor colours across the scene to respect structural affinities. Anchors are found through clustering of super-pixel features that guarantee independence and full image coverage, and the generator refines results with luminance-guided corrections. Experiments on ImageNet, COCO-Stuff and historical photographs show higher perceptual quality, colourfulness and competitive FID compared with prior frameworks, without extra computational burden. "
  },
  {
    "Model Unique Name": "Colorization-DISCO-rand",
    "Category": "Img2Img",
    "Detailed Category": "Colorization",
    "Dataset": "ImageNet,COCO",
    "Paper": "https://menghanxia.github.io/projects/disco/disco_main.pdf",
    "Github": "https://github.com/MenghanXia/DisentangledColorization",
    "HuggingFace": null,
    "Summary": "The authors build a dual-branch framework in which a small set of globally located colour “anchors” captures the multimodal colour distribution of an image, while a separate generator propagates those anchor colours across the scene to respect structural affinities. Anchors are found through clustering of super-pixel features that guarantee independence and full image coverage, and the generator refines results with luminance-guided corrections. Experiments on ImageNet, COCO-Stuff and historical photographs show higher perceptual quality, colourfulness and competitive FID compared with prior frameworks, without extra computational burden. "
  },
  {
    "Model Unique Name": "Deblur-MIMO-UNet-Plus",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "Github": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Summary": "The authors propose MIMO-UNet, a single U-shaped network that accepts the blurry image at multiple down-scaled resolutions and outputs deblurred results for those scales in one pass. A multi-input single encoder (MISE) ingests each resolution, while a multi-output single decoder (MOSD) produces matching deblurred images. Cross-scale information is exchanged by an asymmetric feature-fusion module, and training employs a multi-scale content loss combined with a frequency-domain reconstruction loss. Experiments on GoPro and RealBlur benchmarks show that MIMO-UNet delivers higher PSNR with faster inference and far fewer parameters than stacked coarse-to-fine models."
  },
  {
    "Model Unique Name": "Deblur-MIMO-UNet-RealBlur",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "Github": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Summary": "The authors propose MIMO-UNet, a single U-shaped network that accepts the blurry image at multiple down-scaled resolutions and outputs deblurred results for those scales in one pass. A multi-input single encoder (MISE) ingests each resolution, while a multi-output single decoder (MOSD) produces matching deblurred images. Cross-scale information is exchanged by an asymmetric feature-fusion module, and training employs a multi-scale content loss combined with a frequency-domain reconstruction loss. Experiments on GoPro and RealBlur benchmarks show that MIMO-UNet delivers higher PSNR with faster inference and far fewer parameters than stacked coarse-to-fine models."
  },
  {
    "Model Unique Name": "Deblur-MIMO-UNet",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "Github": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Summary": "The authors propose MIMO-UNet, a single U-shaped network that accepts the blurry image at multiple down-scaled resolutions and outputs deblurred results for those scales in one pass. A multi-input single encoder (MISE) ingests each resolution, while a multi-output single decoder (MOSD) produces matching deblurred images. Cross-scale information is exchanged by an asymmetric feature-fusion module, and training employs a multi-scale content loss combined with a frequency-domain reconstruction loss. Experiments on GoPro and RealBlur benchmarks show that MIMO-UNet delivers higher PSNR with faster inference and far fewer parameters than stacked coarse-to-fine models."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-L-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-S-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-RealBlurJ",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-RealBlurR",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model Unique Name": "Denoise-SwinIR-Noise15",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "The paper introduces SwinIR, an image-restoration network built on the hierarchical Swin Transformer. It consists of a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a reconstruction head that merges shallow and deep features. Trained with task-specific losses, SwinIR attains superior PSNR and SSIM across six restoration benchmarks—including classical, lightweight, and real-world super-resolution, grayscale and color denoising, and compression-artifact reduction—while using up to 67 % fewer parameters than previous methods reported in the paper."
  },
  {
    "Model Unique Name": "Denoise-SwinIR-Noise25",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "The paper introduces SwinIR, an image-restoration network built on the hierarchical Swin Transformer. It consists of a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a reconstruction head that merges shallow and deep features. Trained with task-specific losses, SwinIR attains superior PSNR and SSIM across six restoration benchmarks—including classical, lightweight, and real-world super-resolution, grayscale and color denoising, and compression-artifact reduction—while using up to 67 % fewer parameters than previous methods reported in the paper."
  },
  {
    "Model Unique Name": "Denoise-SwinIR-Noise50",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "The paper introduces SwinIR, an image-restoration network built on the hierarchical Swin Transformer. It consists of a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a reconstruction head that merges shallow and deep features. Trained with task-specific losses, SwinIR attains superior PSNR and SSIM across six restoration benchmarks—including classical, lightweight, and real-world super-resolution, grayscale and color denoising, and compression-artifact reduction—while using up to 67 % fewer parameters than previous methods reported in the paper."
  },
  {
    "Model Unique Name": "Img2Txt-HuggingFace-Salesforce-blip-image-captioning-large",
    "Category": "Img2Txt",
    "Detailed Category": "Img2Txt",
    "Dataset": "COCO",
    "Paper": "https://arxiv.org/pdf/2201.12086",
    "Github": "https://github.com/salesforce/BLIP",
    "HuggingFace": "https://huggingface.co/Salesforce/blip-image-captioning-large",
    "Summary": "The paper introduces BLIP, a two-part framework comprising a Multimodal Mixture of Encoder-Decoder (MED) architecture and a Captioning-and-Filtering (CapFilt) data bootstrapping strategy. MED can operate as an image encoder, an image-grounded text encoder, or an image-grounded text decoder, and is jointly pre-trained with image–text contrastive, image–text matching, and image-conditioned language-modeling losses. CapFilt first uses a fine-tuned captioner to generate synthetic captions for web images, then applies a fine-tuned filter to discard noisy original and synthetic captions, producing a cleaner training set. Experiments show state-of-the-art results on image–text retrieval, captioning, VQA, NLVR2, visual dialog, and zero-shot transfers to video-language tasks."
  },
  {
    "Model Unique Name": "ImgTxt2Txt-HuggingFace-dandelin-vilt-b32-finetuned-vqa",
    "Category": "ImgTxt2Txt",
    "Detailed Category": "ImgTxt2Txt",
    "Dataset": "GCC,SBU,VG,COCO,Flickr30k,VQAv2,NLVR2",
    "Paper": "https://arxiv.org/pdf/2102.03334",
    "Github": "https://github.com/dandelin/vilt",
    "HuggingFace": "https://huggingface.co/dandelin/vilt-b32-finetuned-vqa",
    "Summary": "The paper proposes ViLT, a single-stream Transformer that handles images and text with the same lightweight patch projection used for textual tokens, completely removing CNNs and object detectors. Visual inputs are split into 32 × 32 patches, linearly embedded, concatenated with word embeddings, and processed by a 12-layer ViT-based Transformer initialized from ViT-B/32 weights. Pre-training combines image–text contrastive alignment, image–text matching with word-patch alignment, and masked language modeling (with whole-word masking). ViLT attains competitive or superior results on VQAv2, NLVR2, MS-COCO, and Flickr30K while running tens of times faster and using fewer parameters than previous VLP models."
  },
  {
    "Model Unique Name": "SISR-CARN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model Unique Name": "SISR-CARN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model Unique Name": "SISR-CARN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model Unique Name": "SISR-CARN-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model Unique Name": "SISR-CARN-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model Unique Name": "SISR-CARN-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model Unique Name": "SISR-ESRT-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "Github": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Summary": "ESRT-2× couples a Lightweight CNN Backbone (LCB) that contracts the feature maps with a Lightweight Transformer Backbone (LTB) built from Efficient Transformers (ET) employing Efficient Multi-Head Attention (EMHA). A pixel-shuffle head upsamples features by a factor of two. Training on DIV2K yields PSNR/SSIM competitive with deeper CNNs at a fraction of their parameters."
  },
  {
    "Model Unique Name": "SISR-ESRT-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "Github": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Summary": "Using the same LCB + LTB core as the 2× model, ESRT-3× incorporates a three-step pixel-shuffle reconstruction module that upsamples features by 3. Extensive DIV2K training and fine-tuning demonstrate that the model preserves edge sharpness better than CNN baselines of similar size. "
  },
  {
    "Model Unique Name": "SISR-ESRT-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "Github": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Summary": "ESRT-4× deepens the reconstruction head with an additional upsampling stage but keeps the LCB and LTB unchanged. It achieves favourable PSNR/SSIM on DIV2K and Urban100 with fewer parameters and FLOPs than SwinIR or RCAN, validating the efficiency of EMHA."
  },
  {
    "Model Unique Name": "SISR-HAN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "Github": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Summary": "The authors present Holistic Attention Network (HAN), which enhances single-image super-resolution by combining two complementary attention mechanisms. A Layer Attention Module adaptively weights features across residual groups, and a Channel-Spatial Attention Module jointly learns channel and spatial importance within each map. These modules, inserted into a residual channel-attention backbone, allow the network to focus on informative features at every depth and position. Trained on DIV2K, HAN outperforms previous methods across ×2, ×3, ×4, and ×8 scales without relying on perceptual or adversarial losses."
  },
  {
    "Model Unique Name": "SISR-HAN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "Github": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Summary": "The authors present Holistic Attention Network (HAN), which enhances single-image super-resolution by combining two complementary attention mechanisms. A Layer Attention Module adaptively weights features across residual groups, and a Channel-Spatial Attention Module jointly learns channel and spatial importance within each map. These modules, inserted into a residual channel-attention backbone, allow the network to focus on informative features at every depth and position. Trained on DIV2K, HAN outperforms previous methods across ×2, ×3, ×4, and ×8 scales without relying on perceptual or adversarial losses."
  },
  {
    "Model Unique Name": "SISR-HAN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "Github": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Summary": "The authors present Holistic Attention Network (HAN), which enhances single-image super-resolution by combining two complementary attention mechanisms. A Layer Attention Module adaptively weights features across residual groups, and a Channel-Spatial Attention Module jointly learns channel and spatial importance within each map. These modules, inserted into a residual channel-attention backbone, allow the network to focus on informative features at every depth and position. Trained on DIV2K, HAN outperforms previous methods across ×2, ×3, ×4, and ×8 scales without relying on perceptual or adversarial losses."
  },
  {
    "Model Unique Name": "SISR-HAN-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "Github": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Summary": "The authors present Holistic Attention Network (HAN), which enhances single-image super-resolution by combining two complementary attention mechanisms. A Layer Attention Module adaptively weights features across residual groups, and a Channel-Spatial Attention Module jointly learns channel and spatial importance within each map. These modules, inserted into a residual channel-attention backbone, allow the network to focus on informative features at every depth and position. Trained on DIV2K, HAN outperforms previous methods across ×2, ×3, ×4, and ×8 scales without relying on perceptual or adversarial losses."
  },
  {
    "Model Unique Name": "SISR-DRN-S-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "Github": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Summary": "The authors propose a dual-regression scheme that trains two coupled networks: a primal model that upsamples LR to HR, and a lightweight dual model that down-samples the generated HR back to the original LR, forming a closed loop. The dual path enforces consistency—super-resolved output must downsample to the input—thus regularising the primal mapping. A unified loss combines the standard HR reconstruction term with the dual LR reconstruction term, weighted by λ. For unpaired data, the same closed loop allows the model to learn directly from LR images while still leveraging synthetic LR-HR pairs. Extensive experiments show that their Dual Regression Network (DRN) surpasses state-of-the-art CNN and Transformer SR models on ×2–×8 tasks and adapts robustly to real video frames."
  },
  {
    "Model Unique Name": "SISR-DRN-S-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "Github": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Summary": "The authors propose a dual-regression scheme that trains two coupled networks: a primal model that upsamples LR to HR, and a lightweight dual model that down-samples the generated HR back to the original LR, forming a closed loop. The dual path enforces consistency—super-resolved output must downsample to the input—thus regularising the primal mapping. A unified loss combines the standard HR reconstruction term with the dual LR reconstruction term, weighted by λ. For unpaired data, the same closed loop allows the model to learn directly from LR images while still leveraging synthetic LR-HR pairs. Extensive experiments show that their Dual Regression Network (DRN) surpasses state-of-the-art CNN and Transformer SR models on ×2–×8 tasks and adapts robustly to real video frames."
  },
  {
    "Model Unique Name": "SISR-DRN-L-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "Github": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Summary": "The authors propose a dual-regression scheme that trains two coupled networks: a primal model that upsamples LR to HR, and a lightweight dual model that down-samples the generated HR back to the original LR, forming a closed loop. The dual path enforces consistency—super-resolved output must downsample to the input—thus regularising the primal mapping. A unified loss combines the standard HR reconstruction term with the dual LR reconstruction term, weighted by λ. For unpaired data, the same closed loop allows the model to learn directly from LR images while still leveraging synthetic LR-HR pairs. Extensive experiments show that their Dual Regression Network (DRN) surpasses state-of-the-art CNN and Transformer SR models on ×2–×8 tasks and adapts robustly to real video frames."
  },
  {
    "Model Unique Name": "SISR-DRN-L-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "Github": "https://github.com/guoyongcs/DRNN",
    "HuggingFace": null,
    "Summary": "The authors introduce a Dual Regression Network (DRN) that pairs a conventional upsampler (primal network) with a lightweight downsampler (dual network) to form a closed loop. During training, the HR reconstruction loss drives the primal network when ground-truth HR is available, while a dual consistency loss ensures the downsampled output matches the LR input for every sample. This dual constraint regularises learning, narrows the search space, and allows the same framework to exploit both paired synthetic data and unpaired real LR imagery. Empirical results across ×2–×8 upscaling show that DRN surpasses prior CNN and Transformer baselines in PSNR/SSIM and adapts effectively to real video frames without HR references."
  },
  {
    "Model Unique Name": "SISR-RCAN-it-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "Github": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Summary": "The authors revisit RCAN and create RCAN-it, a minimally modified version that replaces ReLU with SiLU and is trained with modern large-batch, large-patch, cosine-annealed schedules. Extensive ablations reveal that RCAN’s main limitation is underfitting: simply extending training iterations, increasing batch size, and using a stronger optimizer improve performance, while typical regularizers hurt. With these updates, RCAN-it matches or exceeds nearly all CNN-based SR models published after RCAN on standard benchmarks and narrows the gap to recent Transformer approaches. Training efficiency is also improved—comparable accuracy is reached in one quarter of the original training time."
  },
  {
    "Model Unique Name": "SISR-RCAN-it-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "Github": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Summary": "The authors revisit RCAN and create RCAN-it, a minimally modified version that replaces ReLU with SiLU and is trained with modern large-batch, large-patch, cosine-annealed schedules. Extensive ablations reveal that RCAN’s main limitation is underfitting: simply extending training iterations, increasing batch size, and using a stronger optimizer improve performance, while typical regularizers hurt. With these updates, RCAN-it matches or exceeds nearly all CNN-based SR models published after RCAN on standard benchmarks and narrows the gap to recent Transformer approaches. Training efficiency is also improved—comparable accuracy is reached in one quarter of the original training time."
  },
  {
    "Model Unique Name": "SISR-RCAN-it-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "Github": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Summary": "The authors revisit RCAN and create RCAN-it, a minimally modified version that replaces ReLU with SiLU and is trained with modern large-batch, large-patch, cosine-annealed schedules. Extensive ablations reveal that RCAN’s main limitation is underfitting: simply extending training iterations, increasing batch size, and using a stronger optimizer improve performance, while typical regularizers hurt. With these updates, RCAN-it matches or exceeds nearly all CNN-based SR models published after RCAN on standard benchmarks and narrows the gap to recent Transformer approaches. Training efficiency is also improved—comparable accuracy is reached in one quarter of the original training time."
  },
  {
    "Model Unique Name": "SISR-Swin2SR-Classical-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model Unique Name": "SISR-Swin2SR-Classical-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model Unique Name": "SISR-Swin2SR-Compressed-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model Unique Name": "SISR-Swin2SR-LightWeight-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model Unique Name": "SISR-Swin2SR-Real-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model Unique Name": "ObjDet-HuggingFace-facebook-detr-resnet-50",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "COCO",
    "Paper": "https://arxiv.org/pdf/2005.12872",
    "Github": "https://github.com/facebookresearch/detr",
    "HuggingFace": "https://huggingface.co/facebook/detr-resnet-50",
    "Summary": "The authors present DETR, a detector that frames object detection as direct set prediction. A convolutional backbone first extracts a compact feature map; a Transformer encoder–decoder then reasons globally over the image and a fixed number of learned object queries; finally, a shared feed-forward head emits class probabilities (including a “no-object” class) and normalized box coordinates for every query. Training uses a bipartite-matching loss that pairs each ground-truth object with one unique prediction and penalizes classification and box regression errors. DETR matches or surpasses a strong Faster R-CNN baseline on the COCO benchmark, excels on large objects, and extends cleanly to panoptic segmentation by adding a lightweight mask head."
  },
  {
    "Model Unique Name": "ObjDet-HuggingFace-hustvl-yolos-small",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "ImageNet-1k",
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/hustvl/yolos-small",
    "Summary": "YOLOS-Small adapts a 12-layer Vision Transformer (ViT-Small, 16 × 16 patches, 384 hidden dimensions) to object detection. It appends a lightweight prediction head that converts the final patch embeddings into fixed-length sets of class logits and bounding-box coordinates. Trained on ImageNet-1K for classification and then fine-tuned on detection data, YOLOS-Small demonstrates that even a modest transformer backbone can localize and classify objects without auxiliary detection machinery. Although accuracy trails large detector backbones, the model is compact, conceptually simple, and free of domain-specific heuristics.\n"
  },
  {
    "Model Unique Name": "ObjDet-HuggingFace-valentinafeve-yolos-fashionpedia",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "Fashionpedia",
    "Paper": null,
    "Github": "https://github.com/valentinafeve/fine_tunning_YOLOS_for_fashion",
    "HuggingFace": "https://huggingface.co/valentinafeve/yolos-fashionpedia",
    "Summary": "YOLOS-Fashionpedia fine-tunes the small Vision-Transformer backbone from the original YOLOS on Fashionpedia’s annotated runway and street images. The network treats an image as a sequence of patches plus a small set of learnable detection tokens; through global self-attention it reasons about apparel context and outputs fixed-length sets of class logits and bounding-box coordinates. Training uses the Hungarian matching loss to align each ground-truth garment with exactly one prediction, encouraging direct set output. The resulting model detects diverse clothing items—from trench coats to belts—in a single pass and is released under an open licence for fashion-domain research and applications."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3-MobileNet-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "The model couples DeepLabV3’s atrous spatial-pyramid pooling (ASPP) head with a lightweight MobileNet-v2 backbone. Trained from scratch—or fine-tuned from ImageNet weights—on PASCAL VOC 2012, it segments 21 categories in a single forward pass while maintaining a small memory footprint."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3-ResNet50-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "Replacing the heavy ResNet-101 in the original DeepLabV3 with ResNet-50 cuts inference time roughly in half while preserving most of the mean Intersection-over-Union (mIoU) score on VOC 2012."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3-ResNet101-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "Using a high-capacity ResNet-101 backbone with dilated convolutions allows finer receptive fields and higher mIoU than the ResNet-50 or MobileNet versions, albeit with greater compute cost."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3Plus-MobileNet-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "DeepLabV3+ augments the ASPP head with a lightweight decoder path. Combining that decoder with MobileNet-v2 yields a fast, edge-aware model suitable for embedded use."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3Plus-ResNet50-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "Integrating the DeepLabV3+ decoder with a ResNet-50 backbone improves mIoU and edge fidelity relative to its V3 counterpart, making it suitable for real-time desktop applications."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3Plus-ResNet101-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "A ResNet-101 encoder, ASPP module, and decoder path together reach the highest VOC mIoU among the listed variants, though at the cost of increased compute."
  },
  {
    "Model Unique Name": "Restoration-SwinIR-Jpeg10",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR couples a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a task-specific reconstruction head. The shifted-window self-attention inside each RSTB yields both local detail handling and cross-window interaction, while a long skip connection feeds low-frequency features directly to the reconstructor. Trained on DIV2K (and optionally Flickr2K) with standard pixel or Charbonnier losses, SwinIR sets new benchmarks on classical, lightweight, and real-world super-resolution, grayscale and color denoising, and JPEG artifact removal—gaining up to 0.45 dB PSNR over previous best CNNs with up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "Restoration-SwinIR-Jpeg20",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR couples a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a task-specific reconstruction head. The shifted-window self-attention inside each RSTB yields both local detail handling and cross-window interaction, while a long skip connection feeds low-frequency features directly to the reconstructor. Trained on DIV2K (and optionally Flickr2K) with standard pixel or Charbonnier losses, SwinIR sets new benchmarks on classical, lightweight, and real-world super-resolution, grayscale and color denoising, and JPEG artifact removal—gaining up to 0.45 dB PSNR over previous best CNNs with up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "Restoration-SwinIR-Jpeg30",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR couples a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a task-specific reconstruction head. The shifted-window self-attention inside each RSTB yields both local detail handling and cross-window interaction, while a long skip connection feeds low-frequency features directly to the reconstructor. Trained on DIV2K (and optionally Flickr2K) with standard pixel or Charbonnier losses, SwinIR sets new benchmarks on classical, lightweight, and real-world super-resolution, grayscale and color denoising, and JPEG artifact removal—gaining up to 0.45 dB PSNR over previous best CNNs with up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "Restoration-SwinIR-Jpeg40",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR couples a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a task-specific reconstruction head. The shifted-window self-attention inside each RSTB yields both local detail handling and cross-window interaction, while a long skip connection feeds low-frequency features directly to the reconstructor. Trained on DIV2K (and optionally Flickr2K) with standard pixel or Charbonnier losses, SwinIR sets new benchmarks on classical, lightweight, and real-world super-resolution, grayscale and color denoising, and JPEG artifact removal—gaining up to 0.45 dB PSNR over previous best CNNs with up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "Segmentation-HuggingFace-facebook-maskformer-swin-base-coco",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "ADE20k,Cityspaces,COCO,Mapillary Vistas",
    "Paper": "https://arxiv.org/pdf/2107.06278",
    "Github": null,
    "HuggingFace": "https://huggingface.co/facebook/maskformer-swin-base-coco",
    "Summary": "MaskFormer converts any fully convolutional network into a mask-classification system. A pixel decoder produces per-pixel embeddings; a Transformer decoder generates a fixed set of query embeddings, each yielding one class prediction and one mask embedding. The binary masks are recovered by a dot product between per-pixel and mask embeddings, supervised with a classification loss and a focal-plus-dice mask loss under Hungarian matching. Without altering architecture or losses, the same model attains state-of-the-art scores on semantic (e.g., 55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO), particularly excelling when class vocabularies are large."
  },
  {
    "Model Unique Name": "Segmentation-HuggingFace-jonathandinu-face-parsing",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "CelebAMask-HQ",
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/jonathandinu/face-parsing",
    "Summary": "SegFormer-FaceParsing fine-tunes the SegFormer MiT-B0 backbone on the CelebAMask-HQ dataset, predicting 19 facial classes plus background. The hierarchical Transformer encoder captures global context, and the all-MLP decoder fuses multi-scale features without convolutions. With roughly 3.7 M parameters, the model runs in real time on standard GPUs and achieves high mean IoU across facial regions, accurately separating subtle boundaries such as eye shadow and inner-lip areas."
  },
  {
    "Model Unique Name": "Segmentation-HuggingFace-nvidia-segformer-b3-finetuned-ade-512-512",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": null,
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/nvidia/segformer-b3-finetuned-ade-512-512",
    "Summary": "SegFormer-B3 couples a MixVision Transformer (MiT-B3) encoder with an all-MLP decoder that fuses multi-scale features into dense per-pixel predictions. Fine-tuned on the ADE20K dataset at 512 × 512 resolution, the model segments 150 classes in a single forward pass, achieving strong mean IoU while maintaining moderate parameter count and fast inference. Thanks to overlapping patch embeddings and efficient local–global self-attention, it captures both fine details and global context without needing feature pyramids or dilated convolutions."
  },
  {
    "Model Unique Name": "SISR-IMDN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "Github": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Summary": "The authors introduce the Information Multi-Distillation Network (IMDN), built from cascaded Information Multi-Distillation Blocks (IMDBs). Each IMDB progressively splits features, keeps the informative part, and further refines the rest, then fuses all distilled features with a contrast-aware channel-attention layer. A lightweight sub-pixel upsampler produces the final high-resolution image. To extend one model to any scale, they develop an adaptive cropping strategy that divides an input into stride-aligned patches, super-resolves each patch, and stitches the results. Experiments on standard benchmarks show that IMDN matches or exceeds previous lightweight methods in PSNR/SSIM while using under one million parameters and achieving real-time speed on a single GPU."
  },
  {
    "Model Unique Name": "SISR-IMDN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "Github": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Summary": "The authors introduce the Information Multi-Distillation Network (IMDN), built from cascaded Information Multi-Distillation Blocks (IMDBs). Each IMDB progressively splits features, keeps the informative part, and further refines the rest, then fuses all distilled features with a contrast-aware channel-attention layer. A lightweight sub-pixel upsampler produces the final high-resolution image. To extend one model to any scale, they develop an adaptive cropping strategy that divides an input into stride-aligned patches, super-resolves each patch, and stitches the results. Experiments on standard benchmarks show that IMDN matches or exceeds previous lightweight methods in PSNR/SSIM while using under one million parameters and achieving real-time speed on a single GPU."
  },
  {
    "Model Unique Name": "SISR-IMDN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "Github": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Summary": "The authors introduce the Information Multi-Distillation Network (IMDN), built from cascaded Information Multi-Distillation Blocks (IMDBs). Each IMDB progressively splits features, keeps the informative part, and further refines the rest, then fuses all distilled features with a contrast-aware channel-attention layer. A lightweight sub-pixel upsampler produces the final high-resolution image. To extend one model to any scale, they develop an adaptive cropping strategy that divides an input into stride-aligned patches, super-resolves each patch, and stitches the results. Experiments on standard benchmarks show that IMDN matches or exceeds previous lightweight methods in PSNR/SSIM while using under one million parameters and achieving real-time speed on a single GPU."
  },
  {
    "Model Unique Name": "SISR-LatticeNet-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "Github": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Summary": "The authors introduce the Lattice Block (LB), an economical unit that replaces a pair of residual blocks with two butterfly-style combination paths. Learnable “combination coefficients,” produced by a channel-attention mechanism, adaptively mix the two residual branches, allowing LB to mimic multiple residual-block topologies yet use roughly half the parameters. Stacking LBs and feeding their outputs to a Backward Feature Fusion Module (BFM) yields LatticeNet, a lightweight SISR network. Trained on DIV2K and evaluated on Set5, Set14, B100, and Urban100 at ×2–×4 scales, LatticeNet surpasses existing compact models such as CARN and IMDN in PSNR/SSIM while remaining under one million parameters and achieving real-time inference."
  },
  {
    "Model Unique Name": "SISR-LatticeNet-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "Github": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Summary": "The authors introduce the Lattice Block (LB), an economical unit that replaces a pair of residual blocks with two butterfly-style combination paths. Learnable “combination coefficients,” produced by a channel-attention mechanism, adaptively mix the two residual branches, allowing LB to mimic multiple residual-block topologies yet use roughly half the parameters. Stacking LBs and feeding their outputs to a Backward Feature Fusion Module (BFM) yields LatticeNet, a lightweight SISR network. Trained on DIV2K and evaluated on Set5, Set14, B100, and Urban100 at ×2–×4 scales, LatticeNet surpasses existing compact models such as CARN and IMDN in PSNR/SSIM while remaining under one million parameters and achieving real-time inference."
  },
  {
    "Model Unique Name": "SISR-LatticeNet-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "Github": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Summary": "The authors introduce the Lattice Block (LB), an economical unit that replaces a pair of residual blocks with two butterfly-style combination paths. Learnable “combination coefficients,” produced by a channel-attention mechanism, adaptively mix the two residual branches, allowing LB to mimic multiple residual-block topologies yet use roughly half the parameters. Stacking LBs and feeding their outputs to a Backward Feature Fusion Module (BFM) yields LatticeNet, a lightweight SISR network. Trained on DIV2K and evaluated on Set5, Set14, B100, and Urban100 at ×2–×4 scales, LatticeNet surpasses existing compact models such as CARN and IMDN in PSNR/SSIM while remaining under one million parameters and achieving real-time inference."
  },
  {
    "Model Unique Name": "SISR-RCAN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "Github": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Summary": "The authors present RCAN, a 400-plus-layer network for single-image super-resolution. Its core is a Residual-in-Residual (RIR) structure that nests residual groups inside a long skip connection so low-frequency signals bypass the main pathway, easing optimisation. Within each group, Residual Channel-Attention Blocks (RCABs) use a lightweight attention module that rates each channel by global statistics and rescales feature maps accordingly. Trained on DIV2K and evaluated on five benchmarks, RCAN surpasses earlier CNNs and even larger models such as EDSR, achieving up to 0.55 dB PSNR gain while using fewer parameters and delivering real-time speed on a GPU."
  },
  {
    "Model Unique Name": "SISR-RCAN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "Github": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Summary": "The authors present RCAN, a 400-plus-layer network for single-image super-resolution. Its core is a Residual-in-Residual (RIR) structure that nests residual groups inside a long skip connection so low-frequency signals bypass the main pathway, easing optimisation. Within each group, Residual Channel-Attention Blocks (RCABs) use a lightweight attention module that rates each channel by global statistics and rescales feature maps accordingly. Trained on DIV2K and evaluated on five benchmarks, RCAN surpasses earlier CNNs and even larger models such as EDSR, achieving up to 0.55 dB PSNR gain while using fewer parameters and delivering real-time speed on a GPU."
  },
  {
    "Model Unique Name": "SISR-RCAN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "Github": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Summary": "The authors present RCAN, a 400-plus-layer network for single-image super-resolution. Its core is a Residual-in-Residual (RIR) structure that nests residual groups inside a long skip connection so low-frequency signals bypass the main pathway, easing optimisation. Within each group, Residual Channel-Attention Blocks (RCABs) use a lightweight attention module that rates each channel by global statistics and rescales feature maps accordingly. Trained on DIV2K and evaluated on five benchmarks, RCAN surpasses earlier CNNs and even larger models such as EDSR, achieving up to 0.55 dB PSNR gain while using fewer parameters and delivering real-time speed on a GPU."
  },
  {
    "Model Unique Name": "SISR-RCAN-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "Github": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Summary": "The authors present RCAN, a 400-plus-layer network for single-image super-resolution. Its core is a Residual-in-Residual (RIR) structure that nests residual groups inside a long skip connection so low-frequency signals bypass the main pathway, easing optimisation. Within each group, Residual Channel-Attention Blocks (RCABs) use a lightweight attention module that rates each channel by global statistics and rescales feature maps accordingly. Trained on DIV2K and evaluated on five benchmarks, RCAN surpasses earlier CNNs and even larger models such as EDSR, achieving up to 0.55 dB PSNR gain while using fewer parameters and delivering real-time speed on a GPU."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Real-DFO-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Real-DFO-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Real-DFOWMFC-64-L-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-AbsoluteReality",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "AbsoluteReality is a LoRA-style fine-tune of Stable Diffusion v1.5 that targets high-fidelity realism. Curated training images featuring varied lighting, ethnicities, and depth cues guide the model toward accurate anatomy, natural skin, and physically plausible materials. AbsoluteReality retains SD v1.5’s 512 × 512 latent structure, runs in the same pipelines, and supports negative-prompt guidance to minimise artifacts. Users report strong “out-of-the-box” realism with minimal prompt length—e.g., “portrait photo of a woman, 35 mm lens, cinematic lighting”—yielding coherent facial features, sharp textures, and neutral color grading."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-majicMix-sombre",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "majicMix-sombre is a LoRA-style fine-tune of SD v1.5 trained on a curated set of under-exposed portraits, foggy cityscapes, and chiaroscuro interiors. It preserves SD’s latent resolution (512 × 512) and sampling settings but biases the diffusion priors toward shadow-rich, high-contrast compositions. Users report that phrases such as “moody portrait, cinematic shadows” yield coherent faces, subtle film grain, and muted tones without additional prompt engineering."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-SimpleMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "SimpleMix blends several specialist SD v1.5 LoRA weights—photographic realism, soft digital-art shading, and light anime linework—into one checkpoint, balancing them through iterative weight averaging. The resulting model responds smoothly to style keywords (“cinematic photo”, “digital painting”, “anime illustration”) while keeping anatomy fidelity and color consistency."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ToonYou",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "ToonYou is a DreamBooth fine-tune of SD v1.5 trained on thousands of stylised head-and-shoulder illustrations featuring thick outlines, flat colors, and expressive eyes. The model excels at single-character close-ups and supports diverse angles while avoiding photoreal textures. Prompts like “toon style portrait of a warrior, cel-shade, thick lines” generate high-resolution PNG-friendly art ready for comics or stickers."
  },
  {
    "Model Unique Name": "SISR-Any-LIIF-EDSR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": "DIV2K,CelebAHQ,Benchmark",
    "Paper": "https://arxiv.org/pdf/2012.09161",
    "Github": "https://github.com/yinboc/liif",
    "HuggingFace": null,
    "Summary": "The authors propose Local Implicit Image Function (LIIF), which views an image as a grid of latent codes and learns a shared multilayer-perceptron decoder that maps any continuous 2-D coordinate, together with nearby latent codes, to an RGB value. An encoder is jointly trained with LIIF through a self-supervised super-resolution task using randomly sampled up-scaling factors between ×1 and ×4. The resulting representation can be queried at extreme magnifications (up to ×30) far beyond training scales and still recover high-frequency detail. LIIF also enables learning tasks where ground-truth images have varied resolutions, outperforming methods that force all targets to a single size."
  },
  {
    "Model Unique Name": "SISR-Any-LIIF-RDN",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": "DIV2K,CelebAHQ,Benchmark",
    "Paper": "https://arxiv.org/pdf/2012.09161",
    "Github": "https://github.com/yinboc/liif",
    "HuggingFace": null,
    "Summary": "The authors propose Local Implicit Image Function (LIIF), which views an image as a grid of latent codes and learns a shared multilayer-perceptron decoder that maps any continuous 2-D coordinate, together with nearby latent codes, to an RGB value. An encoder is jointly trained with LIIF through a self-supervised super-resolution task using randomly sampled up-scaling factors between ×1 and ×4. The resulting representation can be queried at extreme magnifications (up to ×30) far beyond training scales and still recover high-frequency detail. LIIF also enables learning tasks where ground-truth images have varied resolutions, outperforming methods that force all targets to a single size."
  },
  {
    "Model Unique Name": "SISR-Any-LTE-EDSR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "Github": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Summary": "The authors propose the Local Texture Estimator (LTE), a module that predicts a set of dominant Fourier components for every spatial location and feeds them into an implicit decoder. An encoder (e.g., EDSR-baseline, RDN, or SwinIR without its upsampling head) extracts a latent feature map; LTE transforms a query coordinate and its neighbouring latent codes into amplitude, frequency, and phase signals; a small MLP then reconstructs the RGB value, and a bilinearly upsampled LR skip connection supplies low frequencies. Trained on DIV2K with randomly sampled scale factors between ×1 and ×4, the resulting networks outperform previous arbitrary-scale SR approaches (MetaSR, LIIF) on Set5, Set14, B100, Urban100, and DIV2K, while requiring less memory and runtime."
  },
  {
    "Model Unique Name": "SISR-Any-LTE-RDN",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "Github": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Summary": "The authors propose the Local Texture Estimator (LTE), a module that predicts a set of dominant Fourier components for every spatial location and feeds them into an implicit decoder. An encoder (e.g., EDSR-baseline, RDN, or SwinIR without its upsampling head) extracts a latent feature map; LTE transforms a query coordinate and its neighbouring latent codes into amplitude, frequency, and phase signals; a small MLP then reconstructs the RGB value, and a bilinearly upsampled LR skip connection supplies low frequencies. Trained on DIV2K with randomly sampled scale factors between ×1 and ×4, the resulting networks outperform previous arbitrary-scale SR approaches (MetaSR, LIIF) on Set5, Set14, B100, Urban100, and DIV2K, while requiring less memory and runtime."
  },
  {
    "Model Unique Name": "SISR-Any-LTE-SwinIR-LIIF",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "Github": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Summary": "The authors propose the Local Texture Estimator (LTE), a module that predicts a set of dominant Fourier components for every spatial location and feeds them into an implicit decoder. An encoder (e.g., EDSR-baseline, RDN, or SwinIR without its upsampling head) extracts a latent feature map; LTE transforms a query coordinate and its neighbouring latent codes into amplitude, frequency, and phase signals; a small MLP then reconstructs the RGB value, and a bilinearly upsampled LR skip connection supplies low frequencies. Trained on DIV2K with randomly sampled scale factors between ×1 and ×4, the resulting networks outperform previous arbitrary-scale SR approaches (MetaSR, LIIF) on Set5, Set14, B100, Urban100, and DIV2K, while requiring less memory and runtime."
  },
  {
    "Model Unique Name": "SISR-Any-LTE-SwinIR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "Github": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Summary": "The authors propose the Local Texture Estimator (LTE), a module that predicts a set of dominant Fourier components for every spatial location and feeds them into an implicit decoder. An encoder (e.g., EDSR-baseline, RDN, or SwinIR without its upsampling head) extracts a latent feature map; LTE transforms a query coordinate and its neighbouring latent codes into amplitude, frequency, and phase signals; a small MLP then reconstructs the RGB value, and a bilinearly upsampled LR skip connection supplies low frequencies. Trained on DIV2K with randomly sampled scale factors between ×1 and ×4, the resulting networks outperform previous arbitrary-scale SR approaches (MetaSR, LIIF) on Set5, Set14, B100, Urban100, and DIV2K, while requiring less memory and runtime."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-1",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Version 1.1 is the first general-release Stable Diffusion checkpoint. A variational auto-encoder compresses 512 × 512 RGB images to a latent space; a UNet denoiser, conditioned on CLIP-ViT-L/14 textual embeddings, iteratively refines latent noise into an image representation; the decoder converts the final latent back to RGB. Trained on hundreds of millions of LAION-filtered captioned images, the model produces diverse, photorealistic or illustrative outputs from short prompts on a single consumer GPU."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-2",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Version 1.2 continues training v1.1 with an expanded, quality-filtered image set and improved caption pre-processing. Aesthetic-score filtering and language-detector pruning raise the proportion of high-grade training pairs. The same architecture and latent resolution are retained, enabling seamless replacement in existing pipelines. Users observe crisper edges, fewer duplicated limbs, and tighter adherence to negative prompts compared with v1.1."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-4",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Version 1.4 restarts training from scratch with a two-stage curriculum: (1) 900 k steps on 2.3 B captioned images filtered for resolution, watermark absence, and language; (2) 100 k high-resolution refinement steps with aggressive caption de-duplication. The resulting checkpoint delivers clearer eyes, better composition, and fewer artifacts in complex scenes. It became the de-facto “stable-diffusion-v1-4.ckpt” used by most community UIs.\n"
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-5",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "v1.5 performs an additional 200 k training steps on a hand-curated, “aesthetic-score ≥ 6” subset plus targeted face datasets. Low-resolution noise augmentation and slightly stronger unconditional dropout improve negative-prompt responsiveness. The checkpoint inherits full compatibility with prior v1 models, runs at identical speed, and shows noticeably sharper small objects, cleaner text, and more accurate anatomy. It is the recommended base for photoreal LoRA and DreamBooth training."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ArteYou",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "ArteYou is a fine-tuned SD v1.5 checkpoint focused on rich brush textures, bold color blocking, and story-book lighting. Short prompts such as “vivid oil-paint fantasy landscape” yield coherent compositions with visible stroke patterns and balanced contrast, reducing reliance on negative-prompt tricks.\n"
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Artius_v1.5",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Artius v1.5 refines SD v1.5 on carefully lit portrait and landscape frames inspired by high-end cinema. Prompts like “cinematic 50 mm portrait, moody rim-light” return sharp eyes, correct skin tone, and gentle teal-and-orange hues, reducing the need for elaborate prompt engineering."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-CCDDA_ArtStyle",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "CC DDA ArtStyle merges SD v1.5 with a dataset of experimental artworks—hand-inked sketches, analog glitches, and paper collages. Prompts such as “esoteric ink collage, asymmetrical composition” produce layered, grainy textures, unexpected color overlaps, and controlled chaos suitable for album covers or poster art.\n"
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-colorful",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "colorful is a fine-tuned v1.5 checkpoint that emphasises high-chroma hues, bold contrast, and smooth gradient transitions. With short prompts such as “neon cityscape at dusk,” the model delivers punchy reds, luminous cyans, and clean highlight–shadow separation while minimising colour banding."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ConsistentFactor",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "ConsistentFactor refines v1.5 on repeat-appearance data—photo bursts and multi-view studio shoots—so it learns stronger correlations between facial structure, hairstyle, and apparel. After a single reference prompt (or an embedded trigger token), subsequent prompts yield matching characters with high structural consistency, reducing the need for external face-reference tools."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-CyberRealistic",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "CyberRealistic mixes portrait photography, high-end fashion editorials, and sci-fi concept renders. Prompts like “cyber-realistic half-body portrait, soft rim light, futuristic jacket” yield lifelike skin, believable fabric micro-details, and restrained teal-and-magenta accents—bridging realism with sci-fi flair."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-DreamShaper",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "DreamShaper fuses several specialised v1.5 LoRA checkpoints—realistic portraits, light anime, and ethereal fantasy art—via staged weight averaging, followed by a short stabilisation fine-tune. The model responds to style cues like “dreamshaper realistic,” “dreamshaper anime,” or “dreamshaper fantasy,” adjusting brushwork, line thickness, and colour vibrancy without losing anatomical correctness."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-epiCRealism_newEra",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "The checkpoint is a DreamBooth continuation of SD v1.5 on thousands of DSLR and mirror-less portraits plus 8-K product shots. It excels at fine pores, accurate eye reflections, and believable depth of field while keeping anatomy stable. Prompts such as “studio portrait, 85 mm lens, epic realism” produce magazine-quality imagery with minimal negative prompting."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-GhostMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "GhostMix blends several LoRA checkpoints—cinematic realism, pastel painting, light anime—then stabilises the mix with a short DreamBooth run. Prompts like “ethereal portrait, ghostmix style” yield velvety skin, softened edges, and gentle film-grain bloom, suitable for fantasy covers or romantic scenes."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-henmixReal",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "The model merges a photoreal LoRA with a semi-illustrative LoRA, then finetunes on mixed portraiture. It responds to nuances in prompts, smoothly sliding from realistic to lightly stylised without losing detail."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ICBINP",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Fine-tuned on high-resolution, studio-lit imagery, ICBINP produces strikingly lifelike faces, fabrics, and metals. Prompts like “product shot, icbinp style, 50 mm, f-1.8” yield catalogue-ready visuals with minimal artifacts."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-IDSM",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "IDSM is trained on art featuring mirrored faces, glitch splits, and colour-inverted halves. Prompts such as “idsm style, dual persona, half neon, half charcoal” generate symmetric or bifurcated compositions with high stylistic coherence."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ImpressionismOil",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "ImpressionismOil is a DreamBooth-style continuation of SD v1.5 trained on high-resolution photographs of Monet, Renoir, Sisley, and modern oil studies. The model delivers soft edges, visible impasto, and pastel palettes from short prompts such as “impressionist oil landscape at sunrise”."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-LemonTeaMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "LemonTeaMix blends three specialist LoRAs (semi-real anime, oil-paint portrait, atmospheric concept art) and stabilises the merge with a brief DreamBooth run. Prompts like “lemonteamix, vivid fantasy heroine” yield clean lines, layered brush strokes, and a gentle 2.5-D parallax feel."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-majicMix-realistic",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "majicMix-realistic is a fine-tuned v1.5 checkpoint focused on balanced lighting, neutral colour grading, and micro-detail preservation. Simple prompts such as “majicmix realistic portrait, natural light” return magazine-grade faces, crisp hair strands, and believable backgrounds with minimal artifacts."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-NextPhoto",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "NextPhoto is a DreamBooth continuation of SD v1.5 trained on thousands of daylight portraits, lifestyle scenes, and travel snaps. Prompts such as “nextphoto candid street portrait, 50 mm” yield lifelike skin texture, gentle bokeh, and accurate ambient light without extensive negative prompting."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-PirsusEpicRealism",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Trained on a balanced mix of studio portraits, period costume photography, and high-budget film stills, this model yields striking depth, controlled highlights, and precise anatomy. Prompts like “epic realism medium-shot, rim-light, golden hour” return magazine-cover quality with minimal artifacts."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-QGO-PromptingReal",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "This checkpoint blends lightly fine-tuned realism weights with the base model, then undergoes a “prompt perturbation” training phase where captions are randomly shuffled, lengthened, or truncated. The result is a model that keeps facial accuracy, natural colours, and clean backgrounds even when prompts are casual or unordered."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Realisian",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": null
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Reliberate",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Reliberate starts from v1.5, receives a light realism fine-tune, then undergoes a “prompt-shuffle” phase in which caption tokens are randomly permuted, truncated, or expanded. The model learns to map diverse linguistic patterns to consistent visual output. Users can write relaxed prompts—“woman wearing red coat snowy forest”—and obtain sharp, coherent scenes without special keywords."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-RunDiffusionFX",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "The model is a DreamBooth continuation of v1.5 on professionally lit fashion, automotive and still-life photographs. It enhances specular highlights, deep shadows and micro-contrast. Prompts like “rundiffusion fx, studio car shot, rim-light” yield crisp edges, realistic reflections and controlled bokeh with minimal colour banding.\n"
  },
  {
    "Model Unique Name": "Txt2Txt-HuggingFace-facebook-bart-large-cnn",
    "Category": "Txt2Txt",
    "Detailed Category": "Txt2Txt",
    "Dataset": "CNN Daily Mail",
    "Paper": "https://arxiv.org/pdf/1910.13461",
    "Github": "https://github.com/facebookresearch/fairseq/tree/main/examples/bart",
    "HuggingFace": "https://huggingface.co/facebook/bart-large-cnn",
    "Summary": "BART is a transformer encoder–decoder trained as a denoising auto-encoder: text is corrupted with arbitrary noising schemes, and the model learns to reconstruct the original. Its design unifies BERT-style bidirectional encoding and GPT-style autoregressive decoding within one framework. Experiments show BART equals RoBERTa on GLUE and SQuAD while setting new records on summarization, dialogue, and abstractive QA, and even boosts machine-translation quality when used as a target-side language model. Ablations confirm that span infilling plus sentence shuffling provide the strongest pre-training signal.\n"
  },
  {
    "Model Unique Name": "Txt2Txt-HuggingFace-microsoft-Promptist",
    "Category": "Txt2Txt",
    "Detailed Category": "Txt2Txt",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/microsoft/LMOps/tree/main/promptist",
    "HuggingFace": "https://huggingface.co/microsoft/Promptist",
    "Summary": "Promptist is a lightweight, instruction-tuned language model that converts ordinary user descriptions into detailed prompts tailored for diffusion-based image generators. Fine-tuned from FLAN-T5-XL on approximately 300 K curated prompt pairs, the model learns to inject artist references, camera settings, lighting cues, aspect ratios, and negative keywords while preserving the user’s core intent. Offline evaluations and user studies show that images produced with Promptist-enhanced prompts score markedly higher in realism, relevance, and aesthetic appeal than those from raw inputs. The model is released under the MIT license and runs in under 2 GB of VRAM, making it easy to embed in existing creative pipelines."
  },
  {
    "Model Unique Name": "Txt2Voice-HuggingFace-espnet-fastspeech2_conformer_with_hifigan",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/espnet/fastspeech2_conformer_with_hifigan",
    "Summary": "The pipeline couples a non-autoregressive FastSpeech 2 acoustic model—enhanced with Conformer blocks for richer local–global context—and a parallel HiFi-GAN vocoder. Alignment is obtained from an external CTC/ASR model, enabling the system to predict mel-spectrograms in one shot and convert them to 24-kHz waveform audio in real time."
  },
  {
    "Model Unique Name": "Txt2Img-HuggingFace-prompthero-openjourney-v4",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/prompthero/openjourney-v4",
    "Summary": "OpenJourney v4 is a fine-tuned SD v1.5 checkpoint trained on high-ranking Midjourney community images. It delivers moody lighting, dramatic compositions, and painterly brushwork from concise prompts like “openjourney cyberpunk streetscape”."
  },
  {
    "Model Unique Name": "Txt2Voice-HuggingFace-suno-bark",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/suno/bark",
    "Summary": "Bark is a GPT-style audio language model trained to jointly generate tokens representing speech, non-speech vocalisations, and background music. It supports zero-shot voice cloning, multilingual synthesis, and emotion control from plain text plus optional reference audio."
  },
  {
    "Model Unique Name": "Voice2Txt-nvidia-parakeet-tdt-1.1b",
    "Category": "Snd2Txt",
    "Detailed Category": "Voice2Txt",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/NVIDIA/NeMo",
    "HuggingFace": "https://huggingface.co/nvidia/parakeet-tdt-1.1b",
    "Summary": "Parakeet TDT is a 1.1-billion-parameter Conformer-Transducer optimized for real-time transcription across accents and noisy conditions. It integrates Neural Residual Vector Quantization bottlenecks for compression and domain adversarial training for noise robustness, achieving near-LibriSpeech performance with < 300 ms end-pointer latency."
  },
  {
    "Model Unique Name": "WeatherRemoval-CLAIO-DeHaze",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "Github": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Summary": "The model is one component of the CL All-In-One framework and is trained exclusively for de-hazing. It learns to map hazy inputs from the OTS set to their haze-free references and is evaluated on Rain100H and Snow100K for cross-weather generalisation."
  },
  {
    "Model Unique Name": "WeatherRemoval-CLAIO-DeRain",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "Github": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Summary": "As part of the CL All-In-One suite, this model is trained on the Rain100H subset and validated on OTS and Snow100K to test generalisability. It targets both heavy streak removal and detail sharpening in a one-shot feed-forward pass."
  },
  {
    "Model Unique Name": "WeatherRemoval-CLAIO-DeSnow",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "Github": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Summary": "This variant of CL All-In-One is trained on the Snow100K dataset and cross-tested on OTS and Rain100H. It aims to clear both translucent flake overlays and dense snow blobs in one inference step."
  },
  {
    "Model Unique Name": "Inpainting-CTSDG-CelebA",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "CelebA",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "Github": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Summary": "The authors present a two-stream generator in which texture-constrained structure reconstruction and structure-constrained texture synthesis are performed jointly. Each stream has its own encoder–decoder path, but the decoders borrow features from the opposite encoder, creating bidirectional guidance. A Bi-directional Gated Feature Fusion (Bi-GFF) module further mixes the decoded features, while a Contextual Feature Aggregation (CFA) module refines long-range dependencies through multi-scale self-patch matching. A two-branch discriminator separately evaluates texture realism and structural plausibility. Trained on CelebA, Paris StreetView, and Places2 with irregular masks, the model achieves lower LPIPS and higher PSNR/SSIM than previous methods and produces visually sharper edges with coherent textures."
  },
  {
    "Model Unique Name": "Inpainting-CTSDG-Paris",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Paris StreetView",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "Github": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Summary": "The authors present a two-stream generator in which texture-constrained structure reconstruction and structure-constrained texture synthesis are performed jointly. Each stream has its own encoder–decoder path, but the decoders borrow features from the opposite encoder, creating bidirectional guidance. A Bi-directional Gated Feature Fusion (Bi-GFF) module further mixes the decoded features, while a Contextual Feature Aggregation (CFA) module refines long-range dependencies through multi-scale self-patch matching. A two-branch discriminator separately evaluates texture realism and structural plausibility. Trained on CelebA, Paris StreetView, and Places2 with irregular masks, the model achieves lower LPIPS and higher PSNR/SSIM than previous methods and produces visually sharper edges with coherent textures."
  },
  {
    "Model Unique Name": "Inpainting-CTSDG-Places2",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Places2",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "Github": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Summary": "The authors present a two-stream generator in which texture-constrained structure reconstruction and structure-constrained texture synthesis are performed jointly. Each stream has its own encoder–decoder path, but the decoders borrow features from the opposite encoder, creating bidirectional guidance. A Bi-directional Gated Feature Fusion (Bi-GFF) module further mixes the decoded features, while a Contextual Feature Aggregation (CFA) module refines long-range dependencies through multi-scale self-patch matching. A two-branch discriminator separately evaluates texture realism and structural plausibility. Trained on CelebA, Paris StreetView, and Places2 with irregular masks, the model achieves lower LPIPS and higher PSNR/SSIM than previous methods and produces visually sharper edges with coherent textures."
  },
  {
    "Model Unique Name": "Inpainting-MISF-CelebA",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "CelebA",
    "Paper": "https://arxiv.org/pdf/2203.06304",
    "Github": "https://github.com/tsingqguo/misf",
    "HuggingFace": null,
    "Summary": "MISF introduces a two-branch architecture in which a Kernel Prediction Branch (KPB) predicts spatially adaptive filtering kernels and a Semantic & Image Filtering Branch (SIFB) applies those kernels at both feature and pixel levels. The branches are interactively linked: SIFB’s multi-level features condition KPB, and KPB’s kernels guide SIFB’s filtering, enabling mutual refinement of structure and texture. A Bi-directional Gated Feature Fusion module merges the two streams, and a Contextual Feature Aggregation block propagates non-local information with multi-scale patch matching. Trained on CelebA, Places2 and Dunhuang with irregular masks, MISF surpasses state-of-the-art baselines in PSNR, SSIM, L1 and LPIPS, particularly under large missing regions."
  },
  {
    "Model Unique Name": "Inpainting-MISF-Places2",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Places2",
    "Paper": "https://arxiv.org/pdf/2203.06304",
    "Github": "https://github.com/tsingqguo/misf",
    "HuggingFace": null,
    "Summary": "MISF introduces a two-branch architecture in which a Kernel Prediction Branch (KPB) predicts spatially adaptive filtering kernels and a Semantic & Image Filtering Branch (SIFB) applies those kernels at both feature and pixel levels. The branches are interactively linked: SIFB’s multi-level features condition KPB, and KPB’s kernels guide SIFB’s filtering, enabling mutual refinement of structure and texture. A Bi-directional Gated Feature Fusion module merges the two streams, and a Contextual Feature Aggregation block propagates non-local information with multi-scale patch matching. Trained on CelebA, Places2 and Dunhuang with irregular masks, MISF surpasses state-of-the-art baselines in PSNR, SSIM, L1 and LPIPS, particularly under large missing regions."
  },
  {
    "Model Unique Name": "Inpainting-ResShift-Face",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift constructs a Markov chain that moves directly between a high-resolution (HR) image and its low-resolution (LR) counterpart by shifting their residual rather than diffusing from Gaussian noise. A learnable UNet (trained in the latent space of a VQ-GAN) predicts the clean HR image from each intermediate state. A flexible noise schedule controls both the residual-shifting speed and per-step noise strength, letting the user balance fidelity and perceptual realism. With only 15 sampling steps, ResShift matches or surpasses state-of-the-art SR methods—including latent-diffusion models sped up by DDIM—on synthetic ImageNet and several real-world datasets."
  },
  {
    "Model Unique Name": "Inpainting-ResShift-ImageNet",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift constructs a Markov chain that moves directly between a high-resolution (HR) image and its low-resolution (LR) counterpart by shifting their residual rather than diffusing from Gaussian noise. A learnable UNet (trained in the latent space of a VQ-GAN) predicts the clean HR image from each intermediate state. A flexible noise schedule controls both the residual-shifting speed and per-step noise strength, letting the user balance fidelity and perceptual realism. With only 15 sampling steps, ResShift matches or surpasses state-of-the-art SR methods—including latent-diffusion models sped up by DDIM—on synthetic ImageNet and several real-world datasets."
  },
  {
    "Model Unique Name": "ImgTxt2Img-HuggingFace-alaa-lab-InstructCV",
    "Category": "ImgTxt2Img",
    "Detailed Category": "ImgTxt2Img",
    "Dataset": "NYUV2,MS-COCO,ADE20k,Oxford-IIIT,SUNRGBD,Pascal VOC2012",
    "Paper": "https://arxiv.org/pdf/2310.00390",
    "Github": "https://github.com/AlaaLab/InstructCV",
    "HuggingFace": "https://huggingface.co/alaa-lab/InstructCV",
    "Summary": "The authors cast multiple vision tasks as text-conditioned image-generation problems where the input image and a task instruction guide the model to output a visually encoded solution. They pool standard datasets, build a multi-modal, multi-task corpus of image pairs, and use a large language model to paraphrase prompt templates, yielding varied instructions. Using the InstructPix2Pix framework, they instruction-tune a Stable Diffusion checkpoint so it learns to produce task outputs instead of purely generative imagery. Experiments on ADE20K, MS-COCO, NYUv2, and Oxford-IIIT Pets show that the resulting model, InstructCV, achieves performance on par with specialist and other generalist systems while generalizing to unseen datasets, categories, and user-written prompts."
  },
  {
    "Model Unique Name": "ImgTxt2Img-HuggingFace-timbrooks-instruct-pix2pix",
    "Category": "ImgTxt2Img",
    "Detailed Category": "ImgTxt2Img",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2211.09800",
    "Github": "https://github.com/timothybrooks/instruct-pix2pix",
    "HuggingFace": "https://huggingface.co/timbrooks/instruct-pix2pix",
    "Summary": "The authors create a large paired dataset by combining GPT-3 and Stable Diffusion: GPT-3 rewrites LAION captions into editing instructions and corresponding “after-edit” captions, while Stable Diffusion with Prompt-to-Prompt generates before/after image pairs that match those captions. A latent diffusion model initialized from Stable Diffusion is then fine-tuned on these synthetic triplets so that, given an image and an instruction, it denoises toward the desired edit. Classifier-free guidance is extended to balance faithfulness to the input image and to the instruction. Trained on roughly 450 k examples at 256×256, the resulting model generalizes zero-shot to real photos and user-written instructions, producing diverse edits in seconds."
  },
  {
    "Model Unique Name": "HDR-DeepHDRR",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://people.engr.tamu.edu/nimak/Papers/SIGGRAPH2020_HDR/files/SIGGRAPH2020Final.pdf",
    "Github": "https://github.com/marcelsan/Deep-HdrReconstruction",
    "HuggingFace": null,
    "Summary": "The authors propose a U-Net–style CNN that (1) masks intermediate feature maps so activations from saturated areas are down-weighted, and (2) is trained with a VGG-based perceptual loss adapted to HDR values. To overcome limited HDR training data, the network is first pretrained on large-scale inpainting, then fine-tuned for HDR using simulated LDR–HDR pairs; a patch-sampling strategy focuses this second stage on challenging textured regions. Quantitative tests on synthetic images and qualitative comparisons on real photographs show lower MSE, higher HDR-VDP-2 scores, and fewer artefacts than previous state-of-the-art approaches."
  },
  {
    "Model Unique Name": "HDR-FHDR-I1",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1912.11463",
    "Github": "https://github.com/mukulkhanna/FHDR",
    "HuggingFace": null,
    "Summary": "FHDR adopts a feedback network in which the output of a dense feedback block is recurrently fed back to guide low-level features, creating a virtually deeper model without increasing physical depth. Running for four iterations, the network produces an HDR prediction at each step, improving quality progressively. Feature masking and a global residual skip pass low-level LDR information directly to the reconstruction head, while a mixed L1 + perceptual loss on µ-law–-tonemapped HDR values sharpens textures. On City Scene and a curated high-resolution HDR dataset, FHDR yields higher PSNR, SSIM, and HDR-VDP-2 scores than prior CNN and inverse-tonemapping baselines, despite using fewer parameters."
  },
  {
    "Model Unique Name": "HDR-FHDR-I2",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1912.11463",
    "Github": "https://github.com/mukulkhanna/FHDR",
    "HuggingFace": null,
    "Summary": "FHDR adopts a feedback network in which the output of a dense feedback block is recurrently fed back to guide low-level features, creating a virtually deeper model without increasing physical depth. Running for four iterations, the network produces an HDR prediction at each step, improving quality progressively. Feature masking and a global residual skip pass low-level LDR information directly to the reconstruction head, while a mixed L1 + perceptual loss on µ-law–-tonemapped HDR values sharpens textures. On City Scene and a curated high-resolution HDR dataset, FHDR yields higher PSNR, SSIM, and HDR-VDP-2 scores than prior CNN and inverse-tonemapping baselines, despite using fewer parameters."
  },
  {
    "Model Unique Name": "FaceReplacement-ResShift",
    "Category": "Img2Img",
    "Detailed Category": "Face Replacement",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift links the low- and high-resolution images through a short Markov chain that gradually shifts their residual instead of diffusing from Gaussian noise. Starting near the LR image, only 15 sampling steps suffice to reach the HR target. A learnable UNet operating in latent space predicts the clean HR image at each step. A flexible noise schedule, parameterised by a residual-shift rate and a noise-strength factor, lets users trade fidelity for realism. On synthetic ImageNet and multiple real-world datasets, ResShift matches or surpasses state-of-the-art GAN and diffusion baselines while cutting inference time by roughly 4× relative to latent diffusion accelerated by DDIM."
  },
  {
    "Model Unique Name": "Enhancement-low-light-img-enhancer",
    "Category": "Img2Img",
    "Detailed Category": "Low Light Enhancement",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/dblasko/low-light-event-img-enhancer",
    "HuggingFace": "https://huggingface.co/dblasko/mirnet-low-light-img-enhancement",
    "Summary": "This model adapts the original MIRNet architecture—designed for general image restoration—to the specific task of low-light enhancement. The network contains a multi-scale residual encoder–decoder in which information is exchanged across three spatial resolutions via attention-guided feature fusion. Trained on paired low/normal-exposure images, the model learns to suppress noise, boost luminance, and correct color casts simultaneously. A publicly released checkpoint (≈38 M parameters) runs at full resolution on commodity GPUs and substantially improves PSNR, SSIM, and NIQE scores over classic histogram equalization or Retinex-based methods."
  },
  {
    "Model Unique Name": "Harmonization-INR-RAW-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model Unique Name": "Harmonization-INR-RAW-iHarmony4",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "iHarmony4",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model Unique Name": "Harmonization-INR-Res256-iHarmony4",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "iHarmony4",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model Unique Name": "Harmonization-INR-Res1024-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model Unique Name": "Harmonization-INR-Res2048-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model Unique Name": "NST-fast-neural-style-candy",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Summary": "Each listed checkpoint is a separate feed-forward transform network trained to reproduce one reference painting’s style while preserving the content of any input photograph. Once trained, the model runs in ∼20 ms for a 512 px image on a modern GPU and below 100 ms on many smartphones, enabling live video stylisation. The four provided styles correspond to:\n\t•\tCandy – a bright, saturated candy-wrapper painting (Sato)."
  },
  {
    "Model Unique Name": "NST-fast-neural-style-mosaic",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Summary": "Each listed checkpoint is a separate feed-forward transform network trained to reproduce one reference painting’s style while preserving the content of any input photograph. Once trained, the model runs in ∼20 ms for a 512 px image on a modern GPU and below 100 ms on many smartphones, enabling live video stylisation. The four provided styles correspond to:\n\t•\tMosaic – a Cubist, tile-like Paul Klee mosaic."
  },
  {
    "Model Unique Name": "NST-fast-neural-style-rain-princess",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Summary": "Each listed checkpoint is a separate feed-forward transform network trained to reproduce one reference painting’s style while preserving the content of any input photograph. Once trained, the model runs in ∼20 ms for a 512 px image on a modern GPU and below 100 ms on many smartphones, enabling live video stylisation. The four provided styles correspond to:\n\t•\tRain Princess – Leonid Afremov’s palette-knife “Rain Princess”."
  },
  {
    "Model Unique Name": "NST-fast-neural-style-udnie",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Summary": "Each listed checkpoint is a separate feed-forward transform network trained to reproduce one reference painting’s style while preserving the content of any input photograph. Once trained, the model runs in ∼20 ms for a 512 px image on a modern GPU and below 100 ms on many smartphones, enabling live video stylisation. The four provided styles correspond to:\n\t•\tUdnie – Francis Picabia’s abstract “Udnie (Young American Girl)”."
  },
  {
    "Model Unique Name": "PoseEstimation-OpenPose",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Pose Estimation",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1812.08008",
    "Github": "https://github.com/CMU-Perceptual-Computing-Lab/openpose",
    "HuggingFace": null,
    "Summary": "The authors introduce a bottom-up framework that first predicts body-part locations and then groups them into individual skeletons. A fully convolutional network outputs two sets of heatmaps: (1) confidence maps for part locations and (2) Part Affinity Fields (PAFs) that encode both position and orientation of limbs. A greedy parsing algorithm uses PAF scores to assemble parts into full-body poses for all people simultaneously. Because the network processes the image only once and parsing is lightweight, the system runs in real time on a single GPU while maintaining high accuracy on standard benchmarks.\n"
  },
  {
    "Model Unique Name": "SISR-ResShift-BICSR-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift shortens the diffusion chain to ∼15 steps by shifting the LR–HR residual instead of starting from Gaussian noise. A learned reverse kernel progressively removes scaled residual noise in latent space, enabling ×4 SR that runs 4× faster than latent-diffusion baselines while matching or surpassing their visual metrics. Separate checkpoints are provided for bicubic (BICSR) and three real-world RealSR variants (v1, v2, v3)."
  },
  {
    "Model Unique Name": "SISR-ResShift-RealSR-v1-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift shortens the diffusion chain to ∼15 steps by shifting the LR–HR residual instead of starting from Gaussian noise. A learned reverse kernel progressively removes scaled residual noise in latent space, enabling ×4 SR that runs 4× faster than latent-diffusion baselines while matching or surpassing their visual metrics. Separate checkpoints are provided for bicubic (BICSR) and three real-world RealSR variants (v1, v2, v3)."
  },
  {
    "Model Unique Name": "SISR-ResShift-RealSR-v2-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift shortens the diffusion chain to ∼15 steps by shifting the LR–HR residual instead of starting from Gaussian noise. A learned reverse kernel progressively removes scaled residual noise in latent space, enabling ×4 SR that runs 4× faster than latent-diffusion baselines while matching or surpassing their visual metrics. Separate checkpoints are provided for bicubic (BICSR) and three real-world RealSR variants (v1, v2, v3)."
  },
  {
    "Model Unique Name": "SISR-ResShift-RealSR-v3-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift shortens the diffusion chain to ∼15 steps by shifting the LR–HR residual instead of starting from Gaussian noise. A learned reverse kernel progressively removes scaled residual noise in latent space, enabling ×4 SR that runs 4× faster than latent-diffusion baselines while matching or surpassing their visual metrics. Separate checkpoints are provided for bicubic (BICSR) and three real-world RealSR variants (v1, v2, v3)."
  }
]