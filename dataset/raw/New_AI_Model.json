[
  {
    "Model":null,
    "Model Unique Name":"SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation",
    "Category":"GNN",
    "Detailed Category":null,
    "Dataset":"Amazon-Book, Gowalla, MovieLens, Yelp",
    "Paper":"https:\/\/arxiv.org\/pdf\/2405.20878",
    "GitHub":"https:\/\/github.com\/HKUDS\/SelfGNN",
    "HuggingFace":null,
    "Query1":"I never told the shopping app what I like, but it shows me exactly what I’d pick. How does it get it so right?",
    "Query2":"I watched just a few videos on YouTube, and suddenly it’s recommending ones I actually love. How does it know?",
    "Query3":"My music app keeps picking songs I love, just based on what I listen to. How does it know my taste?",
    "Summary":"SelfGNN is a self-supervised graph neural network designed for sequential recommendation. It builds short-term collaborative graphs to capture local user-item interactions, integrates multi-level long-term sequence modeling using attention and GRU, and applies a personalized self-augmentation mechanism to filter out noisy user behaviors. The method significantly outperforms existing baselines on multiple real-world datasets, showing both improved accuracy and robustness."
  },
  {
    "Model":null,
    "Model Unique Name":"AnimateDiff-Lightning: Cross-Model Diffusion Distillation",
    "Category":"Text-to-Video",
    "Detailed Category":null,
    "Dataset":"WebVid-10M, Pandas-70M",
    "Paper":"https:\/\/arxiv.org\/pdf\/2403.12706",
    "GitHub":null,
    "HuggingFace":"https:\/\/huggingface.co\/ByteDance\/AnimateDiff-Lightning",
    "Query1":"I uploaded just one photo of my dog and it turned into a video of him running—how can it bring a still image to life like that?",
    "Query2":"I typed “a panda dancing in the snow” and it made a super smooth video in seconds—how can it create motion just from text?",
    "Query3":"I gave it a sketch of my character, and it started moving like in a cartoon—how does it animate static drawings like that?",
    "Summary":"AnimateDiff‑Lightning is a lightweight, few‑step video-generation model optimized via a cross-model distillation framework. It takes a robust teacher model (AnimateDiff) and distills it into a streamlined student architecture capable of producing high-quality video sequences in just 1–4 inference steps. A key innovation is the cross-model distillation mechanism, where a shared motion module is trained jointly across multiple base diffusion models to ensure consistent style and dynamics. This approach achieves performance competitive with state-of-the-art models like AnimateLCM, while drastically reducing computational cost and inference time. The implementation is released for the research community."
  },
  {
    "Model":null,
    "Model Unique Name":"AnimateLCM: Computation-Efficient Personalized Style Video Generation without Personalized Video Data",
    "Category":"Text-to-Video",
    "Detailed Category":null,
    "Dataset":"UCF101",
    "Paper":"https:\/\/arxiv.org\/pdf\/2402.00769",
    "GitHub":"https:\/\/github.com\/G-U-N\/AnimateLCM",
    "HuggingFace":"https:\/\/huggingface.co\/wangfuyun\/AnimateLCM",
    "Query1":"I uploaded just a selfie and it made a video of me dancing in my own style—how can it do that without ever seeing my videos?",
    "Query2":"I gave it a cartoon drawing and it created a video in that exact style—it’s like it learned the vibe instantly. How is that even possible?",
    "Query3":"I just described a scene and it made a video that looked like it was in my favorite art style. How can it match styles without needing custom data?",
    "Summary":"AnimateLCM introduces a fast, efficient framework for generating videos in a personalized style without needing personalized video data. It decouples two learning objectives: (1) capturing video style from (non-personalized) style data, and (2) accelerating both image generation and motion generation. By separating image and motion priors during training, the method reduces inference time drastically—from ~25 seconds to ~1 second per clip—while maintaining high visual quality. The model supports adapter modules (e.g., ControlNet), remaining lightweight and highly adaptable. Experiments show that AnimateLCM achieves performance comparable to heavier diffusion methods, but in a fraction of the time . Code and pretrained weights are publicly available ."
  },
  {
    "Model":null,
    "Model Unique Name":"jina-embeddings-v3: Multilingual Embeddings With Task LoRA",
    "Category":"Feature Extraction",
    "Detailed Category":null,
    "Dataset":"MTEB",
    "Paper":"https:\/\/arxiv.org\/pdf\/2409.10173",
    "GitHub":null,
    "HuggingFace":"https:\/\/huggingface.co\/jinaai\/jina-embeddings-v3",
    "Query1":"I searched something in Korean, and it gave me perfect results from English websites. How does it understand across languages like that?",
    "Query2":"I wrote a short sentence, and it matched documents that said the same thing in totally different words. How can it tell the meaning is the same?",
    "Query3":"I gave it product reviews in five different languages, and it grouped them by meaning. How does it recognize similar content no matter the language?",
    "Summary":"jina‑embeddings‑v3 introduces a 570M‑parameter transformer-based embedding model optimized via task-specific LoRA adapters. It supports multilingual (over 100 languages) and long-context (up to 8192 tokens) scenarios, while remaining efficient enough for on-edge deployment. Key techniques include adapter tuning for retrieval, clustering, classification, and text matching, Matryoshka Representation Learning for flexible dimensionality (from 1024 down to 32), and synthetic data augmentation to improve retrieval robustness. On the MTEB benchmark, it outperforms proprietary models (e.g. OpenAI, Cohere) in English and exceeds multilingual‑e5‑large‑instruct across all multilingual tasks."
  },
  {
    "Model":null,
    "Model Unique Name":"JINA CLIP: Your CLIP Model Is Also Your Text Retriever",
    "Category":"Feature Extraction",
    "Detailed Category":null,
    "Dataset":"LAION, MTEB",
    "Paper":"https:\/\/arxiv.org\/pdf\/2405.20204",
    "GitHub":null,
    "HuggingFace":null,
    "Query1":"I dropped in an image of a sunset, and it instantly found poems and captions that perfectly matched the mood. How does it connect images and text like that?",
    "Query2":"I searched using a picture, and it brought back text descriptions that really understood what was in the photo. How can it understand both text and images?",
    "Query3":"I typed a weird sentence, and it showed me a bunch of pictures that actually matched the vibe. How does it know what kind of image fits that kind of text?",
    "Summary":"This paper introduces Jina CLIP, a unified contrastive learning model trained jointly on both image-caption and text-text pairs. By combining cross-modal and text-only objectives in a multi-task framework, Jina CLIP closes the performance gap between CLIP and specialized text retrievers. Experiments show that its text encoder rivals dedicated text models on MTEB benchmarks, while its multimodal capability matches or exceeds CLIP (e.g., achieving ~85.8% Recall@5 across Flickr8k, Flickr30K, MS-COCO), on par with EVA‑CLIP, and outperforming OpenAI’s CLIP on text retrieval . Jina CLIP is fully open-sourced and supports both text-to-text and text-to-image retrieval within a single, efficient model ."
  },
  {
    "Model":null,
    "Model Unique Name":"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "Category":"Fill-Mask",
    "Detailed Category":null,
    "Dataset":"GLUE, BEIR, MLDR, CodeSearchNet, StackQA",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.13663",
    "GitHub":"https:\/\/github.com\/AnswerDotAI\/ModernBERT",
    "HuggingFace":"https:\/\/huggingface.co\/answerdotai\/ModernBERT-large",
    "Query1":"I started typing a sentence, and it filled in the rest in such a natural way—even with a super long document. How does it handle that so smoothly?",
    "Query2":"I searched for something buried deep in a giant report, and it instantly pulled the exact part I needed. How can it scan and understand such long texts so fast?",
    "Query3":"I gave it technical code mixed with natural language, and it figured out what I meant right away. How does it handle both coding and regular language together?",
    "Summary":"ModernBERT is a thoroughly modernized encoder-only transformer model that incorporates state-of-the-art architecture optimizations—such as RoPE positional embeddings, alternating local-global attention, Flash Attention, and unpadding—for significant efficiency gains. Pre-trained on a massive 2 trillion tokens, including code and long-context data (up to 8,192 tokens), it achieves state-of-the-art results across diverse classification and retrieval tasks—especially single- and multi-vector semantic search, including code retrieval. The model offers excellent performance–size trade-offs, strong speed and memory efficiency on standard GPUs, and is available in base (~149 M parameters) and large (~395 M parameters) sizes."
  },
  {
    "Model":null,
    "Model Unique Name":"M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
    "Category":"Sentence Similarity",
    "Detailed Category":null,
    "Dataset":"MTEB, MIRACL, MKQA, BEIR, C-MTEB, MLDR",
    "Paper":"https:\/\/arxiv.org\/pdf\/2402.03216",
    "GitHub":"https:\/\/github.com\/FlagOpen\/FlagEmbedding",
    "HuggingFace":"https:\/\/huggingface.co\/BAAI\/bge-m3",
    "Query1":"I typed the same question in English and Korean, and it gave me the same search results. How does it understand meaning across languages so well?",
    "Query2":"I searched using just a short sentence, and it found super detailed answers. How can it understand what I really mean with so little input?",
    "Query3":"I gave it a product title, a review, and a question, and it figured out they were all talking about the same thing. How does it link such different types of text?",
    "Summary":"BGE M3‑Embedding (M3‑Embedding) is a unified text embedding model designed for versatility across 100+ languages, three retrieval functions (dense, multi-vector, sparse), and input lengths up to 8,192 tokens. It employs a novel self-knowledge distillation technique, using ensemble-style teacher signals derived from its own multi-modal retrieval heads, alongside an optimized batching strategy for large-scale training. M3‑Embedding offers state-of-the-art performance on multilingual, cross-lingual, and long-document retrieval benchmarks (e.g., MIRACL, MKQA), and its model, code, and data have been made publicly available ."
  },
  {
    "Model":null,
    "Model Unique Name":"DEPTH PRO : Sharp Monocular Metric Depth In Less Than a Second",
    "Category":"Depth Estimation",
    "Detailed Category":null,
    "Dataset":"AM-2k, DIS-5k",
    "Paper":"https:\/\/arxiv.org\/pdf\/2410.02073",
    "GitHub":"https:\/\/github.com\/apple\/ml-depth-pro",
    "HuggingFace":"https:\/\/huggingface.co\/apple\/DepthPro",
    "Query1":"I took a quick photo with my phone, and it instantly mapped out the depth of everything in the room—how does it know how far things are from just one image?",
    "Query2":"I used it on a street photo, and it could tell how far the cars and people were—without any special sensors. How is that even possible?",
    "Query3":"I added depth to my portrait photo and it made the background blur look so real. How can it create that effect so accurately from a flat image?",
    "Summary":"Depth Pro is a fast and accurate model for monocular depth estimation that delivers sharp, metric-scale depth maps in under a second. Unlike previous models that require camera intrinsics or extensive fine-tuning, Depth Pro works zero-shot across diverse scenes and devices. It uses a multi-scale Vision Transformer to preserve fine boundary details and structural sharpness in high-resolution outputs."
  },
  {
    "Model":null,
    "Model Unique Name":"Depth-Anything-V2",
    "Category":"Depth Estimation",
    "Detailed Category":null,
    "Dataset":"BlendedMVS, Hypersim, IRS, TartanAir, KITTI",
    "Paper":"https:\/\/arxiv.org\/pdf\/2406.09414",
    "GitHub":"https:\/\/github.com\/DepthAnything\/Depth-Anything-V2",
    "HuggingFace":"https:\/\/huggingface.co\/depth-anything\/Depth-Anything-V2-Large",
    "Query1":"Snapped a photo out the window, and it instantly showed how far away every building and tree was. How can it measure depth from just a picture?",
    "Query2":"Ran it on some drone footage, and it mapped out the entire terrain’s shape in seconds. How does it turn 2D video into 3D structure like that?",
    "Query3":"Dropped in a street scene, and it nailed the depth even in tricky lighting. How can it be so accurate with just one image and no special sensors?",
    "Summary":"Depth Anything V2 improves performance by leveraging three core ideas: using synthetic-only labeled data, training a large DINOv2-G based teacher model, and distilling it into smaller student models using pseudo-labels on 62M real images."
  },
  {
    "Model":null,
    "Model Unique Name":"FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION",
    "Category":"Text-to-Image",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2406.06858",
    "GitHub":"https:\/\/github.com\/black-forest-labs\/flux",
    "HuggingFace":"https:\/\/huggingface.co\/black-forest-labs\/FLUX.1-dev",
    "Query1":"Started generating high-res images from prompts, and it was done way faster than usual—even on my GPU. How is it speeding that up so much without new hardware?",
    "Query2":"Ran a batch of image generations in parallel, and it barely slowed down. How can it handle multiple generations so efficiently?",
    "Query3":"Tried it on a shared GPU server and got smooth performance without lag. What’s going on under the hood to make it so fast and stable?",
    "Summary":"FLUX introduces a novel software-level GPU optimization designed to hide communication latency by tightly fusing fine-grained communication and computation steps into unified kernels. Instead of treating communication and computation separately, FLUX over-decomposes operations and fuses them into larger GPU kernels, enabling up to 96% overlap of communication with work that would otherwise sit idle. In practice, this achieves up to 1.24× speedups in distributed training with Megatron‑LM on 128 GPUs, and up to 1.66×\/1.30× speedups in prefill and decoding inference with vLLM on 8-GPU clusters. This approach delivers significant efficiency gains without altering the model architecture or compromising kernel performance ."
  },
  {
    "Model":null,
    "Model Unique Name":"In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
    "Category":"Image-to-Image",
    "Detailed Category":null,
    "Dataset":"Emu Edit, MagicBrush",
    "Paper":"https:\/\/arxiv.org\/abs\/2504.20690",
    "GitHub":"https:\/\/github.com\/River-Zhang\/ICEdit?tab=readme-ov-file",
    "HuggingFace":"https:\/\/huggingface.co\/RiverZ\/normal-lora",
    "Query1":"I showed it an example of what I wanted, like turning day into night, and it applied the same edit to my photo perfectly. How does it learn edits just from examples?",
    "Query2":"I dragged in a sketch and a style image, and it updated my picture with exactly that vibe. How can it follow visual instructions like that?",
    "Query3":"I used a reference photo to guide the change, and it edited my image without me typing anything. What kind of model understands edits that way?",
    "Summary":"In‑Context Edit introduces a unified and efficient framework for instruction-guided image editing that requires no structural model modifications or extensive data. It leverages the native contextual capabilities of a large-scale Diffusion Transformer (DiT) to perform zero-shot edits based on textual instructions. The framework enhances adaptability by integrating a LoRA-MoE hybrid tuning method that activates modular experts dynamically, and it employs an early-filter mechanism using vision-language models to refine initial noise samples and improve output quality. Evaluated against state-of-the-art methods, it achieves superior editing precision while using only 0.5 % of training data and 1 % of trainable parameters compared to conventional baselines . Code and demos are available via its project page ."
  },
  {
    "Model":null,
    "Model Unique Name":"Wan: Open and Advanced Large-Scale Video Generative Models",
    "Category":"Image-to-Video",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/abs\/2503.20314",
    "GitHub":"https:\/\/github.com\/Wan-Video\/Wan2.1",
    "HuggingFace":"https:\/\/huggingface.co\/Wan-AI",
    "Query1":"I described a scene—like “a tiger running through snow”—and it turned it into a full video that looked like a movie. How does it generate all that just from words?",
    "Query2":"I adjusted the prompt slightly, and the whole video changed with the right mood and motion. How can it be that sensitive to small edits in text?",
    "Query3":"I combined a setting, a character, and an action, and it stitched together a full story in video. What kind of model can turn ideas into such smooth animation?",
    "Summary":"Wan is a suite of open-source video foundation models built on diffusion transformer architecture, engineered to push the boundaries of video generation. The suite includes two models—a compact 1.3 B parameter version optimized for consumer-grade GPUs (≈8 GB VRAM) and a larger 14 B parameter variant—for balanced efficiency and performance. Key innovations include a novel diffusion-aware VAE, scalable pretraining on billions of image\/video samples, curated large-scale datasets, and automated evaluation protocols. Across multiple benchmarks, Wan consistently outperforms existing open-source and commercial models, demonstrating clear scaling benefits. By openly releasing all model checkpoints and code, Wan aims to accelerate innovation in video generative modeling ."
  },
  {
    "Model":null,
    "Model Unique Name":"SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation",
    "Category":"Image-to-Video",
    "Detailed Category":null,
    "Dataset":"ObjaverseDSy, Consistent4D, DAVIS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.16396",
    "GitHub":"https:\/\/github.com\/Stability-AI\/generative-models",
    "HuggingFace":"https:\/\/huggingface.co\/stabilityai\/stable-video-diffusion-img2vid-xt",
    "Query1":"I dropped in a single image, and it turned into a smooth, rotating video of the whole object—how does it figure out what it looks like from all sides?",
    "Query2":"I used different angles of the same scene, and it made one seamless video with perfect motion and lighting. How can it keep everything so consistent?",
    "Query3":"I tried generating a 360° view video from a single frame, and it looked stable the whole time. What kind of tech makes that level of smooth 4D output possible?",
    "Summary":"SV4D 2.0 is a unified diffusion-based framework that transforms a single-view input video into high-fidelity novel-view videos and optimizes a 4D dynamic 3D asset. By redesigning the network architecture to remove dependency on separate multi-view references and adopting a blend of 3D and temporal attention, it achieves robust performance even with occlusions and large motion. The model is further enhanced through progressive multi-stage training, improved data curation, and a two-stage refinement process with progressive frame sampling. Compared to its predecessor, SV4D 2.0 significantly enhances visual detail sharpness (14% LPIPS reduction) and spatio-temporal consistency (44% FV4D improvement), and delivers pronounced gains across both synthetic and real-world benchmarks (e.g., ObjaverseDy, DAVIS) ."
  },
  {
    "Model":null,
    "Model Unique Name":"HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters",
    "Category":"Image-to-Video",
    "Detailed Category":null,
    "Dataset":"LatentSync-filtered AV data, CelebV-HQ, HDTF, Self-Collected Full-Body Wild",
    "Paper":"https:\/\/arxiv.org\/pdf\/2505.20156",
    "GitHub":"https:\/\/github.com\/Tencent-Hunyuan\/HunyuanVideo-Avatar",
    "HuggingFace":"https:\/\/huggingface.co\/tencent\/HunyuanVideo-Avatar",
    "Query1":"I uploaded an audio clip, and it created a video of a person talking with perfect lip sync and gestures. How can it animate a human so accurately just from sound?",
    "Query2":"I gave it voices for two characters, and it made both of them move and interact like in a real conversation. How does it handle multiple people that well?",
    "Query3":"I used a full-body photo and it turned it into a video avatar that spoke and moved just like a real person. What kind of model can animate full bodies so realistically?",
    "Summary":"HunyuanVideo‑Avatar is a diffusion transformer model that generates highly dynamic, multi-character dialogue videos driven solely by audio inputs. It overcomes character identity drift by injecting reference images channel-wise instead of via naive addition, enabling both expressiveness and consistency. An Audio Emotion Module ensures that generated facial expressions accurately reflect emotional tone, while a Face‑Aware Audio Adapter isolates audio-driven motion to specific face regions, enabling synchronized multi-character performance. Evaluated on standard benchmarks and a new wild dataset, the model demonstrates marked improvements in motion realism, lip-sync accuracy, and emotion alignment, outperforming prior audio-driven animation methods across key metrics ."
  },
  {
    "Model":null,
    "Model Unique Name":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
    "Category":"Zero-Shot Image Classification",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.00915",
    "GitHub":"https:\/\/huggingface.co\/microsoft\/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
    "HuggingFace":"https:\/\/huggingface.co\/microsoft\/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
    "Query1":"I uploaded a microscope image, and it instantly told me what kind of cells they were—without any extra training. How does it know that right away?",
    "Query2":"I tested it on diagrams from old research papers, and it correctly matched them with the right medical terms. How can it connect visuals and scientific language like that?",
    "Query3":"I ran it on X-ray and pathology scans, and it grouped them by condition without labels. What kind of model can sort medical images with no supervision?",
    "Summary":"BiomedCLIP introduces PMC‑15M, a large-scale biomedical dataset consisting of 15 million image–caption pairs extracted from 4.4 million articles in PubMed Central. By fine-tuning both image and text encoders—leveraging domain-specialized architectures like PubMedBERT—it adapts CLIP-style contrastive training to the biomedical domain. The resulting model sets new state-of-the-art results in cross-modal retrieval, zero-shot image classification, and visual question answering across eight standard biomedical benchmarks. Notably, BiomedCLIP even surpasses radiology-specific models such as BioViL on pneumonia detection, highlighting the benefits of pretraining on diverse biomedical image types ."
  },
  {
    "Model":null,
    "Model Unique Name":"Scaling Open-Vocabulary Object Detection",
    "Category":"Zero-Shot Object Detection",
    "Detailed Category":null,
    "Dataset":"WebLI, LVIS, ODinW13",
    "Paper":"https:\/\/arxiv.org\/pdf\/2306.09683",
    "GitHub":"https:\/\/github.com\/inuwamobarak\/OWLv2",
    "HuggingFace":"https:\/\/huggingface.co\/google\/owlv2-large-patch14-ensemble",
    "Query1":"I pointed it at a photo and asked for “a teal backpack,” and it found it—even though that label wasn’t trained in. How does it detect things it’s never seen before?",
    "Query2":"I tried it on a messy room photo and told it to find “wireless earbuds,” and it picked them out perfectly. How can it understand open-ended text like that?",
    "Query3":"I uploaded random street scenes and asked for things like “folding bike” or “yellow umbrella,” and it spotted them instantly. What kind of model can handle that?",
    "Summary":"This paper presents OWLv2 and its OWL‑ST self‑training recipe that unlock web-scale weak supervision for open‑vocabulary object detection by generating pseudo‑box labels on massive image–text datasets. OWLv2 enhances training efficiency through optimized architectures and data augmentation techniques, allowing it to surpass prior state-of-the-art detectors using only ~10 million examples. OWL‑ST further scales training to over 1 billion image–text pairs, improving zero-shot detection average precision (AP) on rare LVIS classes from 31.2% to 44.6% using a ViT-L\/14 backbone. The approach requires no architectural changes or extra annotation, demonstrating that scaling self-training on weak supervision matches trends in image-level models and significantly advances open-world localization ."
  },
  {
    "Model":null,
    "Model Unique Name":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
    "Category":"Zero-Shot Object Detection",
    "Detailed Category":null,
    "Dataset":"OpenImages v5 subset, Localized Narratives (OpenImages), LVIS subset, RefCOCO, RefCOCO+, RefCOCOg, gRefCOCO",
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.05499",
    "GitHub":"https:\/\/github.com\/IDEA-Research\/GroundingDINO?tab=readme-ov-file",
    "HuggingFace":"https:\/\/huggingface.co\/IDEA-Research\/grounding-dino-tiny",
    "Query1":"I highlighted a street scene and asked it to find “a kid holding a red balloon,” and it nailed it right away. How does it pick out specific objects just from text?",
    "Query2":"I uploaded a crowded image and asked for “the person in a blue hoodie near the car,” and it knew exactly who I meant. How can it understand such detailed descriptions?",
    "Query3":"I gave it natural sentences like “a cat sitting on the edge of the couch,” and it still found it without needing any labels. What kind of model can do that?",
    "Summary":"Grounding DINO enhances the DETR-based DINO object detector by incorporating language-aware grounded pre-training, enabling zero-shot detection of arbitrary concepts specified via text. The model integrates language cues into key detector stages—feature enhancement, query selection, and a cross-modality decoder—to achieve tight multimodal fusion. It is pre-trained on detection, grounding, and caption datasets, and excels in open-set and referring-expression detection tasks. Empirical results show that Grounding DINO achieves a zero-shot COCO average precision (AP) of 52.5 and sets new records in the ODinW benchmark, while maintaining strong performance on LVIS and RefCOCO datasets ."
  },
  {
    "Model":null,
    "Model Unique Name":"Structured 3D Latents for Scalable and Versatile 3D Generation∗",
    "Category":"Text-to-3D",
    "Detailed Category":null,
    "Dataset":"ObjaverseXL (sketchfab), ObjaverseXL (github), ABO, 3D-FUTURE, HSSD",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.01506",
    "GitHub":"https:\/\/github.com\/Microsoft\/TRELLIS",
    "HuggingFace":"https:\/\/huggingface.co\/microsoft\/TRELLIS-text-xlarge",
    "Query1":"Describing something as simple as “a futuristic coffee machine” was enough—it built a fully rotatable 3D model with amazing detail. How does it turn plain text into objects like that?",
    "Query2":"Even vague ideas like “an elegant chair made of wood and metal” were interpreted perfectly, down to the curve of the legs. How can it understand design features that well from just words?",
    "Query3":"Without needing sketches or blueprints, it generated 3D shapes for all my product ideas—ready for use in mockups. What kind of model makes that level of creation so accessible?",
    "Summary":"This paper introduces a unified structured latent (SLAT) representation that combines a sparse 3D grid with dense visual features from a vision foundation model, enabling versatile decoding into various 3D formats including radiance fields, 3D Gaussians, and meshes. Leveraging rectified flow transformers, the authors train multi-billion parameter models (up to 2B) on a diverse dataset of half a million 3D assets. The resulting system, named TRELLIS, generates high-fidelity 3D geometry and textures in approximately 10 seconds from either text or image prompts, surpassing prior methods at similar scale. Additionally, TRELLIS supports local, prompt-driven editing without retraining, offering an efficient and flexible foundation for scalable, versatile 3D content creation ."
  },
  {
    "Model":null,
    "Model Unique Name":"TripoSR: Fast 3D Object Reconstruction from a Single Image",
    "Category":"Image-to-3D",
    "Detailed Category":null,
    "Dataset":"Objaverse",
    "Paper":"https:\/\/arxiv.org\/pdf\/2403.02151",
    "GitHub":"https:\/\/github.com\/VAST-AI-Research\/TripoSR",
    "HuggingFace":"https:\/\/huggingface.co\/stabilityai\/stable-fast-3d",
    "Query1":"Snapping just one photo was all it needed—it instantly turned a 2D image into a full 3D model I could spin around. How does it rebuild depth from a single view?",
    "Query2":"With no special equipment or setup, it recreated the shape of an object from a random picture I found online. What kind of tech makes that possible?",
    "Query3":"Even complex shapes like chairs and bikes came out clean and detailed from a single image. How can it guess the hidden parts so accurately?",
    "Summary":"TripoSR is a transformer-based model designed for fast, feed-forward 3D mesh reconstruction from a single image, achieving inference in under 0.5 seconds on high-end GPUs without per-shape optimization. Building on the LRM architecture, TripoSR integrates advances in data curation, encoder-decoder design, and training techniques. It outputs textured 3D triplane-based neural radiance fields, and evaluations across standard benchmarks demonstrate state-of-the-art performance—improving Chamfer distance and F-score over prior open-source methods—while offering significantly faster throughput. Released under the MIT license with code, demo, and pretrained models publicly available, TripoSR supports practical deployment and further research ."
  },
  {
    "Model":null,
    "Model Unique Name":"Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
    "Category":"Image-to-3D",
    "Detailed Category":null,
    "Dataset":"Objaverse, Objaverse-XL",
    "Paper":"https:\/\/arxiv.org\/abs\/2501.12202",
    "GitHub":"https:\/\/github.com\/Tencent-Hunyuan\/Hunyuan3D-2",
    "HuggingFace":"https:\/\/huggingface.co\/tencent\/Hunyuan3D-2",
    "Query1":"Typing out something like “a vintage camera made of brass and leather” gave me a fully textured 3D model in seconds. How can it generate such realistic materials from text alone?",
    "Query2":"I tested it with detailed prompts, and it produced high-res 3D objects ready for games or AR. What kind of model handles both shape and texture that well?",
    "Query3":"Even when I fed it imaginative concepts like “a crystal spaceship with glowing engines,” the model delivered stunning 3D assets. How does it turn creative text into production-ready visuals?",
    "Summary":"Hunyuan3D 2.0 delivers an open-source, two-stage diffusion-based system for generating high-resolution textured 3D assets from images. The first stage, Hunyuan3D‑DiT, employs a flow-based diffusion transformer to generate detailed geometry conditioned on input images, while the second stage, Hunyuan3D‑Paint, synthesizes vibrant texture maps using geometric and diffusion priors. Together with a user-friendly production platform—Hunyuan3D‑Studio—users can easily generate, edit, stylize, and animate meshes. Extensive quantitative and qualitative evaluations, including user studies, confirm that Hunyuan3D 2.0 surpasses both open- and closed-source baselines in geometry fidelity, texture quality, and condition alignment, offering a significant tool for scalable 3D content creation ."
  },
  {
    "Model":null,
    "Model Unique Name":"VGGT: Visual Geometry Grounded Transformer",
    "Category":"Image-to-3D",
    "Detailed Category":null,
    "Dataset":"Co3Dv2, BlendMVS, DL3DV, MegaDepth, Kubric, WildRGB, ScanNet, HyperSim, Mapillary Metropolis, Habitat, Replica, MVS-Synth, PointOdyssey, Virtual KITTI, Aria Synthetic Environments, Aria Digital Twin, Objaverse",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.11651",
    "GitHub":"https:\/\/github.com\/facebookresearch\/vggt",
    "HuggingFace":"https:\/\/huggingface.co\/facebook\/VGGT-1B",
    "Query1":"Captured just one image of a toy figure, and it reconstructed a full 3D version I could rotate and zoom in on. How does it understand geometry so well from one photo?",
    "Query2":"Even outdoor scenes with tricky lighting or complex shapes turned into accurate 3D structures. What lets this model handle real-world visuals so robustly?",
    "Query3":"All I had was a single RGB image—no depth, no labels—and still, it produced realistic 3D results. What kind of model can pull that off?",
    "Summary":"VGGT is a feed-forward transformer designed to directly infer comprehensive 3D scene attributes—camera parameters, depth maps, point maps, and 3D point tracks—from one or hundreds of input views in under a second. By forgoing explicit geometric post-processing (like bundle adjustment) and applying a standard transformer with alternating frame-level and global attention, VGGT consistently matches or exceeds traditional optimization-based methods on multiple 3D benchmarks. The model’s learned features also effectively enhance downstream tasks such as novel-view synthesis and non-rigid point tracking. With publicly released code and pretrained weights, VGGT establishes a fast, unified foundation for versatile, multi-task 3D vision that scales seamlessly with increased input views ."
  },
  {
    "Model":null,
    "Model Unique Name":"BGE: One-Stop Retrieval Toolkit For Search and RAG",
    "Category":"Text Classification",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.14475",
    "GitHub":"https:\/\/github.com\/FlagOpen\/FlagEmbedding\/tree\/master?tab=readme-ov-file",
    "HuggingFace":"https:\/\/huggingface.co\/BAAI\/bge-reranker-v2-m3",
    "Query1":"Threw in a search query like “ways to reduce memory usage in Python,” and it pulled up exactly what I needed—even from long, messy docs. How does it rank results so precisely?",
    "Query2":"Even when the wording was vague, it still understood what I meant and found spot-on answers. What helps it go beyond exact keyword matches?",
    "Query3":"Tested it with complex questions across multiple topics, and it reorganized the best answers in the right order. What kind of reranker model can do that?",
    "Summary":"MegaPairs proposes a scalable data synthesis pipeline that combines vision-language models (CLIP and DINO) with open-domain image corpora to generate heterogeneous triplets of images and textual instructions. By mining correlated image pairs with varied relationships and using multimodal and language models to produce descriptive, open-ended instructions, the method creates over 26 million high-quality training instances. Models trained on MegaPairs, named MMRet, outperform baselines trained on 70× more private data, achieving state-of-the-art zero-shot performance on composed image retrieval (CIR) benchmarks and across 36 datasets in the MMEB suite. MegaPairs and its trained retrievers are fully open-sourced to support further research and applications ."
  },
  {
    "Model":null,
    "Model Unique Name":"GLiNER: Generalist and Lightweight Model for Named Entity Recognition",
    "Category":"Token Classification",
    "Detailed Category":null,
    "Dataset":"Pile‑NER",
    "Paper":"https:\/\/arxiv.org\/pdf\/2311.08526",
    "GitHub":"https:\/\/github.com\/urchade\/GLiNER",
    "HuggingFace":"https:\/\/huggingface.co\/urchade\/gliner_multi_pii-v1",
    "Query1":"Pasted in a document full of names, places, and emails, and it automatically picked out every entity—no training needed. How can it recognize all that so quickly?",
    "Query2":"Even when the format was messy, it still found all the key terms without missing a beat. What helps it stay so accurate across different writing styles?",
    "Query3":"Tried it on different topics—from medical notes to social media posts—and it adapted instantly. How does a single model handle so many domains?",
    "Summary":"GLiNER is a compact, bidirectional transformer model that reframes named entity recognition as a matching problem between textual span embeddings and entity-type embeddings, rather than sequential token generation. By encoding both entity labels and candidate text spans in a shared latent space, it enables efficient parallel extraction of arbitrary entity types. Despite its smaller size, GLiNER outperforms larger LLMs like ChatGPT and fine-tuned models on zero-shot benchmarks, and demonstrates robust multilingual capabilities—even on languages unseen during training. It serves as a resource-efficient and accurate solution for flexible, instruction-driven NER tasks ."
  },
  {
    "Model":null,
    "Model Unique Name":"Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
    "Category":"Feature Extraction",
    "Detailed Category":null,
    "Dataset":"MTEB, synthetic text relevance dataset (~150M pairs), high-quality supervised dataset (~7M pairs)",
    "Paper":"https:\/\/arxiv.org\/pdf\/2506.05176",
    "GitHub":"https:\/\/github.com\/QwenLM\/Qwen3-Embedding",
    "HuggingFace":"https:\/\/huggingface.co\/Qwen\/Qwen3-Embedding-0.6B",
    "Query1":"Just by entering a short search phrase, it brought up highly relevant results from huge text collections. How does it capture meaning so effectively in embeddings?",
    "Query2":"Even vague or indirect queries were understood, and the most relevant answers came first. What enables it to rerank results so intelligently?",
    "Query3":"Tested it across topics—from finance to medicine—and it handled them all smoothly. How can a single model adapt across such diverse domains?",
    "Summary":"Qwen3 Embedding presents a series of embedding models (0.6B, 4B, and 8B parameters) derived from the Qwen3 foundation models, designed for both text embedding and reranking tasks. The authors introduce a multi-stage training pipeline with large-scale unsupervised pretraining followed by supervised fine-tuning on high-quality relevance datasets, and apply model-merging strategies to improve robustness. They further leverage the Qwen3 instruct model to synthesize a diverse, multilingual text relevance dataset for supervised training. Evaluations show that Qwen3 Embedding achieves state-of-the-art performance across multiple benchmarks, outperforming its predecessor (GTE‑Qwen) and matching or surpassing other contemporary embedding models in both efficiency and effectiveness ."
  },
  {
    "Model":null,
    "Model Unique Name":"DeepSeek-R1",
    "Category":null,
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.12948",
    "GitHub":"https:\/\/github.com\/deepseek-ai\/DeepSeek-R1",
    "HuggingFace":"https:\/\/huggingface.co\/deepseek-ai\/DeepSeek-R1-0528",
    "Query1":"Started chatting with it about a complex topic, and it responded with clear, accurate reasoning—almost like talking to an expert. How can a language model think that deeply?",
    "Query2":"Even when I switched languages mid-sentence or jumped between topics, it kept up with no problem. What kind of training makes that level of fluency possible?",
    "Query3":"Gave it messy notes and half-finished thoughts, and it turned them into clean summaries and full paragraphs. How does it organize ideas so well from scattered input?",
    "Summary":"DeepSeek‑R1 presents a novel paradigm for enhancing large language model reasoning through pure reinforcement learning (RL), first demonstrating that a base model (DeepSeek‑R1‑Zero) can develop chain-of-thought reasoning and self-reflection without any supervised fine-tuning. To improve readability and reduce language mixing, the authors introduce DeepSeek‑R1, which adds a cold-start supervised stage followed by additional RL and supervised fine-tuning. This multi-stage approach achieves reasoning performance on par with OpenAI’s o1‑1217 model across benchmarks like AIME, MATH‑500, MMLU, GPQA, Codeforces, and more. They also showcase effective distillation, transferring reasoning capabilities into smaller models (1.5B–70B), where a 14B student outperforms open‑source baselines and larger models—highlighting RL + distillation as a cost-efficient path to producing strong reasoning systems. All models and code are open-sourced ."
  },
  {
    "Model":null,
    "Model Unique Name":"VGGT: Visual Geometry Grounded Transformer",
    "Category":" 3D Vision",
    "Detailed Category":null,
    "Dataset":"Co3Dv2, Objaverse, NYUv2",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.11651",
    "GitHub":"https:\/\/github.com\/facebookresearch\/vggt",
    "HuggingFace":null,
    "Query1":"Dropped in a single photo of an object, and it built a 3D model I could rotate and view from every angle. How does it figure out the full shape from just one image?",
    "Query2":"Even indoor scenes with clutter and odd lighting turned into clean 3D geometry. What allows it to stay accurate in such messy conditions?",
    "Query3":"Tested it with random photos from my phone, and it still produced detailed 3D outputs. What kind of model handles real-world images so reliably?",
    "Summary":"VGGT is a large feed-forward transformer that directly infers comprehensive 3D scene attributes—including camera intrinsics\/extrinsics, depth maps, point maps, and 3D point tracks—from anywhere between one to hundreds of input views, in under a second, eliminating the need for traditional geometry post-processing like bundle adjustment. It achieves state-of-the-art performance across multiple tasks—camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and point tracking—often matching or surpassing methods that rely on optimization, while also enhancing downstream applications such as feed-forward novel-view synthesis and non-rigid point tracking. Built with minimal 3D-specific inductive biases (aside from alternating frame-level and global attention), VGGT leverages a transformer backbone trained on large-scale annotated 3D data, and its code and models are publicly released ."
  },
  {
    "Model":null,
    "Model Unique Name":"MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors",
    "Category":" 3D Vision",
    "Detailed Category":null,
    "Dataset":"TUM-RGBD, 7-Scenes, EuRoC, ETH3D",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.12392",
    "GitHub":"https:\/\/github.com\/rmurai0610\/MASt3R-SLAM",
    "HuggingFace":null,
    "Query1":"While moving around with a handheld camera, it built a full 3D map of the room in real time. How can it reconstruct and localize everything so quickly?",
    "Query2":"Even in low light and cluttered spaces, it tracked my motion and built a stable model. What makes it so robust in tough conditions?",
    "Query3":"I ran it on a drone indoors, and it mapped the environment with incredible accuracy—without GPS. How does this system navigate and build 3D in real time?",
    "Summary":"MASt3R‑SLAM introduces a real‑time monocular dense SLAM system built atop a two-view 3D reconstruction prior from MASt3R, enabling robust and in‑the‑wild performance without requiring fixed camera intrinsics beyond a single camera center. The system fuses fine-grained pointmap matching, efficient camera tracking, local volumetric fusion, graph construction with loop closure, and second-order global optimization into a unified framework running at around 15 FPS. When camera calibration is known, it achieves state‑of‑the‑art accuracy in both trajectory estimation and dense geometry recovery across standard benchmarks. By integrating a learned geometric prior directly into the SLAM pipeline, MASt3R‑SLAM provides a plug‑and‑play solution for globally consistent monocular dense mapping and localization ."
  },
  {
    "Model":null,
    "Model Unique Name":"UniK3D: Universal Camera Monocular 3D Estimation",
    "Category":"depth estimation",
    "Detailed Category":null,
    "Dataset":"KITTI, NYUv2, MegaDepth, NianticMapFree, Mapillary",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.16591",
    "GitHub":"https:\/\/github.com\/lpiccinelli-eth\/UniK3D",
    "HuggingFace":null,
    "Query1":"Snapped a photo with my phone, and it instantly figured out how far everything was—whether indoors or outside. How does it estimate depth so accurately from one image?",
    "Query2":"Even with different camera types and no calibration, it still produced solid 3D structure. What makes this model so flexible across devices?",
    "Query3":"Tried it on travel photos and street scenes, and it gave me depth maps that actually felt real. What allows it to generalize so well to everyday images?",
    "Summary":"UniK3D introduces a breakthrough in monocular 3D estimation by supporting any camera model—from standard pinhole to fisheye and panoramic—without requiring calibration or rectification. The model leverages a spherical representation for both scene geometry and camera projection, where rays are encoded via learned spherical harmonics. An angular loss is incorporated to prevent geometric contraction in wide-field scenarios. Evaluated zero-shot across 13 diverse datasets, UniK3D achieves state-of-the-art performance in metric depth, 3D point-cloud reconstruction, and camera parameter estimation, particularly excelling in extreme field-of-view conditions while matching performance on standard narrow views. The method is fully open-sourced, with code and pretrained models available."
  },
  {
    "Model":null,
    "Model Unique Name":"DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
    "Category":"depth estimation",
    "Detailed Category":null,
    "Dataset":"Sintel, ScanNet, KITTI, Bonn, NYU-v2",
    "Paper":"https:\/\/arxiv.org\/pdf\/2409.02095",
    "GitHub":"https:\/\/github.com\/Tencent\/DepthCrafter",
    "HuggingFace":null,
    "Query1":"Dropped in a regular video clip, and it created a smooth depth map for every frame—like the whole scene came alive in 3D. How does it stay consistent over time like that?",
    "Query2":"Even when people or objects moved quickly, it kept the depth stable and accurate. What helps it handle dynamic scenes so well?",
    "Query3":"Ran it on some outdoor footage, and it still captured the depth consistently—even with changing light and motion. How can a model handle such open-world complexity?",
    "Summary":"DepthCrafter adapts a pre-trained image-to-video diffusion model to generate temporally coherent, long-range depth sequences for diverse real-world videos without relying on camera poses or optical flow. Through a carefully designed three-stage training pipeline using both synthetic and real paired video-depth data, the method enables zero-shot depth generation of up to 110 frames at once. At inference, it processes longer clips via segment-wise estimation with seamless stitching. It delivers state-of-the-art performance in zero-shot open-world video depth estimation, with rich geometry and temporal consistency, and supports downstream tasks such as depth-based visual effects and conditional video editing ."
  },
  {
    "Model":null,
    "Model Unique Name":"Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
    "Category":"depth estimation",
    "Detailed Category":null,
    "Dataset":"KITTI, Bonn, ScanNet, Sinte l, DAVIS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.12375",
    "GitHub":"https:\/\/github.com\/DepthAnything\/Video-Depth-Anything",
    "HuggingFace":null,
    "Query1":"Tried it on a full-length video, and every frame had smooth, consistent depth—even across scene changes. How does it keep things stable over such long sequences?",
    "Query2":"Even with fast camera motion and moving objects, the depth didn’t flicker or break. What makes it so robust for dynamic video?",
    "Query3":"Used it on a handheld video from my phone, and it produced depth that looked like it came from a pro 3D scanner. How can it achieve that quality without special gear?",
    "Summary":"Video Depth Anything extends the image-trained Depth Anything V2 model by replacing its head with a lightweight spatio-temporal module and introducing a simple temporal-gradient matching loss to enforce depth consistency across time, all without requiring optical flow or camera pose priors. The system also implements a key-frame–based inference strategy that enables zero-shot depth estimation on arbitrarily long videos—spanning several minutes—while maintaining both spatial accuracy and temporal coherence. Evaluated across multiple video benchmarks, it establishes new state-of-the-art performance in zero-shot video depth estimation, balancing quality, efficiency, and scalability, with the smallest variant running in real-time (~30 FPS) ."
  },
  {
    "Model":null,
    "Model Unique Name":"Interpreting Object-level Foundation Models via Visual Precision Search",
    "Category":"explainability and interpretability",
    "Detailed Category":null,
    "Dataset":"MS COCO, RefCOCO, LVIS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2411.16198",
    "GitHub":"https:\/\/github.com\/RuoyuChen10\/VPS",
    "HuggingFace":null,
    "Query1":"I was trying to figure out why my AI app kept tagging the wrong object in a photo, and this tool showed exactly what part of the image it was focusing on. How does it explain what the model is actually seeing?",
    "Query2":"When I asked it to find “a small red cup,” it highlighted the wrong item. Using this, I could finally see what confused the model. How can it help us understand those mistakes?",
    "Query3":"I compared two models on the same image, and this tool made it obvious which one actually understood the object better. How does it visualize model reasoning so clearly?",
    "Summary":"This paper introduces Visual Precision Search (VPS), a novel, gradient-free method for generating accurate, instance-specific saliency maps to interpret object-level foundation models like Grounding DINO and Florence‑2. VPS works by segmenting input images into sparse superpixel regions and ranking them using consistency and collaboration scores via a submodular selection mechanism—resulting in precise localization of critical decision regions. Evaluations on MS COCO, RefCOCO, and LVIS show that VPS greatly improves attribution faithfulness—up to 31.6% for Grounding DINO and 102.9% for Florence‑2—and reliably identifies factors contributing to detection failures. The method bypasses internal gradient reliance common in multimodal detectors, offers theoretical guarantees on region selection, and opens a path for more trustworthy interpretation of vision-language models ."
  },
  {
    "Model":null,
    "Model Unique Name":"Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders",
    "Category":"explainability and interpretability",
    "Detailed Category":null,
    "Dataset":"GazeFollow, VideoAttentionTarget, ChildPlay, GOO-Real",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.09586",
    "GitHub":"https:\/\/github.com\/fkryan\/gazelle",
    "HuggingFace":null,
    "Query1":"I showed it a photo of someone just looking off to the side, and it could tell exactly what they were focused on. How does it figure out where people are looking so precisely?",
    "Query2":"While watching a video, I always wondered what the person on screen was paying attention to—this tool made it totally clear. How does it visualize someone’s focus like that?",
    "Query3":"Tried it on a clip of a child playing, and it showed exactly what the kid was tracking with their eyes. What kind of model understands gaze in such real-world scenes?",
    "Summary":"Gaze‑LLE simplifies gaze target estimation by harnessing a frozen DINOv2 visual encoder to extract a unified scene representation, then injecting person‑specific positional prompts to decode gaze targets using a lightweight transformer decoder. This streamlined approach eliminates the need for complex multi‑branch pipelines and auxiliary inputs like depth or pose, while achieving state‑of‑the‑art performance on multiple gaze benchmarks. Despite using one to two orders of magnitude fewer trainable parameters, Gaze‑LLE delivers efficient inference (~50 FPS on an RTX 4090), strong cross‑dataset generalization, and rapid training (≈1.5 GPU hours), all with publicly released code and models ."
  },
  {
    "Model":null,
    "Model Unique Name":"MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
    "Category":"generative models",
    "Detailed Category":null,
    "Dataset":"AudioSet, Freesound, VGGSound, AudioCaps, WavCaps",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.15322",
    "GitHub":"https:\/\/github.com\/hkchengrex\/MMAudio",
    "HuggingFace":null,
    "Query1":"I uploaded a video of a waterfall with no sound, and it generated audio that matched perfectly—even the splashing and echo felt real. How does it know what that scene should sound like?",
    "Query2":"While editing a silent drone clip, I used this to add background sound—and the result fit so naturally. How can it generate audio that matches the video content so well?",
    "Query3":"Tried it on different types of videos—cars, animals, city scenes—and it created fitting sounds without any manual work. What kind of model handles that variety so smoothly?",
    "Summary":"MMAudio presents a unified framework for video-to-audio synthesis that is jointly trained on paired video–audio data and large-scale text–audio corpora, enabling high-quality and semantically aligned audio generation. The model enhances audio–visual synchrony using a novel conditional synchronization module that aligns audio latents with video frames at a fine-grained level. With only 157M parameters and using a flow-matching objective, MMAudio achieves state-of-the-art performance in terms of audio fidelity, semantic relevance, and temporal alignment, while keeping inference time low (~1.23 s for an 8‑second clip). Interestingly, joint training does not compromise single-modality performance, as MMAudio also demonstrates competitive results in text-to-audio tasks ."
  },
  {
    "Model":null,
    "Model Unique Name":"SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
    "Category":"generative models",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2403.09055",
    "GitHub":"https:\/\/github.com\/ironjr\/semantic-draw",
    "HuggingFace":null,
    "Query1":"I sketched out a few blobs and labeled them, and it turned that into a full detailed scene in real time. How does it know what I’m trying to draw with just simple shapes?",
    "Query2":"While designing a game map, I used this tool to quickly block out areas, and it filled in realistic textures and objects live. How can it generate full images while I’m still editing?",
    "Query3":"Just by dragging and labeling regions—like “sky,” “grass,” or “building”—I watched a full image come to life on the canvas. What kind of model allows that level of interactive generation?",
    "Summary":"SemanticDraw introduces a transformative interactive image-generation tool that allows content creators to define multiple user-drawn regions, each tied to separate text prompts, and synthesize high-quality images in real time (≈0.64 s per 512×512 image) on a single RTX 2080 Ti GPU. It tackles the incompatibility between region-based control and accelerated diffusion sampling by integrating latent pre-averaging, mask-centering bootstrapping, and mask quantization, enabling LCM-compatible processing in just a few steps. Further, a multi-prompt streaming pipeline batches region-based prompts through continuous diffusion steps, achieving ~1.57 FPS in panorama generation—over 10× faster than prior MultiDiffusion approaches. This enables a new “semantic palette” interface where users can paint with text-linked brushes interactively. The method is model-agnostic, supports any existing diffusion models and schedulers, and is supported by open-source code ."
  },
  {
    "Model":null,
    "Model Unique Name":"MINIMA: Modality Invariant Image Matching",
    "Category":"image matching",
    "Detailed Category":null,
    "Dataset":"MD-syn (MegaDepth-Syn), LLVIP, M3FD, MSRS, METU-VisTIR, MMIM, DIODE, DSEC (RGB-Event)",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.19412",
    "GitHub":"https:\/\/github.com\/LSXI7\/MINIMA",
    "HuggingFace":null,
    "Query1":"Tried matching an infrared night image with a normal daytime photo, and it still aligned them perfectly. How does it recognize the same place across such different styles?",
    "Query2":"Even when I used blurry thermal images or motion-blurred shots, it still found the right matches. What helps it stay so reliable across tough conditions?",
    "Query3":"I used images from completely different sensors—like event cameras and RGB—and it matched them seamlessly. How does this model work across such different modalities?",
    "Summary":"MINIMA introduces a unified image matching framework that generalizes across diverse modalities—such as RGB, depth, infrared, event cameras, and artistic styles—by generating large-scale multimodal training data from inexpensive RGB sources. The core innovation is a “data engine” that converts richly annotated RGB datasets into synthetic multimodal pairs (MD‑syn) using generative models, effectively scaling paired data with accurate pixel-wise correspondence labels. Trained on MD‑syn, MINIMA achieves remarkable cross-modal performance and strong zero-shot transfer, outperforming modality-specific methods on 19 real-world matching tasks. The code and dataset are publicly available to support future multimodal matching research ."
  },
  {
    "Model":null,
    "Model Unique Name":"Layered Image Vectorization via Semantic Simplification",
    "Category":"image vectorization",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2406.05404",
    "GitHub":"https:\/\/github.com\/SZUVIZ\/layered_vectorization",
    "HuggingFace":null,
    "Query1":"I uploaded a messy illustration, and it turned it into clean, layered vector shapes that were super easy to edit. How does it break down an image like that so neatly?",
    "Query2":"When working on a poster design, I used this to simplify a photo into editable parts—each color and region became a separate layer. What kind of model makes that possible?",
    "Query3":"Even with noisy or shaded input, it managed to pull out clean shapes and preserve meaning. How can it simplify images while keeping their structure intact?",
    "Summary":"This paper introduces a progressive, two-stage image vectorization pipeline that reconstructs raster images as layered vector graphics by employing semantic simplification. First, a diffusion-based Score Distillation Sampling method simplifies the image in stages—from coarse semantic structures down to fine details—producing a sequence of abstraction levels. Then, those simplified images guide a structured vector-fitting process that builds clean, semantic-aligned SVG layers. The resulting vector output is both visually faithful and compact, enabling intuitive editing and semantic-aware manipulation. Experiments across diverse image types demonstrate that this method outperforms traditional single-pass vectorization approaches in terms of layer coherence, editability, and file size efficiency ."
  },
  {
    "Model":null,
    "Model Unique Name":"DEIM: DETR with Improved Matching for Fast Convergence",
    "Category":"object detection",
    "Detailed Category":null,
    "Dataset":"COCO",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.04234",
    "GitHub":"https:\/\/github.com\/ShihuaHuang95\/DEIM",
    "HuggingFace":null,
    "Query1":"I ran object detection on a bunch of images, and this model locked onto targets way faster than others. How does it learn to detect so quickly?",
    "Query2":"Even when I added new objects mid-training, it adapted fast and didn’t fall apart. What kind of matching process helps it stay so stable?",
    "Query3":"Used it on COCO-style scenes with lots of small and overlapping objects, and it still nailed the detections. How can it stay that precise under complex setups?",
    "Summary":"DEIM enhances Transformer-based DETR object detectors by replacing sparse one-to-one (O2O) matching with Dense O2O, which generates more positive samples per image via standard data augmentations. To address noise from lower-quality matches, the framework introduces a Matchability-Aware Loss (MAL) that weights matches based on confidence, improving training stability and speed. Experiments on COCO show DEIM reduces training time by 50% and achieves 53.2% AP on RT-DETRv2 after just one day of training on a 4090 GPU. Real-time detectors trained with DEIM—such as DEIM-D-FINE-L and DEIM-D-FINE-X—outperform state-of-the-art models at up to 124 FPS, without extra data or architectural changes ."
  },
  {
    "Model":null,
    "Model Unique Name":"MITracker: Multi-View Integration for Visual Object Tracking",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"MVTrack, GOT-10k, GMTD",
    "Paper":"https:\/\/arxiv.org\/pdf\/2502.20111",
    "GitHub":"https:\/\/github.com\/XuM007\/MITracker",
    "HuggingFace":null,
    "Query1":"I tracked a moving person across camera angles, and it kept following them perfectly—even when they went behind objects. How does it stay locked on across views like that?",
    "Query2":"Tried it in a store with multiple security cams, and the model still recognized the same person as they moved around. What allows it to integrate views that well?",
    "Query3":"Even in low lighting and heavy occlusion, the tracker held on without drifting. What kind of model keeps tracking so reliably under tough conditions?",
    "Summary":"MITracker presents an innovative framework that addresses occlusion and target loss by integrating synchronized multi-view inputs into a unified 3D feature volume projected into bird’s-eye-view (BEV) space. Key to its design is an attention mechanism that fuses geometric consistency from this paired 3D representation to refine per-view tracking results, enabling robust, class-agnostic object tracking over arbitrary-length sequences. Alongside, the authors introduce MVTrack—a high-quality multi-view dataset featuring 234,000 annotated frames of 27 distinct objects across diverse scenes—which supports both training and evaluation. MITracker achieves state-of-the-art performance on the MVTrack and GMTD benchmarks, demonstrating superior robustness to occlusion and long-term consistency compared to leading single-view methods  ￼.\n"
  },
  {
    "Model":null,
    "Model Unique Name":"Multiple Object Tracking as ID Prediction",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"DanceTrack, SportsMOT, MOT17",
    "Paper":"https:\/\/arxiv.org\/pdf\/2403.16848",
    "GitHub":"https:\/\/github.com\/MCG-NJU\/MOTIP",
    "HuggingFace":null,
    "Query1":"While watching a crowded sports video, I noticed it could keep track of each player without mixing them up—even when they crossed paths. How does it recognize who’s who so accurately?",
    "Query2":"Tried tracking dancers in a performance video, and it kept their IDs consistent throughout the whole scene. What kind of model can handle that much motion and identity switching?",
    "Query3":"Even when multiple people entered and left the frame, it assigned the right ID every time. How does it manage consistent tracking without re-identifying errors?",
    "Summary":"This paper recasts multi-object tracking as a direct ID-prediction task, using a transformer-based ID Decoder that learns to assign consistent identities across frames rather than relying on tracking-by-detection pipelines. The ID Decoder employs a dynamic multi-layer structure—optimized through ablation studies—to handle appearance changes and occlusions, achieving a HOTA score improvement from 54.3 to 60.5 by moving from 1 to 6 layers. The method is fully feed-forward and demonstrates strong performance across standard MOT benchmarks without requiring complex association or motion models. By unifying detection and tracking through identity prediction, the approach simplifies the tracking framework while maintaining competitive accuracy ."
  },
  {
    "Model":null,
    "Model Unique Name":"EdgeTAM: On-Device Track Anything Model",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"DAVIS 2017, MOSE, SA-V (val\/test), YouTube-VOS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.07256",
    "GitHub":"https:\/\/github.com\/facebookresearch\/EdgeTAM",
    "HuggingFace":null,
    "Query1":"I ran this on my phone to track my dog in a video, and it followed him the entire time—no internet, no lag. How does it manage that kind of tracking directly on a device?",
    "Query2":"Even when I tapped on a moving object in a live video, it instantly started tracking without delay. What makes it responsive enough for real-time use?",
    "Query3":"I tried it while recording outdoors, and it still tracked smoothly despite motion and lighting changes. How does it stay so stable without needing the cloud?",
    "Summary":"EdgeTAM adapts SAM 2 for on-device video segmentation and tracking by introducing a 2D Spatial Perceiver module that compresses frame‑level memory features while preserving spatial structure, and employs feature‑level distillation from the full SAM 2 model. As a result, it runs at 16 FPS on iPhone 15 Pro Max—about 22× faster than SAM 2—while delivering comparable or better accuracy on DAIVS‐2017, MOSE, SA‑V, and YouTubeVOS benchmarks (e.g., 87.7 J&F on DAVIS’17). Training involved aligning both encoder and memory-attention features between teacher and student, yielding a lightweight yet high-performing on-device system ."
  },
  {
    "Model":null,
    "Model Unique Name":"A Distractor-Aware Memory for Visual Object Tracking with SAM2",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"DiDi, VOT2020, VOT2022, VOTChallenge, LaSOT, GOT-10k",
    "Paper":"https:\/\/arxiv.org\/pdf\/2411.17576",
    "GitHub":"https:\/\/github.com\/jovanavidenovic\/DAM4SAM",
    "HuggingFace":null,
    "Query1":"While tracking a person in a crowd, the model didn’t get distracted by others wearing similar clothes. How does it stay focused on the right target?",
    "Query2":"While tracking a person in a crowd, the model didn’t get distracted by others wearing similar clothes. How does it stay focused on the right target?",
    "Query3":"I tried it in a video where the target disappeared and reappeared, but the model still locked onto the same one. How does this memory system work under occlusion?",
    "Summary":"This work enhances memory-based visual object tracking—particularly with SAM2—by introducing a novel Distractor-Aware Memory (DAM) that separates recent target appearances (RAM) from hard-to-distinguish distractors (DRM). The key innovation is an introspection-driven update mechanism: when SAM2’s predicted mask diverges from an alternative segmentation, the frame is added to DRM to boost robustness. They also release DiDi, a distractor-distilled dataset designed to expose real-world tracking failures. Without additional training, the resulting SAM2.1++ significantly outperforms SAM2.1 and related variants across seven benchmarks, setting a new state-of-the-art on six of them."
  },
  {
    "Model":null,
    "Model Unique Name":"From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"Market1501, SYSU‑MM01, Occluded‑ReID",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.00938",
    "GitHub":"https:\/\/github.com\/yuanc3\/Pose2ID",
    "HuggingFace":null,
    "Query1":"I checked footage from two different cameras, and it still recognized the same person—even though their pose had totally changed. How does it match identities without training?",
    "Query2":"Even when someone was partially blocked or turning away, the system still re-identified them correctly. What helps it ignore pose and focus on who it is?",
    "Query3":"Tried it across day and night clips, and it kept matching the same person without needing to fine-tune. How can it stay accurate with no retraining?",
    "Summary":"Pose2ID introduces a training‑free person Re‑ID approach that enhances identity feature stability by centralizing embeddings around per‑identity feature centers. The method comprises two key components: an identity‑guided pedestrian generator that synthesizes diverse view samples to enrich identity representation, and neighbor feature centralization that aggregates proximal neighbors in embedding space to reduce noise from occlusion and background interference. Surprisingly, without any Re‑ID model fine‑tuning, Pose2ID achieves strong performance—52.8 mAP \/ 78.9 Rank‑1 on Market1501—and sets new benchmarks across cross‑modality and occluded Re‑ID tasks ."
  },
  {
    "Model":null,
    "Model Unique Name":"Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models",
    "Category":"open-world detection",
    "Detailed Category":null,
    "Dataset":"Anomaly-Instruct-125k, VisA-D&R, MVTec AD, AITEX, ELPV, BTAD, MPDD, BrainMRI, HeadCT, Br35H",
    "Paper":"https:\/\/arxiv.org\/pdf\/2502.07601",
    "GitHub":"https:\/\/github.com\/honda-research-institute\/Anomaly-OneVision",
    "HuggingFace":null,
    "Query1":"I uploaded a product photo, and it immediately pointed out a defect—even though I never told it what to look for. How does it detect anomalies without any training examples?",
    "Query2":"Even in medical images like MRIs or CT scans, it spotted unusual regions and explained why they looked abnormal. How can a model reason about anomalies so clearly?",
    "Query3":"I tested it on fabrics, electronics, even X-rays, and it still gave useful results with no retraining. What makes this model so flexible across domains?",
    "Summary":"This paper introduces Anomaly‑Instruct‑125k, the first large-scale visual-instruction tuning dataset for zero-shot anomaly detection (ZSAD) and reasoning, paired with a comprehensive benchmark called VisA‑D&R. By evaluating current multimodal large language models (MLLMs) like GPT‑4o, the authors demonstrate these models struggle to detect and explain fine-grained visual anomalies. To address this, they propose Anomaly‑OneVision (Anomaly‑OV), a specialist system that leverages a Look‑Twice Feature Matching mechanism to adaptively select and emphasize abnormal visual tokens for precise detection and human-readable reasoning. Anomaly‑OV significantly outperforms generalist models on detection accuracy and anomaly interpretation, and extends effectively to industrial, medical, and 3D anomaly scenarios ."
  },
  {
    "Model":null,
    "Model Unique Name":"Reconstructing Humans with a Biomechanically Accurate Skeleton",
    "Category":"pose estimation",
    "Detailed Category":null,
    "Dataset":"Human3.6M, MPI-INF-3DHP, COCO, MPII, AI Challenger, AVA, InstaVariety, MOYO",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.21751",
    "GitHub":"https:\/\/github.com\/IsshikiHugh\/HSMR",
    "HuggingFace":null,
    "Query1":"Recorded a short video of someone walking, and it built a full 3D skeleton that even captured subtle joint movement. How does it reconstruct the body so precisely from just video?",
    "Query2":"Even with loose clothing or side views, it got the posture and limb positions exactly right. What allows it to track body mechanics so reliably?",
    "Query3":"I tried it for fitness analysis, and it showed how each joint was behaving, almost like a digital skeleton. How can this model understand human biomechanics so well?",
    "Summary":"This work introduces HSMR (Human Skeleton and Mesh Recovery), a novel transformer-based model that reconstructs a full-body 3D mesh with a biomechanically accurate skeleton, unlike standard models like SMPL. HSMR is trained using a self-generated pseudo-ground-truth pipeline: it synthesizes SKEL-model parameters for real images and iteratively refines them through training loops to improve accuracy. The result delivers anatomically plausible joint predictions and high-fidelity surface meshes from a single image. By combining SKEL’s biomechanical rig with robust image-to-parameter regression, HSMR enables realistic and physically consistent 3D human modeling and is publicly released with project code ."
  },
  {
    "Model":null,
    "Model Unique Name":"MatAnyone: Stable Video Matting with Consistent Memory Propagation",
    "Category":"segmentation",
    "Detailed Category":null,
    "Dataset":"VM800, YoutubeMatte",
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.14677",
    "GitHub":"https:\/\/github.com\/pq-yang\/MatAnyone",
    "HuggingFace":null,
    "Query1":"I removed the background from a full video, and it stayed clean and stable the whole time—even as the person moved around. How does it keep the edges so consistent?",
    "Query2":"Tried editing a vlog clip outdoors, and even with wind-blown hair and shadows, the subject stayed perfectly separated. What makes it handle those tricky details so well?",
    "Query3":"I used it for background replacement, and the result looked smooth frame to frame—no flickering or jitter. How does this model maintain such temporal consistency?",
    "Summary":"MatAnyone introduces a memory-based video matting framework that enables temporally stable and semantically consistent matte extraction for a user-specified target across diverse scenes. By incorporating a region‑adaptive memory fusion module, the system selectively integrates previous-frame embeddings to maintain core object consistency while preserving high-fidelity boundary details. The authors also curated a high-quality, diverse matting dataset and devised a segmentation-assisted training strategy leveraging large-scale segmentation data to bolster model robustness. MatAnyone significantly outperforms prior auxiliary-free matting methods, particularly in complex or cluttered backgrounds, and is backed by an accompanying dataset and open-source implementation ."
  },
  {
    "Model":null,
    "Model Unique Name":"FoundationStereo: Zero-Shot Stereo Matching",
    "Category":"stereo matching",
    "Detailed Category":null,
    "Dataset":"FoundationStereo Dataset (1M synthetic stereo pairs), Scene Flow, Middlebury, ETH3D, KITTI",
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.09898",
    "GitHub":"https:\/\/github.com\/NVlabs\/FoundationStereo",
    "HuggingFace":null,
    "Query1":"I gave it two slightly different photos from a stereo camera, and it created a full depth map—no training or fine-tuning needed. How does it understand depth out of the box like that?",
    "Query2":"Even in unfamiliar scenes it’s never seen before, like cluttered rooms or outdoor roads, it still gave accurate 3D structure. What helps it generalize so well?",
    "Query3":"I tested it with stereo pairs in low light, and it still produced reliable depth. How does this model stay stable in challenging visual conditions?",
    "Summary":"FoundationStereo presents a foundation-model approach for stereo depth estimation that achieves strong zero-shot generalization across diverse real-world domains without requiring per-domain fine-tuning. The model is trained on a curated 1 million synthetic stereo image pair dataset and enhanced with architectural components like a side-tuning feature backbone to adapt monocular priors, attentive long-range cost-volume filtering, and iterative refinement. These strategies together close the sim-to-real gap and deliver state-of-the-art robustness and accuracy on challenging scenarios—such as reflective surfaces, thin structures, and extreme lighting—establishing a new zero-shot benchmark for stereo matching. Though not yet real-time (≈0.7 s per image on A100), it sets the stage for deployable, generalizable stereo depth models ."
  },
  {
    "Model":null,
    "Model Unique Name":"Towards Universal Soccer Video Understanding",
    "Category":"video understanding",
    "Detailed Category":null,
    "Dataset":"SoccerReplay-1988, SoccerNet-v2, SoccerNet",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.01820",
    "GitHub":"https:\/\/github.com\/jyrao\/UniSoccer",
    "HuggingFace":null,
    "Query1":"I watched a full match replay, and it automatically identified key moments—goals, fouls, even tactical plays. How does it understand what’s happening in a soccer game so well?",
    "Query2":"Even with old or low-quality footage, it still tracked players and actions accurately. What helps the model stay reliable across different video types?",
    "Query3":"I tried it during a live match, and it picked up key events almost in real time. How can it process complex team play and make sense of it so quickly?",
    "Summary":"This work introduces SoccerReplay-1988, the largest multi-modal soccer dataset comprising nearly 2,000 full matches with rich annotations—including timestamped events and match commentary—generated via an automated pipeline. The authors also present MatchVision, the first soccer-specific visual-language foundation model that extends visual backbones with spatiotemporal attention to excel across tasks like event classification, commentary generation, and multi-view foul recognition. MatchVision achieves state-of-the-art performance on both existing benchmarks and newly introduced evaluation tasks, demonstrating the effectiveness of large-scale data collection and unified modeling for comprehensive sports video understanding ."
  },
  {
    "Model":null,
    "Model Unique Name":"Magma: A Foundation Model for Multimodal AI Agents",
    "Category":"visual agents",
    "Detailed Category":null,
    "Dataset":"SeeClick, Vision2UI, Ego4D, EpicKitchen, Something‑Something v2, Open‑X‑Embodiment",
    "Paper":"https:\/\/arxiv.org\/pdf\/2502.13130",
    "GitHub":"https:\/\/github.com\/microsoft\/Magma",
    "HuggingFace":null,
    "Query1":"I showed it a screenshot of a software interface and just said “move this to the right,” and it actually did it. How can it understand both the image and my instruction like that?",
    "Query2":"I showed it a screenshot of a software interface and just said “move this to the right,” and it actually did it. How can it understand both the image and my instruction like that?",
    "Query3":"I tried giving it a goal like “make coffee,” and it broke it down into steps by watching what I was doing. What kind of model can act like a helpful visual assistant?",
    "Summary":"Magma is a unified multimodal AI foundation model that bridges verbal intelligence (understanding visual-language inputs) and spatial-temporal intelligence (planning and executing actions in digital and physical environments). Through joint pretraining on heterogeneous datasets—including labeled UIs with actionable “Set-of-Mark” annotations and videos with “Trace-of-Mark” motion traces—it learns to ground actions and plan multi-step sequences across tasks like GUI navigation and robotic manipulation. Magma achieves state-of-the-art performance on both specialized agentic benchmarks and general vision-language tasks, despite not using larger datasets, and offers unified, zero-shot adaptability across modalities. Code and model checkpoints are publicly available."
  },
  {
    "Model":null,
    "Model Unique Name":"Semantic-SAM: Segment and Recognize Anything at Any Granularity",
    "Category":"segmentation",
    "Detailed Category":null,
    "Dataset":"SA-1B, Objects365, COCO panoptic, ADE20k panoptic, PASCAL Part, PACO, PartImageNet",
    "Paper":"https:\/\/arxiv.org\/pdf\/2307.04767",
    "GitHub":"https:\/\/github.com\/UX-Decoder\/Semantic-SAM?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I clicked on an image and it instantly labeled not just the object, but also its parts—like wheels, handles, and buttons. How does it recognize things at that level of detail?",
    "Query2":"Even in crowded photos, it separated overlapping people and objects without a problem—and knew what each thing was. What lets it segment and recognize so precisely?",
    "Query3":"I used it on a street scene and it broke everything down—cars, signs, poles, even sidewalk lines. How can one model handle all that with such fine granularity?",
    "Summary":"Semantic‑SAM extends the Segment‑Anything paradigm by jointly learning semantic awareness and multi-granularity mask generation in an interactive model. It consolidates generic object, part, and SA‑1B segmentation datasets into a unified training pipeline with decoupled classifiers that distinguish both object-level and part-level semantics. By integrating a multi-choice learning strategy, the model produces multiple valid masks for each user click, spanning from entire objects down to fine-grained parts. This first attempt at combined training on SA‑1B, generic, and part datasets yields a model capable of interactive segmentation at any desired level of granularity, and training with SA‑1B further boosts its performance on panoptic and part segmentation tasks  ￼. The project is open-sourced with demos and full code available .\n"
  },
  {
    "Model":null,
    "Model Unique Name":"Segment Everything Everywhere All at Once",
    "Category":"segmentation",
    "Detailed Category":null,
    "Dataset":"MS COCO, DAVIS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2304.06718",
    "GitHub":"https:\/\/github.com\/UX-Decoder\/Segment-Everything-Everywhere-All-At-Once?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I dropped in a random photo, and it instantly outlined every object—big or small—without me selecting anything. How does it know what to segment without prompts?",
    "Query2":"Even in busy street scenes, it separated people, signs, cars, and even bags they were holding. What helps it detect and segment so many things at once?",
    "Query3":"I used it on a video and it kept track of every object frame by frame without losing consistency. How does this model handle space and time together like that?",
    "Summary":"SEEM presents a promptable, interactive segmentation model that supports diverse user inputs—including points, boxes, scribbles, masks, text prompts, and even referring regions from other images—by encoding them into a unified visual-semantic space. It employs a Transformer encoder-decoder architecture with memory prompts to retain segmentation history, enabling efficient multi-round interaction without re-running the image encoder. Designed for versatility, compositionality, interactivity, and open-vocabulary segmentation, SEEM delivers competitive performance across nine segmentation tasks—interactive, generic, referring, and video segmentation—often with minimal supervision (as little as 1%)  ￼. Its model-agnostic design and compositional prompting allow zero-shot generalization to novel combinations of prompts and modalities."
  },
  {
    "Model":null,
    "Model Unique Name":"SAM 2: Segment Anything in Images and Videos",
    "Category":"segmentation",
    "Detailed Category":null,
    "Dataset":"SA-1B, SA-V",
    "Paper":"https:\/\/arxiv.org\/pdf\/2408.00714",
    "GitHub":"https:\/\/github.com\/facebookresearch\/sam2?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I clicked once on a photo and it instantly highlighted the object I meant—no manual outlining or labels. How does it know what to segment with such little input?",
    "Query2":"Tried it on a video clip, and it followed the object through every frame without me doing anything extra. What allows it to handle segmentation across time so smoothly?",
    "Query3":"Even with transparent or overlapping objects, it still nailed the boundaries. What kind of model sees through complex visual scenes that well?",
    "Summary":"SAM 2 is a unified transformer-based foundation model that enables promptable segmentation for both images and videos by introducing streaming memory to carry prompt context across frames. The authors also developed a data engine to create SA‑V, the largest video segmentation dataset to date, collecting 35.5 million masks across 50,900 videos. Compared to its predecessor, SAM 2 achieves around 3× fewer user interactions for video segmentation and is 6× faster on image tasks, while maintaining or improving accuracy. It offers state-of-the-art performance in interactive video segmentation benchmarks and strong zero-shot generalization across 17 video and 37 image datasets. All model weights, dataset, and code are publicly released  ￼."
  },
  {
    "Model":null,
    "Model Unique Name":"Grounding DINO",
    "Category":"Zero-Shot Object Detection",
    "Detailed Category":null,
    "Dataset":"OpenImages v5 subset, Localized Narratives (OpenImages), LVIS subset, RefCOCO, RefCOCO+, RefCOCOg, gRefCOCO, COCO, Objects365 v1 & v2, V3Det, GRIT, GQA, Flickr30K Entities, ODinW13\/35",
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.05499",
    "GitHub":"https:\/\/github.com\/IDEA-Research\/GroundingDINO?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I typed in “a person holding a red umbrella” and clicked the image, and it found the exact person—even in a crowded street. How does it find such specific objects just from my words?",
    "Query2":"Even when I described unusual things like “a green suitcase next to a dog,” it still knew exactly what I meant. What helps this model understand such open-set queries?",
    "Query3":"Tried it across different photos, and I could just say what I wanted to find—it didn’t need categories or labels. How does it recognize anything I describe, even without training on it?",
    "Summary":"Grounding DINO integrates language understanding into the transformer-based DINO object detector, enabling detection of any text-specified concept—be it category names or complex referring expressions—without additional model modifications. It introduces a tight cross-modal fusion design with language-aware feature enhancements, language-guided query selection, and a cross-modality decoder, and is pre-trained across detection, grounding, and caption datasets. The model achieves state-of-the-art zero-shot open-vocabulary performance: 52.5 AP on COCO and 26.1 AP on the ODinW benchmark without seeing COCO labels during pre-training. It also demonstrates strong referring-expression comprehension and supports diverse downstream applications like grounding-aware image editing ."
  },
  {
    "Model":null,
    "Model Unique Name":"One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer",
    "Category":"3D Reconstruction",
    "Detailed Category":null,
    "Dataset":"COCO-WholeBody, MPII, Human3.6M, UBody, AGORA, EHF",
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.16160",
    "GitHub":"https:\/\/github.com\/IDEA-Research\/OSX",
    "HuggingFace":null,
    "Query1":"I uploaded a regular photo of someone standing, and it created a full 3D model—including face, hands, and body—in seconds. How does it rebuild a whole body from just one image?",
    "Query2":"Even with challenging poses or partially hidden limbs, it still captured the full shape accurately. What helps it recover detailed body structure so reliably?",
    "Query3":"I tried it for character animation, and the 3D mesh moved naturally with every joint. What kind of model makes realistic full-body rigs from a single frame?",
    "Summary":"OSX introduces a single-stage transformer-based pipeline that recovers detailed whole-body 3D meshes—including body, hands, and face—from a single image, without separate networks or manual post-processing. The model uses a Component-Aware Transformer, where a global body encoder provides a shared feature map for local face and hand decoders that apply feature-level upsample-crop and keypoint-guided deformable attention. This joint design preserves inter-part coherence, avoids noticeable artifacts in skeletal structure, and achieves top performance (first place) on the AGORA SMPL-X benchmark. The authors also contribute a new UBody dataset containing high-quality 2D and 3D annotations for partially visible upper-body images collected “in the wild.”"
  },
  {
    "Model":null,
    "Model Unique Name":"Recognize Anything Model",
    "Category":"Image Tagging",
    "Detailed Category":null,
    "Dataset":"COCO, Visual Genome, Conceptual Captions, SBU Captions, Conceptual 12M",
    "Paper":"https:\/\/arxiv.org\/pdf\/2306.03514",
    "GitHub":"https:\/\/github.com\/OPPOMKLab\/recognize-anything?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I uploaded a random photo, and it instantly listed everything in the scene—people, objects, even background elements. How does it recognize so much without needing specific prompts?",
    "Query2":"Even abstract things like “sunset,” “celebration,” or “mood” were picked up in the tags. What kind of model understands not just objects, but also context and feeling?",
    "Query3":"I used it on travel photos, and it generated perfect tags I could use for organizing or searching later. How does it create such useful and relevant descriptions?",
    "Summary":"Recognize Anything (RAM) is a foundation-level model for general-purpose image tagging that achieves strong zero-shot performance by training on 14 million image–text pairs without manual labels. RAM’s pipeline uses automatic text semantic parsing to generate large-scale tag annotations, a unified model architecture that combines captioning and tagging, and a data engine that refines tags through expansion and cleaning. The model outperforms CLIP, BLIP, and even fully supervised tagging models—including commercial APIs—across multiple benchmarks, recognizing over 6,400 common categories, and is released with open-source weights and code ."
  },
  {
    "Model":null,
    "Model Unique Name":"VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking",
    "Category":"3D detection & tracking",
    "Detailed Category":null,
    "Dataset":"nuScenes, Waymo, Argoverse2, KITTI",
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.11301",
    "GitHub":"https:\/\/github.com\/dvlab-research\/VoxelNeXt?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I tested it on LiDAR data from a moving car, and it tracked all the vehicles and pedestrians in real time. How does it detect and follow 3D objects so fast and accurately?",
    "Query2":"Even in dense traffic with lots of overlapping objects, it managed to keep everything separated and consistent. What helps it stay reliable in such complex scenes?",
    "Query3":"I used it on data from different cities, and it adapted without retraining. How does this model stay generalizable across environments and sensor types?",
    "Summary":"VoxelNeXt introduces a clean, fully sparse 3D object detection and tracking framework based entirely on sparse voxel features—eliminating anchors, center proxies, sparse-to-dense conversion, and NMS. Built on sparse convolutional networks, it achieves top-tier speed–accuracy trade-offs on benchmarks like nuScenes, Waymo, and Argoverse2, and outperforms all previous LiDAR-based methods in nuScenes tracking—it even set the leaderboard’s best result  ￼."
  }
]