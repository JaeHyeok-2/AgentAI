[
  {
    "Model Unique Name": "Classification-HuggingFace-falconsai-nsfw_image_detection",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-21k",
    "Paper": "https://arxiv.org/pdf/2010.11929",
    "Github": null,
    "HuggingFace": "https://huggingface.co/Falconsai/nsfw_image_detection",
    "Query1": "Will this picture be a problem before I post it in the company chat room? I see people taking it outdoors, please let me know if you don't mind.\n",
    "Summary_update": "This model excels at fast, reliable NSFW detection, with a transformer-based backbone offering strong image understanding and real-world applicability—well-suited for content safety pipelines."
  },
  {
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-18",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "Github": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-18",
    "Query1": "I'm going to post it on a shopping mall bulletin board, please let me know what's in this image. If there's a person, you can just say it's a person.\n",
    "Summary_update": "A lightweight deep residual network with 18 layers, using basic residual blocks featuring skip connections. Enables efficient training and inference, making it ideal for real-time or resource-constrained image classification tasks."
  },
  {
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-50",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "Github": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-50",
    "Query1": "Which trash category is the object shown in this image, 'plastic', 'paper', or 'glass'?\n",
    "Summary_update": "An even deeper 101-layer residual network that retains the bottleneck architecture, improving feature richness and representation power. Ideal for high-accuracy image classification and transfer learning on complex datasets."
  },
  {
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-101",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "Github": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-101",
    "Query1": "I took this picture at a park, and I can see a lot of people and animals. Specifically, what animals are there and how many people do you think there are?",
    "Summary_update": null
  },
  {
    "Model Unique Name": "Classification-HuggingFace-NTQAI-pedestrian_gender_recognition",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "PETA",
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/NTQAI/pedestrian_gender_recognition",
    "Query1": "It was taken on the street, but it was taken from a distance, so I can't see my face very well. Can you tell me if the people in the picture are male or female?",
    "Summary_update": "A fine-tuned BEiT-base vision transformer model for classifying pedestrian gender, trained on the PETA dataset. Achieves ~91% accuracy in distinguishing male vs. female individuals in surveillance-style images."
  },
  {
    "Model Unique Name": "Inpainting-LatentDiffusion",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "LAION-400M",
    "Paper": "https://arxiv.org/pdf/2112.10752",
    "Github": "https://github.com/CompVis/latent-diffusion",
    "HuggingFace": null,
    "Query1": "I want to erase the bag logo because it's taken incorrectly in the picture, can you fill it in like the background naturally?\n",
    "Summary_update": "A foundational latent diffusion model (LDM) operating in compressed latent space, enabling high-resolution image synthesis, inpainting, semantic scene generation, and super-resolution. Notably more efficient than pixel-based diffusion. Developed by CompVis on LAION‑400M."
  },
  {
    "Model Unique Name": "Colorization-DISCO-c0_2",
    "Category": "Img2Img",
    "Detailed Category": "Colorization",
    "Dataset": "ImageNet,COCO",
    "Paper": "https://menghanxia.github.io/projects/disco/disco_main.pdf",
    "Github": "https://github.com/MenghanXia/DisentangledColorization",
    "HuggingFace": null,
    "Query1": "I want to color this black-and-white photo. Paint the wood naturally with a green color scheme and a person with a skin tone feel.",
    "Summary_update": "A variation of the DISentangled Colorization framework featuring restrained color saturation (“c0.2”) to produce naturally toned results. It predicts a small, fixed set of global color anchors for each image, then generates smooth, consistent colors with balanced vibrancy. Tailored for subtle and realistic colorization needs."
  },
  {
    "Model Unique Name": "Colorization-DISCO-rand",
    "Category": "Img2Img",
    "Detailed Category": "Colorization",
    "Dataset": "ImageNet,COCO",
    "Paper": "https://menghanxia.github.io/projects/disco/disco_main.pdf",
    "Github": "https://github.com/MenghanXia/DisentangledColorization",
    "HuggingFace": null,
    "Query1": "Put the color in this black and white picture.Please draw different colors different from the random style.\n",
    "Summary_update": "A variant of the same framework that introduces randomness in anchor placement to generate diverse colorization outputs. Ideal for creative applications that benefit from varying color palettes and styles across runs."
  },
  {
    "Model Unique Name": "Deblur-MIMO-UNet-Plus",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "Github": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Query1": "Please clearly restore the face of the person who was photographed blurryly in the subway during the business trip.",
    "Summary_update": "MIMO UNet Plus excels at removing motion blur using a sophisticated yet efficient architecture that elegantly balances high-quality restoration and runtime performance, making it well-suited for enhancing blurred images from handheld and dynamic environments."
  },
  {
    "Model Unique Name": "Deblur-MIMO-UNet-RealBlur",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "Github": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Query1": "There's a cloudy picture taken on the street.Please restore the car number of automatically.\n",
    "Summary_update": "MIMO UNet RealBlur delivers efficient and robust deblurring by combining multi-scale processing, smart feature fusion, and artifact-free reconstruction—making it ideal for enhancing photos with real-world motion blur."
  },
  {
    "Model Unique Name": "Deblur-MIMO-UNet",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "Github": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Query1": "This is a blurry picture of a person's face taken last night due to shaking hands in the subway. Please restore it clearly with the details.\n",
    "Summary_update": "MIMO‑UNet delivers high-quality deblurring with impressive speed by compressing a coarse-to-fine framework into one efficient encoder-decoder network enhanced with smart feature fusion. It excels in restoring real-world blurred images from datasets like GoPro and RealBlur."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "I'm going to post it on a shopping mall bulletin board, please let me know what's in this image. If there's a person, you can just say it's a person",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Query1": "There is a picture of the blurry scenery shaken in the night view. Please restore the picture clearly.",
    "Summary_update": "MSSNet GoPro excels at removing motion blur and restoring sharpness by combining multi-scale processing, inter‑stage information sharing, and advanced loss functions, making it well-suited for enhancing real-world blurred images."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-L-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Query1": "This is a blurry street photo taken with hand tremors at night. Please restore it clearly with the details.\n",
    "Summary_update": "MSSNet L GoPro excels at restoring sharpness in motion-blurred images (e.g., from handheld cameras) by combining scalable multi-stage processing, feature-sharing across scales, and artifact-minimizing reconstruction—offering clear, detailed deblurring for real-world footage."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-S-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Query1": "The photos of the food I took at the restaurant were shaking. Please restore the food details clearly.\n",
    "Summary_update": "MSSNet S GoPro excels in speed-focused deblurring by offering a streamlined multi-stage architecture with inter-scale feature sharing and artifact-minimizing reconstruction—perfect for real-world, handheld motion-blurred images."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-RealBlurJ",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Query1": "Blurry picture taken on the street where the car was passing by. Please restore the license plate to make it look clear.\n",
    "Summary_update": "MSSNet RealBlurJ excels at recovering facial features, text, and fine textures from real-world blurred images through a structured, multi-scale approach that balances efficiency and restoration quality."
  },
  {
    "Model Unique Name": "Deblur-MSSNet-RealBlurR",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "Github": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Query1": "It's a picture of a blurred puppy during a walk, so please restore it so that the movement is clear",
    "Summary_update": "MSSNet RealBlurR excels in restoring real-world, motion-blurred photos — including faces, text, and fine textures — using a structured, multi-scale design that balances efficient processing with detailed image recovery."
  },
  {
    "Model Unique Name": "Denoise-SwinIR-Noise15",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The pictures of the beach daytime scenery are blurred due to noise, so please restore the sea water and the sky clearly and clearly.\n",
    "Summary_update": "SwinIR Noise15 is a specialized image restoration model based on Swin Transformer architecture, trained to remove light noise (σ=15) from images. It excels in denoising everyday photos, producing clean results while preserving fine textures and details under mild noise conditions."
  },
  {
    "Model Unique Name": "Denoise-SwinIR-Noise25",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "There are noise photos taken in the night picture like sand know.Please restore the sea and sky clean and clean.\n",
    "Summary_update": "SwinIR Noise25 uses Swin Transformer techniques to target moderate noise levels (σ=25). The model balances effective noise reduction with texture preservation, making it ideal for enhancing low-light, indoor, or slightly degraded photographs."
  },
  {
    "Model Unique Name": "Denoise-SwinIR-Noise50",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The interior lighting is dark, so there is a lot of noise in the product picture. Please remove the noise so that the product details can be seen clearly.\n",
    "Summary_update": "SwinIR Noise50 extends the SwinIR framework to handle heavy noise (σ=50), delivering robust denoising while retaining structural integrity. It’s well-suited for severely degraded images, such as high-ISO captures or aged media, restoring clarity and detail."
  },
  {
    "Model Unique Name": "Img2Txt-HuggingFace-Salesforce-blip-image-captioning-large",
    "Category": "Img2Txt",
    "Detailed Category": "Img2Txt",
    "Dataset": "COCO",
    "Paper": "https://arxiv.org/pdf/2201.12086",
    "Github": "https://github.com/salesforce/BLIP",
    "HuggingFace": "https://huggingface.co/Salesforce/blip-image-captioning-large",
    "Query1": "Let me show you a picture of this family outing. Please explain naturally who is doing what in the picture.\n",
    "Summary_update": "BLIP‑L is a unified vision-language model based on a ViT backbone and bootstrapped image-text pretraining. It supports both image captioning and visual question answering, and uses a two-stage approach: generating synthetic captions and filtering noisy web data for robust, real-world performance"
  },
  {
    "Model Unique Name": "ImgTxt2Txt-HuggingFace-dandelin-vilt-b32-finetuned-vqa",
    "Category": "ImgTxt2Txt",
    "Detailed Category": "ImgTxt2Txt",
    "Dataset": "GCC,SBU,VG,COCO,Flickr30k,VQAv2,NLVR2",
    "Paper": "https://arxiv.org/pdf/2102.03334",
    "Github": "https://github.com/dandelin/vilt",
    "HuggingFace": "https://huggingface.co/dandelin/vilt-b32-finetuned-vqa",
    "Query1": "Please let me know how many products there are in the display picture of this store and what kind they are.\n",
    "Summary_update": "ViLT VQAv2 is a lightweight, pure-transformer model for visual question answering. It processes text and image patches in a single Transformer stack (no CNN or region proposals), making it highly efficient while effectively handling VQA tasks. "
  },
  {
    "Model Unique Name": "SISR-CARN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Query1": "I'm sorry about the landscape photos I took while traveling. Please enlarge them twice so that they are clearer and show more details.\n",
    "Summary_update": "CARN 2× is a cascading residual network designed for 2× single-image super-resolution. Its architecture stacks residual blocks with cascading connections to efficiently propagate information, delivering high-quality upscaled images from low-resolution inputs. Trained on DIV2K and validated on standard benchmarks like Set5 and Urban100"
  },
  {
    "Model Unique Name": "SISR-CARN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Query1": "This sunrise photo is blurry and low resolution. Please enlarge it 3x to get clearer details.\n",
    "Summary_update": "CARN 3× applies the same cascading residual network architecture for 3× upscaling, balancing enhanced image detail recovery with computational efficiency. It reconstructs sharper textures and edges compared to simple bicubic interpolation, using the DIV2K training set."
  },
  {
    "Model Unique Name": "SISR-CARN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Query1": "My friend's face looks pixelated. Please enlarge it 4x to restore the facial details clearly.\n",
    "Summary_update": "CARN 4× targets 4× resolution enhancement with its cascading residual design. It preserves structural consistency and recovers intricate patterns in high-upscaling scenarios, demonstrating strong performance across DIV2K-derived datasets."
  },
  {
    "Model Unique Name": "SISR-CARN-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Query1": "The scanned document is blurry. Please restore it to its clarity by enlarging it twice so that the letters are clearly visible.",
    "Summary_update": "CARN-M 2× is the mobile-optimized version of CARN, tailored for 2× super-resolution with lower parameter count and resource needs. It retains cascading residual connections for effective detail enhancement while enabling efficient on-device execution."
  },
  {
    "Model Unique Name": "SISR-CARN-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Query1": "The travel photos are blurry and low resolution. Please enlarge them 3x to make the landscape details clearer.\n",
    "Summary_update": "CARN-M 3× brings the lightweight mobile cascade residual architecture to 3× upscaling, delivering high-fidelity visual enhancement under resource constraints. It enables sharper image reconstruction with minimal computational overhead. "
  },
  {
    "Model Unique Name": "SISR-CARN-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "Github": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Query1": "The scanned contract photo is blurry and the text is small. Please enlarge it by 4x to restore the text so that it is clear.\n",
    "Summary_update": "CARN-M 4× is the mobile-friendly version focused on 4× super-resolution, combining cascading residual blocks with efficient model size. It offers high-resolution reconstruction capabilities optimized for on-device or real-time applications"
  },
  {
    "Model Unique Name": "SISR-ESRT-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "Github": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Query1": "The whiteboard photo taken during the meeting has small and blurry letters. Please enlarge it by 2x so that all letters can be seen clearly.\n",
    "Summary_update": "ESRT 2× is an efficient hybrid model combining a lightweight CNN backbone with transformer-based modules for 2× single-image super-resolution. It dynamically adjusts feature-map resolution via high-frequency preserving blocks and leverages efficient multi-head attention to capture long-range dependencies with reduced GPU memory usage."
  },
  {
    "Model Unique Name": "SISR-ESRT-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "Github": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Query1": "The food photos taken at the restaurant have low resolution. Please enlarge them by 3x so that the texture and color of the food can be seen clearly.",
    "Summary_update": "ESRT 3× extends the efficient transformer-based SR approach to 3× upscaling, maintaining the lightweight CNN-plus-transformer structure. It balances effective texture reconstruction with low memory footprint—ideal for mobile or embedded super-resolution deployment. "
  },
  {
    "Model Unique Name": "SISR-ESRT-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "Github": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Query1": "The photo resolution for online sales is low. Please restore it by enlarging it by 4x so that the product material and label are clearly visible.\n",
    "Summary_update": "ESRT 4× provides 4× resolution enhancement, utilizing the same hybrid architecture of lightweight CNN and efficient transformers. It preserves detailed textures while supporting higher magnification with constrained computational demands."
  },
  {
    "Model Unique Name": "SISR-HAN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "Github": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Query1": "The photos of mountains and trees I took while traveling are blurry. Please enlarge them by 2x to make the details clearer.\n",
    "Summary_update": "HAN 2× delivers sharp and visually consistent upscaling by harnessing holistic attention across layers and nuanced channel-spatial contexts, making it particularly effective at enhancing image textures and preserving structural integrity on DIV2K-derived datasets  "
  },
  {
    "Model Unique Name": "SISR-HAN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "Github": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Query1": "This photo was taken at a sports field, but the runner is far away, so it's blurry. Please enlarge it by 3x so that the athlete's movements can be seen clearly.\n",
    "Summary_update": "HAN 3× delivers crisp and coherent 3× upscaling by blending holistic inter-layer context with precise channel-spatial focus, making it well-suited for detailed image enhancement tasks using DIV2K-trained models."
  },
  {
    "Model Unique Name": "SISR-HAN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "Github": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Query1": "The license plate in the photo taken in the parking lot is small and blurry. Please enlarge it 4 times and restore it so that the number can be read clearly.\n",
    "Summary_update": "HAN 4× employs holistic attention mechanisms for 4× super-resolution, effectively harnessing multi-layer context and spatial-channel signals to reconstruct high-fidelity textures and structure."
  },
  {
    "Model Unique Name": "SISR-HAN-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "Github": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Query1": "The drone footage is too far away, so it’s small and blurry. I’d like to zoom in 8x to see the terrain and building structures much more clearly.\n",
    "Summary_update": "HAN 8× delivers crisp, structurally consistent, and texture-rich results at high magnification by leveraging holistic and channel-spatial attention across layers—ideal for demanding super-resolution applications such as DIV2K-based imagery."
  },
  {
    "Model Unique Name": "SISR-DRN-S-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "Github": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Query1": "The art print photo I am using for display purposes is blurry. Please restore it to a high resolution by enlarging it 4x so that I can clearly see the brush stroke details and texture.\n",
    "Summary_update": "DRN‑S 4× is the small variant of the Deep Recursive Residual Network, designed for 4× single-image super-resolution. It uses recursive residual blocks for both local and global learning, achieving compact yet deep feature extraction (~4.8 M parameters) and delivering clear high-resolution images on datasets like Set5 and DIV2K. "
  },
  {
    "Model Unique Name": "SISR-DRN-S-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "Github": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Query1": "The satellite map screenshot is pixelated. Please upscale it by 8x so that the building outlines and roads are clearer.\n",
    "Summary_update": "DRN‑S 8× extends the small DRN architecture to 8× upscaling, maintaining its recursive residual design. It balances depth and parameter efficiency (~5.4 M) to reconstruct fine-grained detail in high-magnification scenarios, suitable for significant enlargement from low-resolution inputs."
  },
  {
    "Model Unique Name": "SISR-DRN-L-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "Github": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Query1": "The small figure photo is blurry. Please enlarge it 4 times to restore the figure's detailed decoration and expression.",
    "Summary_update": "DRN‑L 4× is the larger counterpart, offering deeper capacity (~9.8 M parameters) for 4× super-resolution. By stacking recursive residual techniques, it captures richer hierarchical features for improved texture restoration while retaining parameter efficiency. "
  },
  {
    "Model Unique Name": "SISR-DRN-L-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "Github": "https://github.com/guoyongcs/DRNN",
    "HuggingFace": null,
    "Query1": "These are low-quality video frames received from the Internet. Please upscale them 8x and restore them so that each frame's details come to life.\n",
    "Summary_update": "DRN‑L 8× scales the Deep Recursive Residual Network to 8× upscaling, preserving its strong feature hierarchy (~10 M parameters). Its recursive residual design enables detailed reconstruction even at very high magnification, ideal for applications like medical imaging and satellite enhancement."
  },
  {
    "Model Unique Name": "SISR-RCAN-it-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "Github": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Query1": "The photos of the bag you are uploading online are blurry. Please enlarge them by 2x so that the sewing lines and material texture can be clearly seen.\n",
    "Summary_update": "RCAN‑it 2× delivers clear, texture-preserving upscaling by combining a deep residual channel-aware architecture with improved training protocols, making it particularly effective for enhancing real-world images."
  },
  {
    "Model Unique Name": "SISR-RCAN-it-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "Github": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Query1": "Grandma's old black and white photo is blurry due to low resolution. Please enlarge it 3x to restore the facial outline and background clearly.\n",
    "Summary_update": "RCAN‑it 3× offers sharp, texture-aware 3× super-resolution by combining a deep residual architecture with adaptive channel attention and optimized training—ideal for enhancing real-world imagery with high fidelity."
  },
  {
    "Model Unique Name": "SISR-RCAN-it-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "Github": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Query1": "The oil painting I photographed in the gallery is blurry. Please enlarge it to 4x to show the brush strokes and colors in high resolution.\n",
    "Summary_update": "RCAN‑it 4× delivers high-fidelity, texture-preserving 4× upscaling by combining a deep residual channel-aware architecture with smart training improvements and robust dataset support—ideal for enhancing real-world images with fine detail and consistency."
  },
  {
    "Model Unique Name": "SISR-Swin2SR-Classical-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Query1": "The photos taken with a mobile phone have low quality because of the way they are saved. Please enlarge them to 2x the resolution and restore them clearly without compression artifacts.\n",
    "Summary_update": "Swin2SR 2× is a SwinV2 Transformer-based model focused on 2× image super-resolution under classical degradation (simple downsampling), combining the strengths of SwinIR and Transformer V2 to offer efficient feature extraction and restoration on clean low-resolution images."
  },
  {
    "Model Unique Name": "SISR-Swin2SR-Classical-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Query1": "The YouTube thumbnail is blurry. Please enlarge it to 4x so that the text and graphics are clear.\n",
    "Summary_update": "Swin2SR 4× applies the same SwinV2 Transformer improvements to 4× upscaling, reconstructing higher-resolution images with enhanced texture and structural detail, making it suitable for tasks like photo enlargement and detail-critical applications"
  },
  {
    "Model Unique Name": "SISR-Swin2SR-Compressed-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Query1": "I scanned an old photo album, but it was compressed and the quality was poor. Please enlarge it by 4x to bring out the details and colors of the face and background clearly.\n",
    "Summary_update": "Swin2SR Compressed 4× is designed for compressed-input super-resolution, handling heavily compressed (e.g., JPEG) images and simultaneously removing artifacts while upscaling 4×, leveraging the SwinV2 backbone to address restoration and upscale in one step. "
  },
  {
    "Model Unique Name": "SISR-Swin2SR-LightWeight-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Query1": "This is a product photo I took myself, but it shows signs of compression. Please enlarge it to 2x high resolution so that the texture and color can be seen clearly.\n",
    "Summary_update": "Swin2SR‑LightWeight 2× is a compact SwinV2 Transformer model tailored for lightweight 2× super-resolution, particularly effective on compressed or artifact-heavy inputs. It adapts the Swin2SR approach—with SwinV2-based shifted-window self-attention—to efficiently upscale compressed images using fewer parameters (~1 M), supporting low-resource deployment scenarios."
  },
  {
    "Model Unique Name": "SISR-Swin2SR-Real-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "Github": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Query1": "I copied an old black and white photo to a digital file, but there are a lot of artifacts. Please restore it so that the facial contour and background details come to life naturally and clearly by upscaling it by 4x.\n",
    "Summary_update": "Swin2SR‑Real 4× extends Swin2SR to real-world super-resolution, trained to handle natural distortions andJPEG compression imperfections before upscaling 4×, using Transformer V2 capacity to adapt to realistic image noise and artifacts."
  },
  {
    "Model Unique Name": "ObjDet-HuggingFace-facebook-detr-resnet-50",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "COCO",
    "Paper": "https://arxiv.org/pdf/2005.12872",
    "Github": "https://github.com/facebookresearch/detr",
    "HuggingFace": "https://huggingface.co/facebook/detr-resnet-50",
    "Query1": "Please check if there are any main objects in this photo, such as people, cars, or bicycles.\n",
    "Summary_update": "DETR is a streamlined, end-to-end object detector that leverages transformers to perform global reasoning and parallel prediction, eliminating traditional detection steps like NMS or anchor generation—ideal for clean and modular object detection workflows."
  },
  {
    "Model Unique Name": "ObjDet-HuggingFace-hustvl-yolos-small",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "ImageNet-1k",
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/hustvl/yolos-small",
    "Query1": "This photo was taken in a park. Can you tell me if there are people or pets among the trees?\n",
    "Summary_update": "YOLOS‑S offers real-time object detection with a streamlined transformer design—performing accurate bounding box and classification tasks without complex heuristics, making it well-suited for clean, end-to-end detection workflows."
  },
  {
    "Model Unique Name": "ObjDet-HuggingFace-valentinafeve-yolos-fashionpedia",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "Fashionpedia",
    "Paper": null,
    "Github": "https://github.com/valentinafeve/fine_tunning_YOLOS_for_fashion",
    "HuggingFace": "https://huggingface.co/valentinafeve/yolos-fashionpedia",
    "Query1": "Please check this image for accessories such as belts, scarves, and necklaces.",
    "Summary_update": "YOLOS‑Fashionpedia excels at fine-grained fashion item detection, combining transformer-driven object detection with deep fashion-specific training data—ideal for applications like e-commerce tagging, virtual fitting rooms, and detailed product search."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3-MobileNet-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Query1": "This is a picture of a living room. Please segment the sofa, desk, chair, window, and carpet.",
    "Summary_update": "DeepLabV3‑MobileNet balances speed, size, and segmentation quality, making it well-suited for real-world use cases like robotics, mobile apps, and lightweight backend services that require fast and accurate scene parsing."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3-ResNet50-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Query1": "This is an outdoor cafe photo. Please segment the tables, chairs, and plants.",
    "Summary_update": "DeepLabV3‑ResNet50 excels at fast, multiscale semantic segmentation by combining dilated convolutions, pyramid pooling, and skip-enriched structure, packaged into an efficient model ideal for both server and mobile deployment tasks."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3-ResNet101-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Query1": "This is a child’s room. Please segment the bed, desk, and toys.",
    "Summary_update": "DeepLabV3‑ResNet101 excels at producing precise, context-aware pixel-level segmentation, thanks to its deep encoding backbone, multi-scale feature extraction via ASPP, and effective boundary refinement—making it ideal for tasks requiring high spatial fidelity, like urban scene parsing or detailed environmental analysis."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3Plus-MobileNet-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Query1": "This is a farm image. Please segment the fields, fences, and livestock such as cows and horses.",
    "Summary_update": "DeepLabV3+ MobileNet delivers fast, resource-efficient semantic segmentation by merging multi-scale context via ASPP and lightweight convolutional operations—perfect for real-time, on-device image understanding."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3Plus-ResNet50-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Query1": "This is a construction site. Please segment the machinery, workers, and materials.",
    "Summary_update": "DeepLabV3+ ResNet50 excels at delivering precise, multiscale semantic segmentation by integrating dilated convolutions, pyramid pooling, and skip-enhanced decoding—ideal for tasks that demand detailed pixel-level understanding across complex scenes."
  },
  {
    "Model Unique Name": "Segmentation-DeepLabV3Plus-ResNet101-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "Github": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Query1": "This is a road CCTV image. Please segment the lanes, pedestrians, bicycles, and motorcycles.",
    "Summary_update": "DeepLabV3+ ResNet101 excels at context-aware and boundary-precise segmentation, thanks to its deep backbone, multi-scale feature aggregation, and effective decoder design—making it well-suited for complex dense prediction tasks in environments like urban scene understanding or high-resolution image analysis."
  },
  {
    "Model Unique Name": "Restoration-SwinIR-Jpeg10",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The night city photos are of low quality due to compression blur. Please clean them up so that the lights and buildings look natural.\n",
    "Summary_update": "SwinIR (JPEG 10) is designed for mild JPEG compression artifact removal, using a three-part Swin Transformer-based architecture composed of shallow feature extraction, deep feature processing with Residual Swin Transformer Blocks (RSTBs), and image reconstruction. It effectively mitigates blocks and ringing artifacts in lightly compressed images using content-aware windowed self-attention. "
  },
  {
    "Model Unique Name": "Restoration-SwinIR-Jpeg20",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The scanned document image was compressed and the letters were blurred. Please restore the letters and lines so that they are clear.\n",
    "Summary_update": "SwinIR (JPEG 20) tackles moderate JPEG artifact reduction. With the same SwinIR restoration pipeline, it specifically targets compression artifacts (e.g. 80% quality images), efficiently smoothing banding and preserving detail through deep Transformer-based context modeling. "
  },
  {
    "Model Unique Name": "Restoration-SwinIR-Jpeg30",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "Please organize the landscape photos I received from the Internet so that they look natural without color bleeding.\n",
    "Summary_update": "SwinIR (JPEG 30) is designed for severe JPEG artifact correction. Using its robust deep-swin Transformer backbone, it removes strong block artifacts and color shifts typical of heavy compression, restoring clarity and texture in highly compressed images."
  },
  {
    "Model Unique Name": "Restoration-SwinIR-Jpeg40",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "SNS food photos are compressed and the texture is blurry. Please make the colors and textures more realistic.\n",
    "Summary_update": "SwinIR (JPEG 40) addresses extreme JPEG artifact restoration on heavily compressed images (e.g. 60% quality or lower). Its Swin Transformer-based design excels at reconstructing compressed textures and suppressing compression noise and block patterns."
  },
  {
    "Model Unique Name": "Segmentation-HuggingFace-facebook-maskformer-swin-base-coco",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "ADE20k,Cityspaces,COCO,Mapillary Vistas",
    "Paper": "https://arxiv.org/pdf/2107.06278",
    "Github": null,
    "HuggingFace": "https://huggingface.co/facebook/maskformer-swin-base-coco",
    "Query1": "This is a picture of a park. Please color-code the benches, trees, and grass where people are sitting.\n",
    "Summary_update": "MaskFormer excels at delivering all-in-one segmentation—semantic, instance, and panoptic—through a mask-centric transformer design that predicts object masks and their labels in parallel, supported by strong multi-scale representation from the Swin backbone."
  },
  {
    "Model Unique Name": "Segmentation-HuggingFace-jonathandinu-face-parsing",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "CelebAMask-HQ",
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/jonathandinu/face-parsing",
    "Query1": "Here is a picture of me wearing earrings. Please use color to differentiate between the earrings and my face.",
    "Summary_update": "Segformer Face-Parsing excels at detailed facial part segmentation, offering precise pixel-level masks of facial features and adornments using a transformer-based encoder with a compact decoder—perfect for face editing, AR enhancement, and nuanced image analysis."
  },
  {
    "Model Unique Name": "Segmentation-HuggingFace-nvidia-segformer-b3-finetuned-ade-512-512",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": null,
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/nvidia/segformer-b3-finetuned-ade-512-512",
    "Query1": "This is a photo taken at a bus stop. I wish the bus, people, benches, and background were color-coded.\n",
    "Summary_update": "SegFormer B3 offers a simple, scalable, and efficient solution for pixel-wise semantic segmentation—ideal for parsing diverse scenes on datasets like ADE20K and Cityscapes."
  },
  {
    "Model Unique Name": "SISR-IMDN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "Github": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Query1": "The game screenshot is low resolution and the screen is blurry. Please enlarge it to 2x so that the border and text are clear.",
    "Summary_update": "IMDN 2× performs 2× single-image super-resolution, enhancing low-resolution images by processing input patches of any size and combining hierarchical features using multi-distillation blocks and contrast-aware attention. Designed for fast, efficient texture and detail reconstruction. "
  },
  {
    "Model Unique Name": "SISR-IMDN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "Github": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Query1": "The cat photo is pixelated. Please enlarge it 3x to restore the fur details.\n",
    "Summary_update": "IMDN 3× handles 3× image upscaling, extending the same architecture to recover more intermediate-level visual detail. Its information multi-distillation blocks and adaptive cropping strategy enable efficient and versatile mid-range super-resolution."
  },
  {
    "Model Unique Name": "SISR-IMDN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "Github": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Query1": "The book cover photo is too small to see the title. Please enlarge it to 4× so that the title and image are clearly visible.\n",
    "Summary_update": "IMDN 4× targets 4× super-resolution, using cascaded distillation blocks and contrast-aware attention to extract and fuse detailed textures. It employs adaptive cropping to upscale large images efficiently, maintaining clarity and structure in high-resolution outputs."
  },
  {
    "Model Unique Name": "SISR-LatticeNet-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "Github": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Query1": "The landscape capture is low resolution. Please restore it to twice its size so that the road, trees, and sky are clear.\n",
    "Summary_update": "LatticeNet 2× performs 2× single-image super-resolution, using innovative Lattice Blocks that integrate two residual blocks with attention-driven, butterfly-structured fusion, followed by a backward feature fusion module. This lightweight network delivers efficient, high-detail image enhancement while minimizing parameter count."
  },
  {
    "Model Unique Name": "SISR-LatticeNet-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "Github": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Query1": "The picture of my cat is blurry. Please enlarge it 3x so I can see each hair strand clearly.\n",
    "Summary_update": "LatticeNet 3× handles 3× super-resolution, leveraging the same architecture of attention-enhanced Lattice Blocks and backward feature fusion to upscale mid-resolution images. It excels at reconstructing textures and edges from moderate-resolution sources in a lightweight framework."
  },
  {
    "Model Unique Name": "SISR-LatticeNet-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "Github": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Query1": "This is a photo of food posted on SNS, but the details of the ingredients are buried. Please enlarge it by 4 times to bring out the texture and color.\n",
    "Summary_update": "LatticeNet 3× handles 3× super-resolution, leveraging the same architecture of attention-enhanced Lattice Blocks and backward feature fusion to upscale mid-resolution images. It excels at reconstructing textures and edges from moderate-resolution sources in a lightweight framework."
  },
  {
    "Model Unique Name": "SISR-RCAN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "Github": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Query1": "The text in the camping video appears small. Please enlarge the screen by 2x so that you can see the menu and instructions clearly.\n",
    "Summary_update": "RCAN-2x is a deep learning model for 2× single-image super-resolution that enhances low-resolution images by reconstructing high-frequency details. It uses a Residual-in-Residual (RIR) architecture with Channel Attention (CA) modules to effectively extract and emphasize texture-rich features while suppressing redundant information."
  },
  {
    "Model Unique Name": "SISR-RCAN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "Github": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Query1": "This is a picture of a dessert taken at a cafe, but I can't see the cream details. Please enlarge it 3x so that the texture can be seen vividly.\n",
    "Summary_update": "RCAN-3x is a deep neural network designed for 3× single-image super-resolution, aiming to restore mid-scale details from lower-resolution inputs. It applies a Residual-in-Residual framework combined with Channel Attention to selectively focus on informative feature channels and produce sharper, more natural image reconstructions."
  },
  {
    "Model Unique Name": "SISR-RCAN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "Github": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Query1": "This is a landscape shot with a drone, but the details of the buildings, cars, and people are too small. Please enlarge it by 4x so that the entire composition can be seen clearly.\n",
    "Summary_update": "RCAN-4x is a convolutional model built for 4× single-image super-resolution, which reconstructs fine textures and structural details from heavily downsampled images. It leverages a deep Residual-in-Residual backbone and Channel Attention mechanism to prioritize high-frequency information for high-quality image upscaling."
  },
  {
    "Model Unique Name": "SISR-RCAN-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "Github": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Query1": "The rural landscape taken by a drone is blurry. Please restore it to 8× so that the houses and trees in the distance can be clearly seen.\n",
    "Summary_update": "RCAN-8x is a high-scale super-resolution model developed for 8× single-image enlargement, targeting ultra-low-resolution inputs. By stacking deep Residual-in-Residual groups with Channel Attention, it is capable of recovering sharp edges and textures even from severely degraded image data."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "I took a picture of a sports game, but the players' license plates are small. Please enlarge it twice so that the numbers and faces are clear.\n",
    "Summary_update": "SwinIR Classical DF2K 2x is a transformer-based model for 2× single-image super-resolution that enhances image quality using a shallow feature extractor, deep Residual Swin Transformer Blocks (RSTBs), and a reconstruction layer. It is trained on bicubic-downsampled DF2K images and excels at restoring textures and sharp edges from low-resolution inputs."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The outdoor mural photo is too small to show the artistic details. Please restore it to 3x so that the colors and lines are clear.\n",
    "Summary_update": "SwinIR Classical DF2K 3x is a model designed for 3× single-image super-resolution, built on Swin Transformer architecture with residual attention mechanisms. It processes bicubic-degraded images and effectively recovers medium-scale image structures while maintaining high visual fidelity."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The drone shot of the landscape is small and I can't see the entire composition. Please enlarge it by 4x to make the entire scene clearer.\n",
    "Summary_update": "SwinIR Classical DF2K 4x is a super-resolution model aimed at 4× upscaling tasks, using a combination of Swin Transformer layers and hierarchical feature aggregation. Trained on DF2K, it delivers high-quality reconstructions by learning long-range dependencies and fine-grained image features."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The license plate photo is too blurry. Please enlarge it 8 times so that the number can be seen clearly.\n",
    "Summary_update": "SwinIR Classical DF2K 8x is a deep learning model for 8× single-image super-resolution, optimized for recovering highly detailed outputs from extremely low-resolution inputs. It leverages window-based attention in Residual Swin Transformer Blocks to model global context and restore complex structures with minimal artifacts."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The photo of the child's drawing is small. Please enlarge it twice so that the colors and lines are clear.\n",
    "Summary_update": "SwinIR Classical DIV2K 2x is a super-resolution model for 2× image upscaling, designed to recover fine image details from bicubic-downsampled low-resolution inputs. It uses a combination of shallow convolutional layers, deep Residual Swin Transformer Blocks (RSTBs), and window-based self-attention to reconstruct high-quality outputs with structural fidelity."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The streetlights are blurry at night. Please zoom in 3x to make the lights and letters clearer.\n",
    "Summary_update": "SwinIR Classical DIV2K 3x is a deep learning model for 3× single-image super-resolution that restores medium-scale textures and edges from degraded inputs. Built on the Swin Transformer backbone, it applies residual attention-based modules to learn contextual information for enhanced visual recovery."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The SNS food photos are blurry. Please enlarge them 4x so that you can see all the toppings and textures.\n",
    "Summary_update": "SwinIR Classical DIV2K 4x is a transformer-based image restoration model used for 4× upscaling in super-resolution tasks. Leveraging long-range feature interactions through windowed attention, it efficiently reconstructs sharp and natural images from low-resolution sources created via bicubic degradation.\n"
  },
  {
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The stadium match photos are small. Please enlarge them 8x so that the players can be recognized.\n",
    "Summary_update": "SwinIR Classical DIV2K 8x is a high-scale single-image super-resolution model that performs 8× enlargement of low-resolution images. Using hierarchical feature processing and deep Residual Swin Transformer Blocks, it captures both local and global context to produce detailed and artifact-free reconstructions from severely downsampled inputs."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The game capture text is not visible. Please zoom in 2x so that the buttons and menus are clearly visible.\n",
    "Summary_update": "SwinIR LightWeight DIV2K 2x is a transformer-based model for 2× single-image super-resolution, designed with a reduced parameter count for efficiency on low-power devices. It combines lightweight convolutional layers and Residual Swin Transformer Blocks to upscale bicubic-degraded images while balancing quality and computational cost."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The photo of the baked bread is small. Please make it clearer at 3× so that you can see the texture and ingredient details.\n",
    "Summary_update": "SwinIR LightWeight DIV2K 3x is a super-resolution model optimized for 3× upscaling of low-resolution images using a lightweight Swin Transformer architecture. By reducing model complexity while preserving attention-based restoration capabilities, it enables faster inference suitable for real-time applications on edge devices.\n\n"
  },
  {
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The map image taken with a smartphone is too small to see. Please enlarge it 4 times so that the road names and terrain can be read.\n",
    "Summary_update": "SwinIR LightWeight DIV2K 4x is a 4× single-image super-resolution model that maintains high image quality with minimal resource usage. It employs a compact version of the SwinIR framework, using window-based attention and lightweight residual blocks to reconstruct fine textures from compressed inputs on devices with limited compute."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Real-DFO-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The street photo taken at night is blurry. Please restore it by 2x so that the street lights and people's outlines are clear.\n",
    "Summary_update": "SwinIR Real DFO 2x is a 2× single-image super-resolution model designed for real-world image restoration using the Swin Transformer architecture. Unlike models trained on synthetic bicubic degradation, it is trained on data from the RealSR dataset (DFO) to handle authentic low-quality inputs, making it effective for enhancing images with unknown or complex degradations."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Real-DFO-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The cat's pupils are captured small. Please enlarge the image by 4x to see the details of the pupils.\n",
    "Summary_update": "SwinIR Real DFO 4x is a transformer-based model for 4× super-resolution, specifically trained on the RealSR dataset (DFO) to upscale real-world low-resolution images. It leverages Residual Swin Transformer Blocks and window-based self-attention to restore fine textures and structural consistency in naturally degraded inputs."
  },
  {
    "Model Unique Name": "SISR-SwinIR-Real-DFOWMFC-64-L-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "Github": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Query1": "The text in the dessert photo cupcakes is small. Please enlarge it 4x so that you can see the text and ingredient details together.\n",
    "Summary_update": "SwinIR Real DFOWMFC 4x is a high-capacity 4× super-resolution model trained on the RealSR DFOWMFC dataset, tailored for restoring high-frequency details in real-world degraded images. Using a large Swin Transformer architecture with deeper residual attention modules, it is well-suited for complex scenes where conventional synthetic degradation models fall short."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-AbsoluteReality",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Turn this rainy city alley photo into a cinematic scene with neon lights reflecting off it.\n",
    "Summary_update": "SD v1.5 - AbsoluteReality is a text-to-image generation model based on Stable Diffusion v1.5, designed to produce realistic and photographic imagery. It focuses on natural lighting, skin tones, and environment coherence, making it suitable for photorealistic portraits, products, and cinematic scenes from text prompts."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-majicMix-sombre",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "This photo of a person in front of a tunnel, please adjust the background to a blurred tone and only the face to a clear tone.\n",
    "Summary_update": "SD v1.5 - majicMix-sombre is a text-to-image model built on Stable Diffusion v1.5, optimized for generating atmospheric, moody, and low-light imagery. It excels at delivering dark-toned compositions with cinematic shading and emotional depth, ideal for dramatic scenes and ambient character art."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-SimpleMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please adjust the store front photos to create a comfortable atmosphere by adjusting the lighting and brightness evenly.\n",
    "Summary_update": "SD v1.5 - SimpleMix is a Stable Diffusion v1.5 variant that balances realism and artistic clarity, emphasizing clean lines, soft lighting, and general-purpose versatility. It is designed for high usability across portraits, products, and illustration-style scenes with a polished, neutral aesthetic."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ToonYou",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please recreate this princess character image like a fairy tale illustration with clear lines and bright colors.\n",
    "Summary_update": "SD v1.5 - ToonYou is a text-to-image diffusion model fine-tuned for cartoon and illustration generation. Based on Stable Diffusion v1.5, it specializes in stylized character rendering with smooth outlines, vivid colors, and animation-friendly features, ideal for storybooks, avatars, and visual branding."
  },
  {
    "Model Unique Name": "SISR-Any-LIIF-EDSR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": "DIV2K,CelebAHQ,Benchmark",
    "Paper": "https://arxiv.org/pdf/2012.09161",
    "Github": "https://github.com/yinboc/liif",
    "HuggingFace": null,
    "Query1": "Please enlarge this photo 7.5 times so that the building exterior texture and window corners are clearly restored.\n",
    "Summary_update": "LIIF - EDSR Baseline is a super-resolution model that performs arbitrary-scale upsampling by learning continuous implicit image functions from discrete image features. It uses an EDSR backbone to extract deep features and predicts RGB values at any coordinate, allowing flexible resolution generation without fixed scaling factors."
  },
  {
    "Model Unique Name": "SISR-Any-LIIF-RDN",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": "DIV2K,CelebAHQ,Benchmark",
    "Paper": "https://arxiv.org/pdf/2012.09161",
    "Github": "https://github.com/yinboc/liif",
    "HuggingFace": null,
    "Query1": "enlarge the aerial photos taken by this drone by 20.2x so that the small buildings and roads are not blurred.\n",
    "Summary_update": "LIIF - RDN is a continuous-resolution super-resolution model that combines a Residual Dense Network (RDN) feature extractor with an implicit function predictor. It enables arbitrary-scale image generation by inferring pixel values at non-grid coordinates, making it suitable for tasks requiring resolution-adaptive output from a single low-resolution input."
  },
  {
    "Model Unique Name": "SISR-Any-LTE-EDSR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "Github": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Query1": "Please enlarge this interior shot by 4x so that you can see the details of the wallpaper pattern and lighting texture.\n",
    "Summary_update": "LTE - EDSR Baseline is an arbitrary-scale super-resolution model that uses an EDSR backbone to extract image features and a Local Texture Estimator to guide high-frequency detail synthesis. It predicts pixel values at arbitrary coordinates by leveraging local textures, enabling precise and resolution-adaptive image reconstruction."
  },
  {
    "Model Unique Name": "SISR-Any-LTE-RDN",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "Github": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Query1": "enlarge this external poster image (including the person's face) by 3.5x. Make sure that both the person's expression and the text are clear.\n",
    "Summary_update": "LTE - RDN is an arbitrary-scale image super-resolution model that combines a Residual Dense Network with a Local Texture Estimator. It enables continuous-resolution output by predicting pixel values conditioned on both hierarchical features and local texture cues, providing fine-grained detail restoration at any target scale."
  },
  {
    "Model Unique Name": "SISR-Any-LTE-SwinIR-LIIF",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "Github": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Query1": "enlarge this cafe sign photo by 3.2x. Please make sure the sign logo and text are not blurred when taken from a distance.\n",
    "Summary_update": "LTE - SwinIR - LIIF integrates SwinIR-based feature extraction with LIIF-style implicit function prediction and LTE's texture guidance. It is designed for continuous-scale super-resolution, producing sharp and natural images across a wide range of resolutions by explicitly modeling local texture patterns in the image domain."
  },
  {
    "Model Unique Name": "SISR-Any-LTE-SwinIR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "Github": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Query1": "Please enlarge this business card photo to twice its size so that the small text and contact information are not blurred.\n",
    "Summary_update": "LTE - SwinIR is a transformer-enhanced variant of the LTE framework for arbitrary-scale super-resolution. It uses SwinIR to extract rich multi-scale features and applies the Local Texture Estimator to refine pixel predictions at any resolution, effectively restoring detailed textures and structural consistency from low-resolution inputs."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-1",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please turn this landscape into a surreal illustration. Feel free to interpret the colors and shapes.\n",
    "Summary_update": "SD v1.1 is a text-to-image diffusion model that generates images from natural language prompts using a latent diffusion architecture. It establishes the foundation for prompt-based image synthesis, capable of producing coherent and creative outputs across general visual concepts."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-2",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "remake this portrait in a pop art style. I would like a style with strong color contrast and free composition.\n",
    "Summary_update": "SD v1.2 is an improved version of the Stable Diffusion v1.1 model, offering enhanced prompt fidelity and visual coherence. It retains the same latent diffusion architecture while refining sampling behavior to improve realism, composition, and stylistic diversity in generated images."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-4",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "retouch this product photo so that it is smooth and free of impurities. Please make the background blurry under soft lighting.\n",
    "Summary_update": "SD v1.4 is a widely adopted text-to-image generation model built on the Stable Diffusion architecture, known for its balanced output quality, speed, and creative flexibility. It excels at generating diverse and semantically aligned images from open-ended prompts, and supports artistic, realistic, and conceptual styles."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-5",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please reproduce this city night view photo as if it were in 4K high definition. I would like to see the lights and building exteriors clearly.\n",
    "Summary_update": "SD v1.5 is a fine-tuned Stable Diffusion model that improves upon v1.4 by enhancing detail fidelity, image sharpness, and prompt responsiveness. It is widely used for both artistic and photorealistic image generation tasks, supporting a wide range of creative workflows through stable and high-quality visual output."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ArteYou",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please illustrate this landscape painting with magical forest lighting and soft shading.\n",
    "Summary_update": "SD v1.5 – ArteYou is a text-to-image model enhanced with an Arcane-inspired illustration style. It combines flat design with subtle 3D shading, producing clean color blocks, gentle gradients, and a polished, artful appearance"
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Artius_v1.5",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please redesign the product image to look like a high-resolution illustration, with clear lines and details.\n",
    "Summary_update": "SD v1.5 - Artius v1.5 is a text-to-image generation model based on Stable Diffusion v1.5, fine-tuned for producing elegant, art-inspired visuals with a cinematic and realistic finish. It balances clarity and mood, making it well-suited for concept design, portraiture, and storytelling visuals with a refined, expressive aesthetic."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-CCDDA_ArtStyle",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Re-create handwritten menus as antique works of art. Bring out the texture of the paper and the feel of cursive writing.\n",
    "Summary_update": "SD v1.5 – CCCDA ArtStyle is a Stable Diffusion variant tailored for blending classical painting techniques with contemporary art sensibilities. It emphasizes layered textures, painterly brushwork, and refined color palettes, ideal for artistic visuals that feel both timeless and modern."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-colorful",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "retouch this bouquet photo with vivid colors so that the colors of each flower stand out more.",
    "Summary_update": "SD v1.5 - colorful is a text-to-image model fine-tuned for generating vibrant, saturated, and color-rich outputs. It emphasizes vivid palettes and bold contrasts, making it well-suited for stylized compositions, fantasy art, and visually dynamic illustrations based on user prompts."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ConsistentFactor",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Using this portrait, create multiple cuts with similar expressions and hairstyles, so that the facial features remain consistent.\n",
    "Summary_update": "SD v1.5 - ConsistentFactor is a Stable Diffusion variant designed for consistent subject rendering across multiple generations. It is optimized for identity preservation and character coherence, making it useful in workflows like character design, branding, and storyboard generation."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-CyberRealistic",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Retouch your portraits for banners with cinematic, high-quality lighting and realistic textures for a hyper-realistic look.\n",
    "Summary_update": "SD v1.5 - CyberRealistic is a photorealistic variant of Stable Diffusion v1.5 trained to produce clean, natural-looking human figures and environments. It is frequently used for portrait-style outputs and realistic human modeling with well-balanced lighting and skin tones."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-DreamShaper",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Transform this landscape photo into a soft and dreamy art style.\n",
    "Summary_update": "SD v1.5 - DreamShaper is a hybrid text-to-image generation model that blends realism and fantasy aesthetics. It is designed to create dreamlike compositions, character-centric artworks, and stylized scenery with a soft, painterly finish."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-epiCRealism_newEra",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch the CEO's profile photo in a style that uses high-quality lighting and realistic skin texture.\n",
    "Summary_update": "SD v1.5 - epiCRealism newEra is a fine-tuned model focused on cinematic realism and dramatic lighting. It generates highly detailed, emotionally expressive imagery, and is particularly suited for creating filmic scenes, concept art, and moody environments from textual prompts."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-GhostMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Transform your character illustrations into a hybrid style that combines realistic details and soft coloring.\n",
    "Summary_update": "SD v1.5 - GhostMix is a stylized text-to-image model designed to produce ethereal, surreal, and visually haunting compositions. It emphasizes spectral lighting, soft diffusion, and ghostly atmospheres, making it well-suited for fantasy scenes, spiritual motifs, and dreamlike artwork."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-henmixReal",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch this employee profile photo into a soft, realistic portrait as if lit by natural light.\n",
    "Summary_update": "SD v1.5 - henmixReal is a realistic-style model fine-tuned for clean, photographic human rendering and sharp environmental detail. It performs well in producing lifelike portraits and structured scenes, with a focus on natural lighting, skin tone balance, and realism across a wide range of prompts."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ICBINP",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please make the representative's face photo look like a real photo by expressing the skin pores and hair details precisely.\n",
    "Summary_update": "SD v1.5 - ICBINP (\"I Can't Believe It's Not Photography\") is a text-to-image model optimized for ultra-realistic outputs that closely resemble high-resolution photographs. It is often used for producing commercial-grade visuals, synthetic portraits, and simulated product imagery with photographic fidelity."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-IDSM",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch the photos of clothes worn by models with soft and realistic tones that make the skin, fabric, and light appear lifelike.\n",
    "Summary_update": "SD v1.5 - IDSM is a diffusion model focused on delivering stylish, expressive, and character-rich images with a painterly or semi-realistic feel. It blends imaginative design with subtle realism, making it suitable for creative concept art, fashion poses, and stylized renderings."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ImpressionismOil",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Transform your store exterior photo into an impressionist oil painting style. Express the soft brush texture and warm colors.\n",
    "Summary_update": "SD v1.5 - ImpressionismOil is a text-to-image generation model fine-tuned for creating artwork in the style of impressionist oil paintings. It emphasizes brushstroke texture, warm lighting, and painterly aesthetics, making it ideal for landscape art, portraits, and classical-themed visuals."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-LemonTeaMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch this cafe interior image with a soft conversational feel and warm yellow tones. For a cozy atmosphere like a hand-painted painting.\n",
    "Summary_update": "SD v1.5 - LemonTeaMix is a stylized Stable Diffusion model that produces light-toned, warm, and softly colored images with a cozy and uplifting feel. It is well-suited for casual illustrations, lifestyle visuals, and friendly, approachable scenes."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-majicMix-realistic",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch the product model photos to make them look natural and realistic. Express Asian skin tones and facial expressions in detail.\n",
    "Summary_update": "SD v1.5 - majicMix-realistic is a high-fidelity text-to-image model tuned for realism and clarity. It focuses on generating sharp, lifelike visuals with accurate lighting and textures, making it useful for photorealistic portraiture, product rendering, and environmental scenes."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-NextPhoto",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch the menu photos to make them look like high-quality photos with DSLR-level clarity and natural light.",
    "Summary_update": "SD v1.5 - NextPhoto is a Stable Diffusion variant trained for high-resolution photographic generation. It is optimized to simulate professional DSLR-like image quality, capturing fine textures and lighting consistency suitable for realistic portraits, fashion, and commercial imagery."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-PirsusEpicRealism",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch the portrait to make it look as realistic as a movie poster, with lighting, expressions, and textures.\n",
    "Summary_update": "SD v1.5 - PirsusEpicRealism is a cinematic-style model focused on highly detailed, dramatic, and photorealistic scene generation. It emphasizes epic composition, rich lighting, and depth, making it suitable for storytelling visuals, concept art, and realistic fantasy scenes."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-QGO-PromptingReal",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Retouch your store exterior photos with realistic textures that bring the walls, signs, and lighting to life.\n",
    "Summary_update": "SD v1.5 - QGO-PromptingReal is a Stable Diffusion variant fine-tuned for consistent and prompt-aligned photorealistic image generation. It is built to respond precisely to textual inputs while preserving realism, making it reliable for coherent visual storytelling."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Realisian",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch this staff profile photo with warm tones and soft lighting to convey the emotion well.\n",
    "Summary_update": "SD v1.5 - Realisian is a realism-focused model that generates highly natural images with balanced skin tones, lighting, and sharpness. It is effective for producing portraits, lifestyle photography, and realistic people-centric visuals with minimal stylistic distortion."
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Reliberate",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please make this product photo a clean, refined image without any impurities. Make it look like an actual product cut from a shopping mall.\n",
    "Summary_update": "SD v1.5 - Reliberate is a refined Stable Diffusion model designed for creative illustration and semi-realistic styles. It blends clarity with imaginative flexibility, producing images that are visually clean while supporting varied artistic genres from fantasy to slice-of-life.\n"
  },
  {
    "Model Unique Name": "Txt2Img-StableDiffusionV1-RunDiffusionFX",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Query1": "Please retouch the staff group photo with cinematic colors and textures. Make it atmospheric and natural.\n",
    "Summary_update": "SD v1.5 - RunDiffusion FX is a text-to-image generation model specialized for polished, visually dynamic compositions with studio-lighting quality. It delivers detailed, stylish visuals ideal for editorial-style portraits, fashion, and media-oriented production."
  },
  {
    "Model Unique Name": "Txt2Txt-HuggingFace-facebook-bart-large-cnn",
    "Category": "Txt2Txt",
    "Detailed Category": "Txt2Txt",
    "Dataset": "CNN Daily Mail",
    "Paper": "https://arxiv.org/pdf/1910.13461",
    "Github": "https://github.com/facebookresearch/fairseq/tree/main/examples/bart",
    "HuggingFace": "https://huggingface.co/facebook/bart-large-cnn",
    "Query1": "Please summarize this 300-character blog post in three key sentences.\n",
    "Summary_update": "BART-L is a large text-to-text transformer model fine-tuned for abstractive summarization. It takes long-form documents and generates concise summaries while maintaining semantic coherence, making it suitable for news, article, and report summarization tasks."
  },
  {
    "Model Unique Name": "Txt2Txt-HuggingFace-microsoft-Promptist",
    "Category": "Txt2Txt",
    "Detailed Category": "Txt2Txt",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/microsoft/LMOps/tree/main/promptist",
    "HuggingFace": "https://huggingface.co/microsoft/Promptist",
    "Query1": "Please make this image prompt more vivid: ‘a cat on a chair in the sun’\n",
    "Summary_update": "Promptist is a text-to-text transformer model designed to rewrite or enhance user prompts for better performance in downstream tasks, such as text-to-image generation. It improves prompt clarity and structure, helping users create more effective and visually aligned AI-generated outputs."
  },
  {
    "Model Unique Name": "Txt2Voice-HuggingFace-espnet-fastspeech2_conformer_with_hifigan",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/espnet/fastspeech2_conformer_with_hifigan",
    "Query1": "Please read this sentence in a soft female voice: ‘Hello, the weather is sunny today.’\n",
    "Summary_update": "FastSpeech2Conformer is a text-to-speech (TTS) model that converts written sentences into natural-sounding speech using a FastSpeech 2 backbone and Conformer blocks, paired with a HiFi-GAN vocoder. It generates fluent, expressive audio with low latency and high intelligibility."
  },
  {
    "Model Unique Name": "Txt2Img-HuggingFace-prompthero-openjourney-v4",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/prompthero/openjourney-v4",
    "Query1": "Please create a full-shot social media promotional illustration with the theme of 'Sunset Beach'. Please make it a warm and emotional landscape.\n",
    "Summary_update": "OpenJourney v4 is a text-to-image generation model based on Stable Diffusion, fine-tuned for producing artistic and conceptual visuals in the style of fantasy and digital concept art. It is ideal for creating illustrations, environments, and character art from text prompts."
  },
  {
    "Model Unique Name": "Txt2Voice-HuggingFace-suno-bark",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "Github": null,
    "HuggingFace": "https://huggingface.co/suno/bark",
    "Query1": "Please make it sound soft and trustworthy by saying, ‘We look forward to hearing your valuable opinions.’\n",
    "Summary_update": "Bark is a text-to-speech model that generates natural speech, expressive voice styles, and even non-verbal sounds like laughter or music. It supports multilingual inputs and is capable of generating highly realistic, emotion-infused audio outputs from text.\n"
  },
  {
    "Model Unique Name": "Voice2Txt-nvidia-parakeet-tdt-1.1b",
    "Category": "Snd2Txt",
    "Detailed Category": "Voice2Txt",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/NVIDIA/NeMo",
    "HuggingFace": "https://huggingface.co/nvidia/parakeet-tdt-1.1b",
    "Query1": "Please receive the interview recording file and convert the audio into text.\n",
    "Summary_update": "Parakeet TDT is a speech-to-text (S2T) model developed by NVIDIA for transcribing spoken audio into text. It uses a transformer-based architecture trained on multilingual datasets, providing accurate and efficient transcription suitable for general-purpose and low-latency applications."
  },
  {
    "Model Unique Name": "WeatherRemoval-CLAIO-DeHaze",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "Github": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Query1": "The photo in front of this store came out blurry due to the fog. Please clean it up so that the background is clear and sharp.\n",
    "Summary_update": "CLAIO-DeHaze is an image-to-image restoration model designed to remove haze from outdoor photographs. It is part of the CL All-In-One framework and is specifically trained on the OTS dataset to recover clear visual details and natural colors from hazy or fog-obscured scenes."
  },
  {
    "Model Unique Name": "WeatherRemoval-CLAIO-DeRain",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "Github": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Query1": "There are raindrop marks on this outdoor menu board photo. Please remove the raindrops so that the text and background are clean.\n",
    "Summary_update": "CLAIO-DeRain is a weather removal model trained to eliminate rain streaks and motion blur from rainy images. Using single-pass inference, it restores clarity and sharpness while preserving fine details, and is evaluated for generalization on datasets like OTS and Snow100K."
  },
  {
    "Model Unique Name": "WeatherRemoval-CLAIO-DeSnow",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "Github": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Query1": "The scene outside this window is hazy with snow and frost. Please remove all the snow specks and frost marks.\n",
    "Summary_update": "CLAIO-DeSnow is a deep learning model developed for removing snow artifacts such as translucent flakes and dense snow blobs from images. As part of the CLAIO framework, it performs one-step de-snowing and is trained on Snow100K to enhance visibility in snowy environments."
  },
  {
    "Model Unique Name": "Inpainting-CTSDG-CelebA",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "CelebA",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "Github": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Query1": "There are some parts of this representative's face that are blurry or have flaws. Please restore the natural facial contour, eyebrows, and skin texture.",
    "Summary_update": "CTSDG - CelebA is an image inpainting model trained on the CelebA dataset, designed to restore missing or corrupted facial regions. It uses a coarse-to-fine structure-aware approach with dual generators to fill in masked areas realistically, maintaining symmetry and texture continuity in face images."
  },
  {
    "Model Unique Name": "Inpainting-CTSDG-Paris",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Paris StreetView",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "Github": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Query1": "Some of the doors and windows in the store exterior photos are damaged. Please fill them in so that the window frames and brick structures can be restored naturally.\n",
    "Summary_update": "CTSDG - Paris is an inpainting model trained on Paris StreetView images, aimed at restoring architectural and urban scene details. It reconstructs missing building parts or damaged regions with context-aware synthesis, preserving structural alignment and geometric consistency."
  },
  {
    "Model Unique Name": "Inpainting-CTSDG-Places2",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Places2",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "Github": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Query1": "The corner of the table was cut off in the interior photo of the cafe. Please restore the table structure and lighting texture to be natural.\n",
    "Summary_update": "CTSDG - Places2 is a general-purpose inpainting model trained on the large-scale Places2 dataset. It restores various types of natural and man-made scenes by learning contextual semantics and textures, making it suitable for completing missing regions in diverse environments."
  },
  {
    "Model Unique Name": "Inpainting-MISF-CelebA",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "CelebA",
    "Paper": "https://arxiv.org/pdf/2203.06304",
    "Github": "https://github.com/tsingqguo/misf",
    "HuggingFace": null,
    "Query1": "This profile picture shows some blurry or flawed areas (such as around the eyes and mouth). Please restore the natural facial structure and skin texture with precision.\n",
    "Summary_update": "MISF - CelebA is an image inpainting model trained on facial images from the CelebA dataset. It uses a multi-scale information fusion (MISF) network to reconstruct missing facial regions by progressively integrating contextual and structural cues across spatial scales, resulting in realistic and identity-preserving face completions."
  },
  {
    "Model Unique Name": "Inpainting-MISF-Places2",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Places2",
    "Paper": "https://arxiv.org/pdf/2203.06304",
    "Github": "https://github.com/tsingqguo/misf",
    "HuggingFace": null,
    "Query1": "In this restaurant interior photo, some tables are cut off and some walls are damaged. Please fill in the space structure and lighting texture naturally.\n",
    "Summary_update": "MISF - Places2 is a scene-aware inpainting model trained on the Places2 dataset, designed to complete diverse natural and urban images. It leverages a multi-scale fusion strategy to capture global structure and fine details simultaneously, allowing for semantically plausible and visually coherent image restoration."
  },
  {
    "Model Unique Name": "Inpainting-ResShift-Face",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Query1": "This profile picture is a bit blurry and the eyes look puffy. Please restore it to clarity while maintaining the natural facial structure and pose.\n",
    "Summary_update": "ResShift - Face is a face inpainting model based on the ResShift framework, optimized for reconstructing occluded or masked facial regions with high fidelity. It uses a shift-based diffusion mechanism to refine facial textures and symmetry without relying on explicit segmentation, enabling natural identity-consistent restoration."
  },
  {
    "Model Unique Name": "Inpainting-ResShift-ImageNet",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Query1": "Please remove the blur from this mountain landscape photo and restore the tree and sky details to vivid clarity.\n",
    "Summary_update": "ResShift - ImageNet is a general-purpose inpainting model from the ResShift family, trained on ImageNet-style images to perform class-agnostic image completion. It applies a residual shifting mechanism within a diffusion-based pipeline to generate visually realistic and semantically aligned content in missing or corrupted regions."
  },
  {
    "Model Unique Name": "ImgTxt2Img-HuggingFace-alaa-lab-InstructCV",
    "Category": "ImgTxt2Img",
    "Detailed Category": "ImgTxt2Img",
    "Dataset": "NYUV2,MS-COCO,ADE20k,Oxford-IIIT,SUNRGBD,Pascal VOC2012",
    "Paper": "https://arxiv.org/pdf/2310.00390",
    "Github": "https://github.com/AlaaLab/InstructCV",
    "HuggingFace": "https://huggingface.co/alaa-lab/InstructCV",
    "Query1": "Please detect and display products such as donuts, cakes, and breads in the image of a bakery display case.\n",
    "Summary_update": "Instruct CV is a multi-task image editing and understanding model that performs instruction-guided image-to-image transformations. Trained on datasets like NYUv2, ADE20K, and COCO, it can handle diverse tasks such as depth estimation, segmentation, and image manipulation based on natural language prompts, bridging vision and language for interpretable visual outputs."
  },
  {
    "Model Unique Name": "ImgTxt2Img-HuggingFace-timbrooks-instruct-pix2pix",
    "Category": "ImgTxt2Img",
    "Detailed Category": "ImgTxt2Img",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2211.09800",
    "Github": "https://github.com/timothybrooks/instruct-pix2pix",
    "HuggingFace": "https://huggingface.co/timbrooks/instruct-pix2pix",
    "Query1": "Please change the background in the video conference screenshot to something like a library.\n",
    "Summary_update": "Instruct Pix2Pix is a diffusion-based image editing model that performs text-driven transformations on images using a single forward pass. It fine-tunes Stable Diffusion to respond to instructions like \"make it snowy\" or \"add graffiti,\" enabling localized, prompt-aligned edits without requiring paired training data."
  },
  {
    "Model Unique Name": "HDR-DeepHDRR",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://people.engr.tamu.edu/nimak/Papers/SIGGRAPH2020_HDR/files/SIGGRAPH2020Final.pdf",
    "Github": "https://github.com/marcelsan/Deep-HdrReconstruction",
    "HuggingFace": null,
    "Query1": "The outdoor event photos are too blurry. Please change them to HDR to give them a sunlight feel.\n",
    "Summary_update": "DeepHDRR is an image-to-image restoration model that generates high dynamic range (HDR) images from single low dynamic range (LDR) inputs. It uses a dual-branch CNN architecture with domain-specific priors to recover both saturated highlights and dark details, making it suitable for real-world HDR photography from a single exposure."
  },
  {
    "Model Unique Name": "HDR-FHDR-I1",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1912.11463",
    "Github": "https://github.com/mukulkhanna/FHDR",
    "HuggingFace": null,
    "Query1": "The night photo with lighting has low contrast. Please retouch it with emphasized light and shade.\n",
    "Summary_update": "FHDR I1 is a single-image HDR reconstruction model that restores high dynamic range content from LDR inputs using a two-branch fully convolutional network. One branch predicts the base tone while the other estimates residual high-frequency components, enabling more detailed and perceptually accurate HDR synthesis."
  },
  {
    "Model Unique Name": "HDR-FHDR-I2",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1912.11463",
    "Github": "https://github.com/mukulkhanna/FHDR",
    "HuggingFace": null,
    "Query1": "Outdoor photos with lighting have blown out highlights. Please restore brightness information.\n",
    "Summary_update": "FHDR I2 is an extension of the FHDR framework that incorporates perceptual loss and additional structural cues to improve HDR restoration. It further refines tone mapping and texture recovery from LDR inputs, enhancing visual consistency and luminance realism in reconstructed HDR outputs."
  },
  {
    "Model Unique Name": "FaceReplacement-ResShift",
    "Category": "Img2Img",
    "Detailed Category": "Face Replacement",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Query1": "My face is blurry in my travel photos. Please restore it so that my skin tone and contours are naturally clear.\n",
    "Summary_update": "ResShift FaceReplacement is a face-swapping model that replaces faces in images while preserving pose, lighting, and skin tone consistency. Built on a residual-shifting diffusion framework, it enables high-quality face replacement without requiring explicit segmentation, making it suitable for identity editing in a variety of visual contexts."
  },
  {
    "Model Unique Name": "Enhancement-low-light-img-enhancer",
    "Category": "Img2Img",
    "Detailed Category": "Low Light Enhancement",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/dblasko/low-light-event-img-enhancer",
    "HuggingFace": "https://huggingface.co/dblasko/mirnet-low-light-img-enhancement",
    "Query1": "This concert hall photo is dark and noisy. Please correct it to make it clearer by improving the lighting and colors.\n",
    "Summary_update": "Low Light Enhancement is an image enhancement model based on MIRNet, designed to improve the visibility and clarity of images captured in poorly lit environments. It enhances brightness, contrast, and fine details while suppressing noise, making it useful for event photography, surveillance footage, and nighttime mobile images."
  },
  {
    "Model Unique Name": "Harmonization-INR-RAW-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Query1": "I composited my friend's face onto a beach background, but the face tone doesn't match the background lighting of the photo. Please adjust the overall color temperature to make it harmonious.\n",
    "Summary_update": "INR - HAdobe5K is an image harmonization model trained on the HAdobe5K dataset to adjust foreground elements so they blend naturally with complex photographic backgrounds. It uses an implicit neural representation to align lighting, color, and tone between composited regions and their surrounding context."
  },
  {
    "Model Unique Name": "Harmonization-INR-RAW-iHarmony4",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "iHarmony4",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Query1": "I photoshopped the newly released hoodie onto a model photo, but the brightness and contrast don't match the face. Please make the lighting and tone match naturally.\n",
    "Summary_update": "INR - iHarmony4 is a harmonization model that uses an INR-based architecture to refine foreground compositing on scenes from the iHarmony4 dataset. It adjusts illumination and color balance between pasted objects and background environments to produce seamless, realistic image edits."
  },
  {
    "Model Unique Name": "Harmonization-INR-Res256-iHarmony4",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "iHarmony4",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Query1": "I quickly put together this prototype ad image. Could you please make a color corrected version as well?\n",
    "Summary_update": "INR - Res256 iHarmony4 is a resolution-specific variant of the INR harmonization model, optimized for inputs with a resolution of 256 pixels. Trained on iHarmony4, it performs fast and accurate color and tone matching between foreground and background elements in small to mid-size composite images."
  },
  {
    "Model Unique Name": "Harmonization-INR-Res1024-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Query1": "There is a small flower pot with a composite light in this photo. Please adjust the light so that it blends naturally with the background.\n",
    "Summary_update": "INR - Res1024 HAdobe5K is a high-resolution version of the INR harmonization model designed for 1024px inputs. It adapts foregrounds to high-detail photographic backgrounds from the HAdobe5K dataset, ensuring stylistic and tonal consistency across large-scale compositions."
  },
  {
    "Model Unique Name": "Harmonization-INR-Res2048-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "Github": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Query1": "This is an A4 image for flyers. Please adjust the high resolution so that the character's tone blends smoothly with the background. I hope the colors won't pop out when printed.\n",
    "Summary_update": "INR - Res2048 HAdobe5K is an ultra-high-resolution image harmonization model using implicit neural representation techniques. Trained on HAdobe5K, it fine-tunes color, lighting, and shadow interactions between foregrounds and 2048px-resolution backgrounds for photorealistic image blending."
  },
  {
    "Model Unique Name": "NST-fast-neural-style-candy",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Query1": "Transform your new product cup photos into bright, vivid, candy-like photos with high saturation.\n",
    "Summary_update": "Fast NST – Candy is an image-to-image neural style transfer model that applies a vibrant, abstract painting style inspired by modern art. It transforms ordinary photos into stylized images with bold color palettes and dynamic brushstroke patterns, emphasizing expressive and vivid aesthetics."
  },
  {
    "Model Unique Name": "NST-fast-neural-style-mosaic",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Query1": "Add mosaic texture to your handmade craft photos to make them look like works of art.",
    "Summary_update": "Fast NST – Mosaic stylizes input images with a cubist, tile-like effect reminiscent of classical mosaic artwork. It creates geometric and fragmented compositions with sharp contrast and structured color regions, giving images a historical or architectural artistic appearance."
  },
  {
    "Model Unique Name": "NST-fast-neural-style-rain-princess",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Query1": "Please retouch the wedding photos with a blurred feeling.\n",
    "Summary_update": "Fast NST – Rain Princess offers an instant artistic transform, applying shimmering brushstrokes and dreamy hues of the “Rain Princess” style in a single forward pass—perfect for live style filters and creative content tools."
  },
  {
    "Model Unique Name": "NST-fast-neural-style-udnie",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "Github": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Query1": "Convert your portrait into an abstract art style for social media.\n",
    "Summary_update": "Fast NST – Udnie is a style transfer model that recreates the fluid, surreal aesthetics of Francis Picabia's \"Udnie\" painting. It introduces bold movement, swirling forms, and abstract distortions, ideal for producing artistic outputs with an energetic and avant-garde look."
  },
  {
    "Model Unique Name": "PoseEstimation-OpenPose",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Pose Estimation",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1812.08008",
    "Github": "https://github.com/CMU-Perceptual-Computing-Lab/openpose",
    "HuggingFace": null,
    "Query1": "In this employee training video, please recognize all employees' arm raising movements and record them for each position.\n",
    "Summary_update": "OpenPose is a real-time multi-person 2D pose estimation model that detects body parts and maps skeletal keypoints from RGB images. It uses Part Affinity Fields (PAFs) to associate limbs and joints across individuals, enabling accurate full-body pose tracking in crowded scenes or dynamic environments."
  },
  {
    "Model Unique Name": "SISR-ResShift-BICSR-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Query1": "There is light bleed in the interior shots of the house. Please increase the resolution while preserving the structure and furniture details.\n",
    "Summary_update": "ResShift - BicSR is a diffusion-based super-resolution model trained on bicubic-downsampled inputs for 4× upscaling. It introduces a residual shifting mechanism to enhance pixel-wise sharpness and structure, enabling detailed and photorealistic reconstructions with efficient sampling."
  },
  {
    "Model Unique Name": "SISR-ResShift-RealSR-v1-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Query1": "Enlarge family photos taken in a dark house by 4x to make facial expressions and clothing textures clearer.\n",
    "Summary_update": "ResShift v1 4× is the first real-world version of the ResShift super-resolution model, trained on real-world degradation patterns instead of synthetic bicubic inputs. It applies residual shifting to produce sharp, artifact-free 4× upscaled images from naturally low-resolution inputs, such as mobile or compressed photos."
  },
  {
    "Model Unique Name": "SISR-ResShift-RealSR-v2-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Query1": "Please enlarge the photo of the exhibited work by 4 times and correct it so that the material texture and color appear closer to reality.\n",
    "Summary_update": "ResShift v2 4× builds upon the original ResShift RealSR framework with improved training data and refined noise modeling. It enhances natural image restoration by maintaining edge sharpness and texture consistency while mitigating over-smoothing and hallucination during 4× upscaling."
  },
  {
    "Model Unique Name": "SISR-ResShift-RealSR-v3-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "Github": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Query1": "The outdoor food stall photo is blurry. Please enlarge it 4x so that the lighting and cooking scenes are clear.\n",
    "Summary_update": "ResShift v3 4× is the third generation of the RealSR-trained ResShift series, optimized for high-fidelity 4× image super-resolution under diverse real-world conditions. It further stabilizes fine-grained detail recovery and suppresses degradation-induced artifacts using residual diffusion sampling techniques."
  }
]