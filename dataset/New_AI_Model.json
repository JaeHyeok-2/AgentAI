[
  {
    "Model":null,
    "Model Unique Name":"SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation",
    "Category":"GNN",
    "Detailed Category":null,
    "Dataset":"Amazon-Book, Gowalla, MovieLens, Yelp",
    "Paper":"https:\/\/arxiv.org\/pdf\/2405.20878",
    "GitHub":"https:\/\/github.com\/HKUDS\/SelfGNN",
    "HuggingFace":null,
    "Query1":"I never told the shopping app what I like, but it shows me exactly what I’d pick. How does it get it so right?",
    "Query2":"I watched just a few videos on YouTube, and suddenly it’s recommending ones I actually love. How does it know?",
    "Query3":"My music app keeps picking songs I love, just based on what I listen to. How does it know my taste?"
  },
  {
    "Model":null,
    "Model Unique Name":"AnimateDiff-Lightning: Cross-Model Diffusion Distillation",
    "Category":"Text-to-Video",
    "Detailed Category":null,
    "Dataset":"WebVid-10M, Pandas-70M",
    "Paper":"https:\/\/arxiv.org\/pdf\/2403.12706",
    "GitHub":null,
    "HuggingFace":"https:\/\/huggingface.co\/ByteDance\/AnimateDiff-Lightning",
    "Query1":"I uploaded just one photo of my dog and it turned into a video of him running—how can it bring a still image to life like that?",
    "Query2":"I typed “a panda dancing in the snow” and it made a super smooth video in seconds—how can it create motion just from text?",
    "Query3":"I gave it a sketch of my character, and it started moving like in a cartoon—how does it animate static drawings like that?"
  },
  {
    "Model":null,
    "Model Unique Name":"AnimateLCM: Computation-Efficient Personalized Style Video Generation without Personalized Video Data",
    "Category":"Text-to-Video",
    "Detailed Category":null,
    "Dataset":"UCF101",
    "Paper":"https:\/\/arxiv.org\/pdf\/2402.00769",
    "GitHub":"https:\/\/github.com\/G-U-N\/AnimateLCM",
    "HuggingFace":"https:\/\/huggingface.co\/wangfuyun\/AnimateLCM",
    "Query1":"I uploaded just a selfie and it made a video of me dancing in my own style—how can it do that without ever seeing my videos?",
    "Query2":"I gave it a cartoon drawing and it created a video in that exact style—it’s like it learned the vibe instantly. How is that even possible?",
    "Query3":"I just described a scene and it made a video that looked like it was in my favorite art style. How can it match styles without needing custom data?"
  },
  {
    "Model":null,
    "Model Unique Name":"jina-embeddings-v3: Multilingual Embeddings With Task LoRA",
    "Category":"Feature Extraction",
    "Detailed Category":null,
    "Dataset":"MTEB",
    "Paper":"https:\/\/arxiv.org\/pdf\/2409.10173",
    "GitHub":null,
    "HuggingFace":"https:\/\/huggingface.co\/jinaai\/jina-embeddings-v3",
    "Query1":"I searched something in Korean, and it gave me perfect results from English websites. How does it understand across languages like that?",
    "Query2":"I wrote a short sentence, and it matched documents that said the same thing in totally different words. How can it tell the meaning is the same?",
    "Query3":"I gave it product reviews in five different languages, and it grouped them by meaning. How does it recognize similar content no matter the language?"
  },
  {
    "Model":null,
    "Model Unique Name":"JINA CLIP: Your CLIP Model Is Also Your Text Retriever",
    "Category":"Feature Extraction",
    "Detailed Category":null,
    "Dataset":"LAION, MTEB",
    "Paper":"https:\/\/arxiv.org\/pdf\/2405.20204",
    "GitHub":null,
    "HuggingFace":null,
    "Query1":"I dropped in an image of a sunset, and it instantly found poems and captions that perfectly matched the mood. How does it connect images and text like that?",
    "Query2":"I searched using a picture, and it brought back text descriptions that really understood what was in the photo. How can it understand both text and images?",
    "Query3":"I typed a weird sentence, and it showed me a bunch of pictures that actually matched the vibe. How does it know what kind of image fits that kind of text?"
  },
  {
    "Model":null,
    "Model Unique Name":"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "Category":"Fill-Mask",
    "Detailed Category":null,
    "Dataset":"GLUE, BEIR, MLDR, CodeSearchNet, StackQA",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.13663",
    "GitHub":"https:\/\/github.com\/AnswerDotAI\/ModernBERT",
    "HuggingFace":"https:\/\/huggingface.co\/answerdotai\/ModernBERT-large",
    "Query1":"I started typing a sentence, and it filled in the rest in such a natural way—even with a super long document. How does it handle that so smoothly?",
    "Query2":"I searched for something buried deep in a giant report, and it instantly pulled the exact part I needed. How can it scan and understand such long texts so fast?",
    "Query3":"I gave it technical code mixed with natural language, and it figured out what I meant right away. How does it handle both coding and regular language together?"
  },
  {
    "Model":null,
    "Model Unique Name":"M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
    "Category":"Sentence Similarity",
    "Detailed Category":null,
    "Dataset":"MTEB, MIRACL, MKQA, BEIR, C-MTEB, MLDR",
    "Paper":"https:\/\/arxiv.org\/pdf\/2402.03216",
    "GitHub":"https:\/\/github.com\/FlagOpen\/FlagEmbedding",
    "HuggingFace":"https:\/\/huggingface.co\/BAAI\/bge-m3",
    "Query1":"I typed the same question in English and Korean, and it gave me the same search results. How does it understand meaning across languages so well?",
    "Query2":"I searched using just a short sentence, and it found super detailed answers. How can it understand what I really mean with so little input?",
    "Query3":"I gave it a product title, a review, and a question, and it figured out they were all talking about the same thing. How does it link such different types of text?"
  },
  {
    "Model":null,
    "Model Unique Name":"DEPTH PRO : Sharp Monocular Metric Depth In Less Than a Second",
    "Category":"Depth Estimation",
    "Detailed Category":null,
    "Dataset":"AM-2k, DIS-5k",
    "Paper":"https:\/\/arxiv.org\/pdf\/2410.02073",
    "GitHub":"https:\/\/github.com\/apple\/ml-depth-pro",
    "HuggingFace":"https:\/\/huggingface.co\/apple\/DepthPro",
    "Query1":"I took a quick photo with my phone, and it instantly mapped out the depth of everything in the room—how does it know how far things are from just one image?",
    "Query2":"I used it on a street photo, and it could tell how far the cars and people were—without any special sensors. How is that even possible?",
    "Query3":"I added depth to my portrait photo and it made the background blur look so real. How can it create that effect so accurately from a flat image?"
  },
  {
    "Model":null,
    "Model Unique Name":"Depth-Anything-V2",
    "Category":"Depth Estimation",
    "Detailed Category":null,
    "Dataset":"BlendedMVS, Hypersim, IRS, TartanAir, KITTI",
    "Paper":"https:\/\/arxiv.org\/pdf\/2406.09414",
    "GitHub":"https:\/\/github.com\/DepthAnything\/Depth-Anything-V2",
    "HuggingFace":"https:\/\/huggingface.co\/depth-anything\/Depth-Anything-V2-Large",
    "Query1":"Snapped a photo out the window, and it instantly showed how far away every building and tree was. How can it measure depth from just a picture?",
    "Query2":"Ran it on some drone footage, and it mapped out the entire terrain’s shape in seconds. How does it turn 2D video into 3D structure like that?",
    "Query3":"Dropped in a street scene, and it nailed the depth even in tricky lighting. How can it be so accurate with just one image and no special sensors?"
  },
  {
    "Model":null,
    "Model Unique Name":"FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION",
    "Category":"Text-to-Image",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2406.06858",
    "GitHub":"https:\/\/github.com\/black-forest-labs\/flux",
    "HuggingFace":"https:\/\/huggingface.co\/black-forest-labs\/FLUX.1-dev",
    "Query1":"Started generating high-res images from prompts, and it was done way faster than usual—even on my GPU. How is it speeding that up so much without new hardware?",
    "Query2":"Ran a batch of image generations in parallel, and it barely slowed down. How can it handle multiple generations so efficiently?",
    "Query3":"Tried it on a shared GPU server and got smooth performance without lag. What’s going on under the hood to make it so fast and stable?"
  },
  {
    "Model":null,
    "Model Unique Name":"In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
    "Category":"Image-to-Image",
    "Detailed Category":null,
    "Dataset":"Emu Edit, MagicBrush",
    "Paper":"https:\/\/arxiv.org\/abs\/2504.20690",
    "GitHub":"https:\/\/github.com\/River-Zhang\/ICEdit?tab=readme-ov-file",
    "HuggingFace":"https:\/\/huggingface.co\/RiverZ\/normal-lora",
    "Query1":"I showed it an example of what I wanted, like turning day into night, and it applied the same edit to my photo perfectly. How does it learn edits just from examples?",
    "Query2":"I dragged in a sketch and a style image, and it updated my picture with exactly that vibe. How can it follow visual instructions like that?",
    "Query3":"I used a reference photo to guide the change, and it edited my image without me typing anything. What kind of model understands edits that way?"
  },
  {
    "Model":null,
    "Model Unique Name":"Wan: Open and Advanced Large-Scale Video Generative Models",
    "Category":"Image-to-Video",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/abs\/2503.20314",
    "GitHub":"https:\/\/github.com\/Wan-Video\/Wan2.1",
    "HuggingFace":"https:\/\/huggingface.co\/Wan-AI",
    "Query1":"I described a scene—like “a tiger running through snow”—and it turned it into a full video that looked like a movie. How does it generate all that just from words?",
    "Query2":"I adjusted the prompt slightly, and the whole video changed with the right mood and motion. How can it be that sensitive to small edits in text?",
    "Query3":"I combined a setting, a character, and an action, and it stitched together a full story in video. What kind of model can turn ideas into such smooth animation?"
  },
  {
    "Model":null,
    "Model Unique Name":"SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation",
    "Category":"Image-to-Video",
    "Detailed Category":null,
    "Dataset":"ObjaverseDSy, Consistent4D, DAVIS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.16396",
    "GitHub":"https:\/\/github.com\/Stability-AI\/generative-models",
    "HuggingFace":"https:\/\/huggingface.co\/stabilityai\/stable-video-diffusion-img2vid-xt",
    "Query1":"I dropped in a single image, and it turned into a smooth, rotating video of the whole object—how does it figure out what it looks like from all sides?",
    "Query2":"I used different angles of the same scene, and it made one seamless video with perfect motion and lighting. How can it keep everything so consistent?",
    "Query3":"I tried generating a 360° view video from a single frame, and it looked stable the whole time. What kind of tech makes that level of smooth 4D output possible?"
  },
  {
    "Model":null,
    "Model Unique Name":"HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters",
    "Category":"Image-to-Video",
    "Detailed Category":null,
    "Dataset":"LatentSync-filtered AV data, CelebV-HQ, HDTF, Self-Collected Full-Body Wild",
    "Paper":"https:\/\/arxiv.org\/pdf\/2505.20156",
    "GitHub":"https:\/\/github.com\/Tencent-Hunyuan\/HunyuanVideo-Avatar",
    "HuggingFace":"https:\/\/huggingface.co\/tencent\/HunyuanVideo-Avatar",
    "Query1":"I uploaded an audio clip, and it created a video of a person talking with perfect lip sync and gestures. How can it animate a human so accurately just from sound?",
    "Query2":"I gave it voices for two characters, and it made both of them move and interact like in a real conversation. How does it handle multiple people that well?",
    "Query3":"I used a full-body photo and it turned it into a video avatar that spoke and moved just like a real person. What kind of model can animate full bodies so realistically?"
  },
  {
    "Model":null,
    "Model Unique Name":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
    "Category":"Zero-Shot Image Classification",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.00915",
    "GitHub":"https:\/\/huggingface.co\/microsoft\/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
    "HuggingFace":"https:\/\/huggingface.co\/microsoft\/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
    "Query1":"I uploaded a microscope image, and it instantly told me what kind of cells they were—without any extra training. How does it know that right away?",
    "Query2":"I tested it on diagrams from old research papers, and it correctly matched them with the right medical terms. How can it connect visuals and scientific language like that?",
    "Query3":"I ran it on X-ray and pathology scans, and it grouped them by condition without labels. What kind of model can sort medical images with no supervision?"
  },
  {
    "Model":null,
    "Model Unique Name":"Scaling Open-Vocabulary Object Detection",
    "Category":"Zero-Shot Object Detection",
    "Detailed Category":null,
    "Dataset":"WebLI, LVIS, ODinW13",
    "Paper":"https:\/\/arxiv.org\/pdf\/2306.09683",
    "GitHub":"https:\/\/github.com\/inuwamobarak\/OWLv2",
    "HuggingFace":"https:\/\/huggingface.co\/google\/owlv2-large-patch14-ensemble",
    "Query1":"I pointed it at a photo and asked for “a teal backpack,” and it found it—even though that label wasn’t trained in. How does it detect things it’s never seen before?",
    "Query2":"I tried it on a messy room photo and told it to find “wireless earbuds,” and it picked them out perfectly. How can it understand open-ended text like that?",
    "Query3":"I uploaded random street scenes and asked for things like “folding bike” or “yellow umbrella,” and it spotted them instantly. What kind of model can handle that?"
  },
  {
    "Model":null,
    "Model Unique Name":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
    "Category":"Zero-Shot Object Detection",
    "Detailed Category":null,
    "Dataset":"OpenImages v5 subset, Localized Narratives (OpenImages), LVIS subset, RefCOCO, RefCOCO+, RefCOCOg, gRefCOCO",
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.05499",
    "GitHub":"https:\/\/github.com\/IDEA-Research\/GroundingDINO?tab=readme-ov-file",
    "HuggingFace":"https:\/\/huggingface.co\/IDEA-Research\/grounding-dino-tiny",
    "Query1":"I highlighted a street scene and asked it to find “a kid holding a red balloon,” and it nailed it right away. How does it pick out specific objects just from text?",
    "Query2":"I uploaded a crowded image and asked for “the person in a blue hoodie near the car,” and it knew exactly who I meant. How can it understand such detailed descriptions?",
    "Query3":"I gave it natural sentences like “a cat sitting on the edge of the couch,” and it still found it without needing any labels. What kind of model can do that?"
  },
  {
    "Model":null,
    "Model Unique Name":"Structured 3D Latents for Scalable and Versatile 3D Generation∗",
    "Category":"Text-to-3D",
    "Detailed Category":null,
    "Dataset":"ObjaverseXL (sketchfab), ObjaverseXL (github), ABO, 3D-FUTURE, HSSD",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.01506",
    "GitHub":"https:\/\/github.com\/Microsoft\/TRELLIS",
    "HuggingFace":"https:\/\/huggingface.co\/microsoft\/TRELLIS-text-xlarge",
    "Query1":"Describing something as simple as “a futuristic coffee machine” was enough—it built a fully rotatable 3D model with amazing detail. How does it turn plain text into objects like that?",
    "Query2":"Even vague ideas like “an elegant chair made of wood and metal” were interpreted perfectly, down to the curve of the legs. How can it understand design features that well from just words?",
    "Query3":"Without needing sketches or blueprints, it generated 3D shapes for all my product ideas—ready for use in mockups. What kind of model makes that level of creation so accessible?"
  },
  {
    "Model":null,
    "Model Unique Name":"TripoSR: Fast 3D Object Reconstruction from a Single Image",
    "Category":"Image-to-3D",
    "Detailed Category":null,
    "Dataset":"Objaverse",
    "Paper":"https:\/\/arxiv.org\/pdf\/2403.02151",
    "GitHub":"https:\/\/github.com\/VAST-AI-Research\/TripoSR",
    "HuggingFace":"https:\/\/huggingface.co\/stabilityai\/stable-fast-3d",
    "Query1":"Snapping just one photo was all it needed—it instantly turned a 2D image into a full 3D model I could spin around. How does it rebuild depth from a single view?",
    "Query2":"With no special equipment or setup, it recreated the shape of an object from a random picture I found online. What kind of tech makes that possible?",
    "Query3":"Even complex shapes like chairs and bikes came out clean and detailed from a single image. How can it guess the hidden parts so accurately?"
  },
  {
    "Model":null,
    "Model Unique Name":"Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
    "Category":"Image-to-3D",
    "Detailed Category":null,
    "Dataset":"Objaverse, Objaverse-XL",
    "Paper":"https:\/\/arxiv.org\/abs\/2501.12202",
    "GitHub":"https:\/\/github.com\/Tencent-Hunyuan\/Hunyuan3D-2",
    "HuggingFace":"https:\/\/huggingface.co\/tencent\/Hunyuan3D-2",
    "Query1":"Typing out something like “a vintage camera made of brass and leather” gave me a fully textured 3D model in seconds. How can it generate such realistic materials from text alone?",
    "Query2":"I tested it with detailed prompts, and it produced high-res 3D objects ready for games or AR. What kind of model handles both shape and texture that well?",
    "Query3":"Even when I fed it imaginative concepts like “a crystal spaceship with glowing engines,” the model delivered stunning 3D assets. How does it turn creative text into production-ready visuals?"
  },
  {
    "Model":null,
    "Model Unique Name":"VGGT: Visual Geometry Grounded Transformer",
    "Category":"Image-to-3D",
    "Detailed Category":null,
    "Dataset":"Co3Dv2, BlendMVS, DL3DV, MegaDepth, Kubric, WildRGB, ScanNet, HyperSim, Mapillary Metropolis, Habitat, Replica, MVS-Synth, PointOdyssey, Virtual KITTI, Aria Synthetic Environments, Aria Digital Twin, Objaverse",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.11651",
    "GitHub":"https:\/\/github.com\/facebookresearch\/vggt",
    "HuggingFace":"https:\/\/huggingface.co\/facebook\/VGGT-1B",
    "Query1":"Captured just one image of a toy figure, and it reconstructed a full 3D version I could rotate and zoom in on. How does it understand geometry so well from one photo?",
    "Query2":"Even outdoor scenes with tricky lighting or complex shapes turned into accurate 3D structures. What lets this model handle real-world visuals so robustly?",
    "Query3":"All I had was a single RGB image—no depth, no labels—and still, it produced realistic 3D results. What kind of model can pull that off?"
  },
  {
    "Model":null,
    "Model Unique Name":"BGE: One-Stop Retrieval Toolkit For Search and RAG",
    "Category":"Text Classification",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.14475",
    "GitHub":"https:\/\/github.com\/FlagOpen\/FlagEmbedding\/tree\/master?tab=readme-ov-file",
    "HuggingFace":"https:\/\/huggingface.co\/BAAI\/bge-reranker-v2-m3",
    "Query1":"Threw in a search query like “ways to reduce memory usage in Python,” and it pulled up exactly what I needed—even from long, messy docs. How does it rank results so precisely?",
    "Query2":"Even when the wording was vague, it still understood what I meant and found spot-on answers. What helps it go beyond exact keyword matches?",
    "Query3":"Tested it with complex questions across multiple topics, and it reorganized the best answers in the right order. What kind of reranker model can do that?"
  },
  {
    "Model":null,
    "Model Unique Name":"GLiNER: Generalist and Lightweight Model for Named Entity Recognition",
    "Category":"Token Classification",
    "Detailed Category":null,
    "Dataset":"Pile‑NER",
    "Paper":"https:\/\/arxiv.org\/pdf\/2311.08526",
    "GitHub":"https:\/\/github.com\/urchade\/GLiNER",
    "HuggingFace":"https:\/\/huggingface.co\/urchade\/gliner_multi_pii-v1",
    "Query1":"Pasted in a document full of names, places, and emails, and it automatically picked out every entity—no training needed. How can it recognize all that so quickly?",
    "Query2":"Even when the format was messy, it still found all the key terms without missing a beat. What helps it stay so accurate across different writing styles?",
    "Query3":"Tried it on different topics—from medical notes to social media posts—and it adapted instantly. How does a single model handle so many domains?"
  },
  {
    "Model":null,
    "Model Unique Name":"FullStop: Multilingual Deep Models for Punctuation Prediction",
    "Category":"Token Classification",
    "Detailed Category":null,
    "Dataset":"Europarl, SoNaR",
    "Paper":"https:\/\/ceur-ws.org\/Vol-2957\/sepp_paper4.pdf",
    "GitHub":"https:\/\/github.com\/oliverguhr\/fullstop-deep-punctuation-prediction",
    "HuggingFace":"https:\/\/huggingface.co\/oliverguhr\/fullstop-punctuation-multilang-large",
    "Query1":"Typed a long sentence without any punctuation, and it cleaned it up instantly—commas, periods, even question marks were spot on. How does it know where everything should go?",
    "Query2":"Even in other languages like German and Korean, it fixed the punctuation perfectly. What kind of model can handle multilingual grammar that well?",
    "Query3":"Dropped in a transcript from a meeting, and suddenly it became readable like a polished article. How does it turn messy speech into clean written text?"
  },
  {
    "Model":null,
    "Model Unique Name":"Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
    "Category":"Feature Extraction",
    "Detailed Category":null,
    "Dataset":"MTEB, synthetic text relevance dataset (~150M pairs), high-quality supervised dataset (~7M pairs)",
    "Paper":"https:\/\/arxiv.org\/pdf\/2506.05176",
    "GitHub":"https:\/\/github.com\/QwenLM\/Qwen3-Embedding",
    "HuggingFace":"https:\/\/huggingface.co\/Qwen\/Qwen3-Embedding-0.6B",
    "Query1":"Just by entering a short search phrase, it brought up highly relevant results from huge text collections. How does it capture meaning so effectively in embeddings?",
    "Query2":"Even vague or indirect queries were understood, and the most relevant answers came first. What enables it to rerank results so intelligently?",
    "Query3":"Tested it across topics—from finance to medicine—and it handled them all smoothly. How can a single model adapt across such diverse domains?"
  },
  {
    "Model":null,
    "Model Unique Name":"DeepSeek-R1",
    "Category":null,
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.12948",
    "GitHub":"https:\/\/github.com\/deepseek-ai\/DeepSeek-R1",
    "HuggingFace":"https:\/\/huggingface.co\/deepseek-ai\/DeepSeek-R1-0528",
    "Query1":"Started chatting with it about a complex topic, and it responded with clear, accurate reasoning—almost like talking to an expert. How can a language model think that deeply?",
    "Query2":"Even when I switched languages mid-sentence or jumped between topics, it kept up with no problem. What kind of training makes that level of fluency possible?",
    "Query3":"Gave it messy notes and half-finished thoughts, and it turned them into clean summaries and full paragraphs. How does it organize ideas so well from scattered input?"
  },
  {
    "Model":null,
    "Model Unique Name":"VGGT: Visual Geometry Grounded Transformer",
    "Category":" 3D Vision",
    "Detailed Category":null,
    "Dataset":"Co3Dv2, Objaverse, NYUv2",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.11651",
    "GitHub":"https:\/\/github.com\/facebookresearch\/vggt",
    "HuggingFace":null,
    "Query1":"Dropped in a single photo of an object, and it built a 3D model I could rotate and view from every angle. How does it figure out the full shape from just one image?",
    "Query2":"Even indoor scenes with clutter and odd lighting turned into clean 3D geometry. What allows it to stay accurate in such messy conditions?",
    "Query3":"Tested it with random photos from my phone, and it still produced detailed 3D outputs. What kind of model handles real-world images so reliably?"
  },
  {
    "Model":null,
    "Model Unique Name":"MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors",
    "Category":" 3D Vision",
    "Detailed Category":null,
    "Dataset":"TUM-RGBD, 7-Scenes, EuRoC, ETH3D",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.12392",
    "GitHub":"https:\/\/github.com\/rmurai0610\/MASt3R-SLAM",
    "HuggingFace":null,
    "Query1":"While moving around with a handheld camera, it built a full 3D map of the room in real time. How can it reconstruct and localize everything so quickly?",
    "Query2":"Even in low light and cluttered spaces, it tracked my motion and built a stable model. What makes it so robust in tough conditions?",
    "Query3":"I ran it on a drone indoors, and it mapped the environment with incredible accuracy—without GPS. How does this system navigate and build 3D in real time?"
  },
  {
    "Model":null,
    "Model Unique Name":"UniK3D: Universal Camera Monocular 3D Estimation",
    "Category":"depth estimation",
    "Detailed Category":null,
    "Dataset":"KITTI, NYUv2, MegaDepth, NianticMapFree, Mapillary",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.16591",
    "GitHub":"https:\/\/github.com\/lpiccinelli-eth\/UniK3D",
    "HuggingFace":null,
    "Query1":"Snapped a photo with my phone, and it instantly figured out how far everything was—whether indoors or outside. How does it estimate depth so accurately from one image?",
    "Query2":"Even with different camera types and no calibration, it still produced solid 3D structure. What makes this model so flexible across devices?",
    "Query3":"Tried it on travel photos and street scenes, and it gave me depth maps that actually felt real. What allows it to generalize so well to everyday images?"
  },
  {
    "Model":null,
    "Model Unique Name":"DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
    "Category":"depth estimation",
    "Detailed Category":null,
    "Dataset":"Sintel, ScanNet, KITTI, Bonn, NYU-v2",
    "Paper":"https:\/\/arxiv.org\/pdf\/2409.02095",
    "GitHub":"https:\/\/github.com\/Tencent\/DepthCrafter",
    "HuggingFace":null,
    "Query1":"Dropped in a regular video clip, and it created a smooth depth map for every frame—like the whole scene came alive in 3D. How does it stay consistent over time like that?",
    "Query2":"Even when people or objects moved quickly, it kept the depth stable and accurate. What helps it handle dynamic scenes so well?",
    "Query3":"Ran it on some outdoor footage, and it still captured the depth consistently—even with changing light and motion. How can a model handle such open-world complexity?"
  },
  {
    "Model":null,
    "Model Unique Name":"Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
    "Category":"depth estimation",
    "Detailed Category":null,
    "Dataset":"KITTI, Bonn, ScanNet, Sinte l, DAVIS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.12375",
    "GitHub":"https:\/\/github.com\/DepthAnything\/Video-Depth-Anything",
    "HuggingFace":null,
    "Query1":"Tried it on a full-length video, and every frame had smooth, consistent depth—even across scene changes. How does it keep things stable over such long sequences?",
    "Query2":"Even with fast camera motion and moving objects, the depth didn’t flicker or break. What makes it so robust for dynamic video?",
    "Query3":"Used it on a handheld video from my phone, and it produced depth that looked like it came from a pro 3D scanner. How can it achieve that quality without special gear?"
  },
  {
    "Model":null,
    "Model Unique Name":"Interpreting Object-level Foundation Models via Visual Precision Search",
    "Category":"explainability and interpretability",
    "Detailed Category":null,
    "Dataset":"MS COCO, RefCOCO, LVIS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2411.16198",
    "GitHub":"https:\/\/github.com\/RuoyuChen10\/VPS",
    "HuggingFace":null,
    "Query1":"I was trying to figure out why my AI app kept tagging the wrong object in a photo, and this tool showed exactly what part of the image it was focusing on. How does it explain what the model is actually seeing?",
    "Query2":"When I asked it to find “a small red cup,” it highlighted the wrong item. Using this, I could finally see what confused the model. How can it help us understand those mistakes?",
    "Query3":"I compared two models on the same image, and this tool made it obvious which one actually understood the object better. How does it visualize model reasoning so clearly?"
  },
  {
    "Model":null,
    "Model Unique Name":"Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders",
    "Category":"explainability and interpretability",
    "Detailed Category":null,
    "Dataset":"GazeFollow, VideoAttentionTarget, ChildPlay, GOO-Real",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.09586",
    "GitHub":"https:\/\/github.com\/fkryan\/gazelle",
    "HuggingFace":null,
    "Query1":"I showed it a photo of someone just looking off to the side, and it could tell exactly what they were focused on. How does it figure out where people are looking so precisely?",
    "Query2":"While watching a video, I always wondered what the person on screen was paying attention to—this tool made it totally clear. How does it visualize someone’s focus like that?",
    "Query3":"Tried it on a clip of a child playing, and it showed exactly what the kid was tracking with their eyes. What kind of model understands gaze in such real-world scenes?"
  },
  {
    "Model":null,
    "Model Unique Name":"MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
    "Category":"generative models",
    "Detailed Category":null,
    "Dataset":"AudioSet, Freesound, VGGSound, AudioCaps, WavCaps",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.15322",
    "GitHub":"https:\/\/github.com\/hkchengrex\/MMAudio",
    "HuggingFace":null,
    "Query1":"I uploaded a video of a waterfall with no sound, and it generated audio that matched perfectly—even the splashing and echo felt real. How does it know what that scene should sound like?",
    "Query2":"While editing a silent drone clip, I used this to add background sound—and the result fit so naturally. How can it generate audio that matches the video content so well?",
    "Query3":"Tried it on different types of videos—cars, animals, city scenes—and it created fitting sounds without any manual work. What kind of model handles that variety so smoothly?"
  },
  {
    "Model":null,
    "Model Unique Name":"SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
    "Category":"generative models",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2403.09055",
    "GitHub":"https:\/\/github.com\/ironjr\/semantic-draw",
    "HuggingFace":null,
    "Query1":"I sketched out a few blobs and labeled them, and it turned that into a full detailed scene in real time. How does it know what I’m trying to draw with just simple shapes?",
    "Query2":"While designing a game map, I used this tool to quickly block out areas, and it filled in realistic textures and objects live. How can it generate full images while I’m still editing?",
    "Query3":"Just by dragging and labeling regions—like “sky,” “grass,” or “building”—I watched a full image come to life on the canvas. What kind of model allows that level of interactive generation?"
  },
  {
    "Model":null,
    "Model Unique Name":"MINIMA: Modality Invariant Image Matching",
    "Category":"image matching",
    "Detailed Category":null,
    "Dataset":"MD-syn (MegaDepth-Syn), LLVIP, M3FD, MSRS, METU-VisTIR, MMIM, DIODE, DSEC (RGB-Event)",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.19412",
    "GitHub":"https:\/\/github.com\/LSXI7\/MINIMA",
    "HuggingFace":null,
    "Query1":"Tried matching an infrared night image with a normal daytime photo, and it still aligned them perfectly. How does it recognize the same place across such different styles?",
    "Query2":"Even when I used blurry thermal images or motion-blurred shots, it still found the right matches. What helps it stay so reliable across tough conditions?",
    "Query3":"I used images from completely different sensors—like event cameras and RGB—and it matched them seamlessly. How does this model work across such different modalities?"
  },
  {
    "Model":null,
    "Model Unique Name":"Layered Image Vectorization via Semantic Simplification",
    "Category":"image vectorization",
    "Detailed Category":null,
    "Dataset":null,
    "Paper":"https:\/\/arxiv.org\/pdf\/2406.05404",
    "GitHub":"https:\/\/github.com\/SZUVIZ\/layered_vectorization",
    "HuggingFace":null,
    "Query1":"I uploaded a messy illustration, and it turned it into clean, layered vector shapes that were super easy to edit. How does it break down an image like that so neatly?",
    "Query2":"When working on a poster design, I used this to simplify a photo into editable parts—each color and region became a separate layer. What kind of model makes that possible?",
    "Query3":"Even with noisy or shaded input, it managed to pull out clean shapes and preserve meaning. How can it simplify images while keeping their structure intact?"
  },
  {
    "Model":null,
    "Model Unique Name":"DEIM: DETR with Improved Matching for Fast Convergence",
    "Category":"object detection",
    "Detailed Category":null,
    "Dataset":"COCO",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.04234",
    "GitHub":"https:\/\/github.com\/ShihuaHuang95\/DEIM",
    "HuggingFace":null,
    "Query1":"I ran object detection on a bunch of images, and this model locked onto targets way faster than others. How does it learn to detect so quickly?",
    "Query2":"Even when I added new objects mid-training, it adapted fast and didn’t fall apart. What kind of matching process helps it stay so stable?",
    "Query3":"Used it on COCO-style scenes with lots of small and overlapping objects, and it still nailed the detections. How can it stay that precise under complex setups?"
  },
  {
    "Model":null,
    "Model Unique Name":"MITracker: Multi-View Integration for Visual Object Tracking",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"MVTrack, GOT-10k, GMTD",
    "Paper":"https:\/\/arxiv.org\/pdf\/2502.20111",
    "GitHub":"https:\/\/github.com\/XuM007\/MITracker",
    "HuggingFace":null,
    "Query1":"I tracked a moving person across camera angles, and it kept following them perfectly—even when they went behind objects. How does it stay locked on across views like that?",
    "Query2":"Tried it in a store with multiple security cams, and the model still recognized the same person as they moved around. What allows it to integrate views that well?",
    "Query3":"Even in low lighting and heavy occlusion, the tracker held on without drifting. What kind of model keeps tracking so reliably under tough conditions?"
  },
  {
    "Model":null,
    "Model Unique Name":"Multiple Object Tracking as ID Prediction",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"DanceTrack, SportsMOT, MOT17",
    "Paper":"https:\/\/arxiv.org\/pdf\/2403.16848",
    "GitHub":"https:\/\/github.com\/MCG-NJU\/MOTIP",
    "HuggingFace":null,
    "Query1":"While watching a crowded sports video, I noticed it could keep track of each player without mixing them up—even when they crossed paths. How does it recognize who’s who so accurately?",
    "Query2":"Tried tracking dancers in a performance video, and it kept their IDs consistent throughout the whole scene. What kind of model can handle that much motion and identity switching?",
    "Query3":"Even when multiple people entered and left the frame, it assigned the right ID every time. How does it manage consistent tracking without re-identifying errors?"
  },
  {
    "Model":null,
    "Model Unique Name":"EdgeTAM: On-Device Track Anything Model",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"DAVIS 2017, MOSE, SA-V (val\/test), YouTube-VOS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.07256",
    "GitHub":"https:\/\/github.com\/facebookresearch\/EdgeTAM",
    "HuggingFace":null,
    "Query1":"I ran this on my phone to track my dog in a video, and it followed him the entire time—no internet, no lag. How does it manage that kind of tracking directly on a device?",
    "Query2":"Even when I tapped on a moving object in a live video, it instantly started tracking without delay. What makes it responsive enough for real-time use?",
    "Query3":"I tried it while recording outdoors, and it still tracked smoothly despite motion and lighting changes. How does it stay so stable without needing the cloud?"
  },
  {
    "Model":null,
    "Model Unique Name":"A Distractor-Aware Memory for Visual Object Tracking with SAM2",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"DiDi, VOT2020, VOT2022, VOTChallenge, LaSOT, GOT-10k",
    "Paper":"https:\/\/arxiv.org\/pdf\/2411.17576",
    "GitHub":"https:\/\/github.com\/jovanavidenovic\/DAM4SAM",
    "HuggingFace":null,
    "Query1":"While tracking a person in a crowd, the model didn’t get distracted by others wearing similar clothes. How does it stay focused on the right target?",
    "Query2":"While tracking a person in a crowd, the model didn’t get distracted by others wearing similar clothes. How does it stay focused on the right target?",
    "Query3":"I tried it in a video where the target disappeared and reappeared, but the model still locked onto the same one. How does this memory system work under occlusion?"
  },
  {
    "Model":null,
    "Model Unique Name":"From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization",
    "Category":"Object Tracking",
    "Detailed Category":null,
    "Dataset":"Market1501, SYSU‑MM01, Occluded‑ReID",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.00938",
    "GitHub":"https:\/\/github.com\/yuanc3\/Pose2ID",
    "HuggingFace":null,
    "Query1":"I checked footage from two different cameras, and it still recognized the same person—even though their pose had totally changed. How does it match identities without training?",
    "Query2":"Even when someone was partially blocked or turning away, the system still re-identified them correctly. What helps it ignore pose and focus on who it is?",
    "Query3":"Tried it across day and night clips, and it kept matching the same person without needing to fine-tune. How can it stay accurate with no retraining?"
  },
  {
    "Model":null,
    "Model Unique Name":"Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models",
    "Category":"open-world detection",
    "Detailed Category":null,
    "Dataset":"Anomaly-Instruct-125k, VisA-D&R, MVTec AD, AITEX, ELPV, BTAD, MPDD, BrainMRI, HeadCT, Br35H",
    "Paper":"https:\/\/arxiv.org\/pdf\/2502.07601",
    "GitHub":"https:\/\/github.com\/honda-research-institute\/Anomaly-OneVision",
    "HuggingFace":null,
    "Query1":"I uploaded a product photo, and it immediately pointed out a defect—even though I never told it what to look for. How does it detect anomalies without any training examples?",
    "Query2":"Even in medical images like MRIs or CT scans, it spotted unusual regions and explained why they looked abnormal. How can a model reason about anomalies so clearly?",
    "Query3":"I tested it on fabrics, electronics, even X-rays, and it still gave useful results with no retraining. What makes this model so flexible across domains?"
  },
  {
    "Model":null,
    "Model Unique Name":"Reconstructing Humans with a Biomechanically Accurate Skeleton",
    "Category":"pose estimation",
    "Detailed Category":null,
    "Dataset":"Human3.6M, MPI-INF-3DHP, COCO, MPII, AI Challenger, AVA, InstaVariety, MOYO",
    "Paper":"https:\/\/arxiv.org\/pdf\/2503.21751",
    "GitHub":"https:\/\/github.com\/IsshikiHugh\/HSMR",
    "HuggingFace":null,
    "Query1":"Recorded a short video of someone walking, and it built a full 3D skeleton that even captured subtle joint movement. How does it reconstruct the body so precisely from just video?",
    "Query2":"Even with loose clothing or side views, it got the posture and limb positions exactly right. What allows it to track body mechanics so reliably?",
    "Query3":"I tried it for fitness analysis, and it showed how each joint was behaving, almost like a digital skeleton. How can this model understand human biomechanics so well?"
  },
  {
    "Model":null,
    "Model Unique Name":"MatAnyone: Stable Video Matting with Consistent Memory Propagation",
    "Category":"segmentation",
    "Detailed Category":null,
    "Dataset":"VM800, YoutubeMatte",
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.14677",
    "GitHub":"https:\/\/github.com\/pq-yang\/MatAnyone",
    "HuggingFace":null,
    "Query1":"I removed the background from a full video, and it stayed clean and stable the whole time—even as the person moved around. How does it keep the edges so consistent?",
    "Query2":"Tried editing a vlog clip outdoors, and even with wind-blown hair and shadows, the subject stayed perfectly separated. What makes it handle those tricky details so well?",
    "Query3":"I used it for background replacement, and the result looked smooth frame to frame—no flickering or jitter. How does this model maintain such temporal consistency?"
  },
  {
    "Model":null,
    "Model Unique Name":"FoundationStereo: Zero-Shot Stereo Matching",
    "Category":"stereo matching",
    "Detailed Category":null,
    "Dataset":"FoundationStereo Dataset (1M synthetic stereo pairs), Scene Flow, Middlebury, ETH3D, KITTI",
    "Paper":"https:\/\/arxiv.org\/pdf\/2501.09898",
    "GitHub":"https:\/\/github.com\/NVlabs\/FoundationStereo",
    "HuggingFace":null,
    "Query1":"I gave it two slightly different photos from a stereo camera, and it created a full depth map—no training or fine-tuning needed. How does it understand depth out of the box like that?",
    "Query2":"Even in unfamiliar scenes it’s never seen before, like cluttered rooms or outdoor roads, it still gave accurate 3D structure. What helps it generalize so well?",
    "Query3":"I tested it with stereo pairs in low light, and it still produced reliable depth. How does this model stay stable in challenging visual conditions?"
  },
  {
    "Model":null,
    "Model Unique Name":"Towards Universal Soccer Video Understanding",
    "Category":"video understanding",
    "Detailed Category":null,
    "Dataset":"SoccerReplay-1988, SoccerNet-v2, SoccerNet",
    "Paper":"https:\/\/arxiv.org\/pdf\/2412.01820",
    "GitHub":"https:\/\/github.com\/jyrao\/UniSoccer",
    "HuggingFace":null,
    "Query1":"I watched a full match replay, and it automatically identified key moments—goals, fouls, even tactical plays. How does it understand what’s happening in a soccer game so well?",
    "Query2":"Even with old or low-quality footage, it still tracked players and actions accurately. What helps the model stay reliable across different video types?",
    "Query3":"I tried it during a live match, and it picked up key events almost in real time. How can it process complex team play and make sense of it so quickly?"
  },
  {
    "Model":null,
    "Model Unique Name":"Magma: A Foundation Model for Multimodal AI Agents",
    "Category":"visual agents",
    "Detailed Category":null,
    "Dataset":"SeeClick, Vision2UI, Ego4D, EpicKitchen, Something‑Something v2, Open‑X‑Embodiment",
    "Paper":"https:\/\/arxiv.org\/pdf\/2502.13130",
    "GitHub":"https:\/\/github.com\/microsoft\/Magma",
    "HuggingFace":null,
    "Query1":"I showed it a screenshot of a software interface and just said “move this to the right,” and it actually did it. How can it understand both the image and my instruction like that?",
    "Query2":"I showed it a screenshot of a software interface and just said “move this to the right,” and it actually did it. How can it understand both the image and my instruction like that?",
    "Query3":"I tried giving it a goal like “make coffee,” and it broke it down into steps by watching what I was doing. What kind of model can act like a helpful visual assistant?"
  },
  {
    "Model":null,
    "Model Unique Name":"Semantic-SAM: Segment and Recognize Anything at Any Granularity",
    "Category":"segmentation",
    "Detailed Category":null,
    "Dataset":"SA-1B, Objects365, COCO panoptic, ADE20k panoptic, PASCAL Part, PACO, PartImageNet",
    "Paper":"https:\/\/arxiv.org\/pdf\/2307.04767",
    "GitHub":"https:\/\/github.com\/UX-Decoder\/Semantic-SAM?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I clicked on an image and it instantly labeled not just the object, but also its parts—like wheels, handles, and buttons. How does it recognize things at that level of detail?",
    "Query2":"Even in crowded photos, it separated overlapping people and objects without a problem—and knew what each thing was. What lets it segment and recognize so precisely?",
    "Query3":"I used it on a street scene and it broke everything down—cars, signs, poles, even sidewalk lines. How can one model handle all that with such fine granularity?"
  },
  {
    "Model":null,
    "Model Unique Name":"Segment Everything Everywhere All at Once",
    "Category":"segmentation",
    "Detailed Category":null,
    "Dataset":"MS COCO, DAVIS",
    "Paper":"https:\/\/arxiv.org\/pdf\/2304.06718",
    "GitHub":"https:\/\/github.com\/UX-Decoder\/Segment-Everything-Everywhere-All-At-Once?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I dropped in a random photo, and it instantly outlined every object—big or small—without me selecting anything. How does it know what to segment without prompts?",
    "Query2":"Even in busy street scenes, it separated people, signs, cars, and even bags they were holding. What helps it detect and segment so many things at once?",
    "Query3":"I used it on a video and it kept track of every object frame by frame without losing consistency. How does this model handle space and time together like that?"
  },
  {
    "Model":null,
    "Model Unique Name":"SAM 2: Segment Anything in Images and Videos",
    "Category":"segmentation",
    "Detailed Category":null,
    "Dataset":"SA-1B, SA-V",
    "Paper":"https:\/\/arxiv.org\/pdf\/2408.00714",
    "GitHub":"https:\/\/github.com\/facebookresearch\/sam2?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I clicked once on a photo and it instantly highlighted the object I meant—no manual outlining or labels. How does it know what to segment with such little input?",
    "Query2":"Tried it on a video clip, and it followed the object through every frame without me doing anything extra. What allows it to handle segmentation across time so smoothly?",
    "Query3":"Even with transparent or overlapping objects, it still nailed the boundaries. What kind of model sees through complex visual scenes that well?"
  },
  {
    "Model":null,
    "Model Unique Name":"Grounding DINO",
    "Category":"Zero-Shot Object Detection",
    "Detailed Category":null,
    "Dataset":"OpenImages v5 subset, Localized Narratives (OpenImages), LVIS subset, RefCOCO, RefCOCO+, RefCOCOg, gRefCOCO, COCO, Objects365 v1 & v2, V3Det, GRIT, GQA, Flickr30K Entities, ODinW13\/35",
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.05499",
    "GitHub":"https:\/\/github.com\/IDEA-Research\/GroundingDINO?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I typed in “a person holding a red umbrella” and clicked the image, and it found the exact person—even in a crowded street. How does it find such specific objects just from my words?",
    "Query2":"Even when I described unusual things like “a green suitcase next to a dog,” it still knew exactly what I meant. What helps this model understand such open-set queries?",
    "Query3":"Tried it across different photos, and I could just say what I wanted to find—it didn’t need categories or labels. How does it recognize anything I describe, even without training on it?"
  },
  {
    "Model":null,
    "Model Unique Name":"One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer",
    "Category":"3D Reconstruction",
    "Detailed Category":null,
    "Dataset":"COCO-WholeBody, MPII, Human3.6M, UBody, AGORA, EHF",
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.16160",
    "GitHub":"https:\/\/github.com\/IDEA-Research\/OSX",
    "HuggingFace":null,
    "Query1":"I uploaded a regular photo of someone standing, and it created a full 3D model—including face, hands, and body—in seconds. How does it rebuild a whole body from just one image?",
    "Query2":"Even with challenging poses or partially hidden limbs, it still captured the full shape accurately. What helps it recover detailed body structure so reliably?",
    "Query3":"I tried it for character animation, and the 3D mesh moved naturally with every joint. What kind of model makes realistic full-body rigs from a single frame?"
  },
  {
    "Model":null,
    "Model Unique Name":"Recognize Anything Model",
    "Category":"Image Tagging",
    "Detailed Category":null,
    "Dataset":"COCO, Visual Genome, Conceptual Captions, SBU Captions, Conceptual 12M",
    "Paper":"https:\/\/arxiv.org\/pdf\/2310.15200",
    "GitHub":"https:\/\/github.com\/OPPOMKLab\/recognize-anything?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I uploaded a random photo, and it instantly listed everything in the scene—people, objects, even background elements. How does it recognize so much without needing specific prompts?",
    "Query2":"Even abstract things like “sunset,” “celebration,” or “mood” were picked up in the tags. What kind of model understands not just objects, but also context and feeling?",
    "Query3":"I used it on travel photos, and it generated perfect tags I could use for organizing or searching later. How does it create such useful and relevant descriptions?"
  },
  {
    "Model":null,
    "Model Unique Name":"VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking",
    "Category":"3D detection & tracking",
    "Detailed Category":null,
    "Dataset":"nuScenes, Waymo, Argoverse2, KITTI",
    "Paper":"https:\/\/arxiv.org\/pdf\/2303.11301",
    "GitHub":"https:\/\/github.com\/dvlab-research\/VoxelNeXt?tab=readme-ov-file",
    "HuggingFace":null,
    "Query1":"I tested it on LiDAR data from a moving car, and it tracked all the vehicles and pedestrians in real time. How does it detect and follow 3D objects so fast and accurately?",
    "Query2":"Even in dense traffic with lots of overlapping objects, it managed to keep everything separated and consistent. What helps it stay reliable in such complex scenes?",
    "Query3":"I used it on data from different cities, and it adapted without retraining. How does this model stay generalizable across environments and sensor types?"
  }
]