# src/db/vectordb.py

import faiss
import json
import numpy as np
from pathlib import Path
from db.embedder import MODEL_ID

# ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë””ë ‰í† ë¦¬
MODEL_NAME = MODEL_ID.split("/")[-1]
BASE = Path(__file__).resolve().parent.parent
DATA_DIR = BASE / "data" / "158_model" / MODEL_NAME

def load_index_and_docs(index_path, json_path):
    index = faiss.read_index(str(index_path))
    docs = json.load(open(json_path, encoding="utf-8"))
    return index, docs

# â”€â”€ ì¸ë±ìŠ¤ ë¶„ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
index_dict = {
    "models": {
        "index": load_index_and_docs(
            DATA_DIR / "merged_data.faiss",
            DATA_DIR / "merged_data.json"
        )[0],
        "docs": load_index_and_docs(
            DATA_DIR / "merged_data.faiss",
            DATA_DIR / "merged_data.json"
        )[1],
    },
    "arxiv": {
        "index": load_index_and_docs(
            DATA_DIR / "arxiv_index.faiss",
            DATA_DIR / "arxiv_data.json"
        )[0],
        "docs": load_index_and_docs(
            DATA_DIR / "arxiv_index.faiss",
            DATA_DIR / "arxiv_data.json"
        )[1],
    }
}

# â”€â”€ ë¶„ë¦¬ ê²€ìƒ‰ í•¨ìˆ˜ ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_split(query_vec: np.ndarray, k_models: int = 5, k_arxiv: int = 5):
    """
    FAISS ì¸ë±ìŠ¤ë¥¼ ëª¨ë¸/ë…¼ë¬¸ ê°ê° ë¶„ë¦¬í•˜ì—¬ ê²€ìƒ‰.
    ë°˜í™˜: (models_docs[], arxiv_docs[])
    """
    results = {}

    for key, k in [("models", k_models), ("arxiv", k_arxiv)]:
        # key = "models", "arxiv"
        # k = 5, 5
        # ì´ 2ë²ˆ loop í•œë‹¤.
        cfg = index_dict[key]
        D, I = cfg["index"].search(np.array([query_vec], dtype="float32"), k)
        results[key] = [cfg["docs"][i] for i in I[0]]

    return results["models"], results["arxiv"]