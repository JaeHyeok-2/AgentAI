# src/db/vectordb.py

import faiss, json, numpy as np
from pathlib import Path
from db.embedder import MODEL_ID  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸ ID

# ğŸ”§ ì„ë² ë”© ëª¨ë¸ëª…ì— ë”°ë¼ í•´ë‹¹ í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
MODEL_NAME = MODEL_ID.split("/")[-1]
BASE = Path(__file__).resolve().parent.parent  # src/
DATA_DIR = BASE / "data" / MODEL_NAME         # e.g., data/e5-large-v2/

def load_index_and_docs(index_path, json_path):
    index = faiss.read_index(str(index_path))
    docs  = json.load(open(json_path, encoding="utf-8"))
    return index, docs

# â”€â”€ ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
index_dict = {
    "models": {
        "index": load_index_and_docs(
            DATA_DIR / "New_AI_model_no_query.faiss",
            DATA_DIR / "New_AI_model_no_query.json"
        )[0],
        "docs": load_index_and_docs(
            DATA_DIR / "New_AI_model_no_query.faiss",
            DATA_DIR / "New_AI_model_no_query.json"
        )[1],
        "boost": 0.0
    },
    "arxiv": {
        "index": load_index_and_docs(
            DATA_DIR / "arxiv_index.faiss",
            DATA_DIR / "arxiv_data.json"
        )[0],
        "docs": load_index_and_docs(
            DATA_DIR / "arxiv_index.faiss",
            DATA_DIR / "arxiv_data.json"
        )[1],
        "boost": 0.0
    }
}

# â”€â”€ ê²€ìƒ‰ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search(query_vec: np.ndarray, k_each: int = 5, k_final: int = 5):
    """
    ë‘ ì¸ë±ìŠ¤(models + arxiv)ë¥¼ ëª¨ë‘ ê²€ìƒ‰í•˜ê³ ,
    ê±°ë¦¬ + boost ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ k_final ë¬¸ì„œ ë°˜í™˜
    """
    hits = []

    for cfg in index_dict.values():
        D, I = cfg["index"].search(
            np.array([query_vec], dtype="float32"), k_each
        )
        for dist, idx in zip(D[0], I[0]):
            doc = cfg["docs"][idx]
            hits.append((dist + cfg["boost"], doc))

    hits.sort(key=lambda x: x[0])
    return [doc for _, doc in hits[:k_final]]