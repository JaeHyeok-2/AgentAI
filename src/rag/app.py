import streamlit as st
import json
import os
from retriever import retrieve_models_and_papers
from prompt import build_prompt
from answer import generate_answer_with_feedback

def build_rag_prompt(prompt: str, model_name: str) -> str:
    return f"""
You are an expert AI scientist.

The user has a question. Please answer based **only on the following retrieved academic papers** from arXiv (from 2023 to 2025).

The recommended AI model for this task is **{model_name}**.

{prompt.strip()}

Remember:
- Start your answer by explicitly recommending the model: {model_name}.
- Structure the answer into numbered steps for clarity.
- Use only the provided context.
- Do not make up facts or refer to papers not listed.
- Make your explanation clear and suitable for non-expert users.

Answer:
"""

st.set_page_config(page_title="RAG ì¿¼ë¦¬ ì‹¤í–‰ê¸°", layout="centered")
st.title("ğŸ“š ëª¨ë¸ 1ê°œ + ë…¼ë¬¸ ì¶”ì²œ + GPT ì‘ë‹µ (ì„ íƒ ì‹¤í–‰)")

# ğŸ”¹ ì¿¼ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
with open("/home/cvlab/Desktop/AgentAI/dataset/merged_query.json", encoding="utf-8") as f:
    model_queries = json.load(f)

query_pool = []
for model in model_queries:
    model_name = model.get("Model Unique Name", "Unknown Model")
    for i in range(1, 4):
        q = model.get(f"Query{i}", "").strip()
        if q:
            query_pool.append((q, model_name))

if "use_rerank" not in st.session_state:
    st.session_state["use_rerank"] = True
use_rerank = st.checkbox("ğŸš€ Cross-Encoder Rerank ì‚¬ìš©", value=st.session_state["use_rerank"])

if "results" not in st.session_state:
    st.session_state["results"] = []

if "cache" not in st.session_state:
    st.session_state["cache"] = {}

st.markdown(f"ì´ {len(query_pool)}ê°œì˜ ì¿¼ë¦¬ì— ëŒ€í•´ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# ğŸ”¹ ì¿¼ë¦¬ ìˆœì°¨ ì‹¤í–‰
for i, (query, model_name) in enumerate(query_pool, 1):
    query_key = f"query_{i}"
    st.markdown(f"---\n### ğŸ” Query {i}: {query}")
    st.markdown(f"ğŸ“Œ ì›ë˜ ëª¨ë¸: `{model_name}`")

    if st.button(f"ğŸ” ë¬¸ì„œ ì¶”ì²œ ì‹¤í–‰ (Query {i})"):
        with st.spinner("â³ ë¬¸ì„œ ë° ëª¨ë¸ ì¶”ì²œ ì¤‘..."):
            try:
                models, papers = retrieve_models_and_papers(query, k_models=1, k_arxiv=5, use_rerank=use_rerank)
                prompt = build_prompt(query, papers[:3], models)
                st.session_state["cache"][query_key] = {
                    "query": query,
                    "model_name": model_name,
                    "model": models[0] if models else None,
                    "papers": papers,
                    "prompt": prompt
                }
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜: {e}")
                continue

    if query_key in st.session_state["cache"]:
        cached = st.session_state["cache"][query_key]
        selected_model = cached["model"]
        papers = cached["papers"]
        prompt = cached["prompt"]

        st.markdown("### ğŸ›  ì¶”ì²œ AI ëª¨ë¸")
        if selected_model:
            url = selected_model.get("GitHub") or selected_model.get("HuggingFace") or "#"
            st.markdown(f"- **[{selected_model['Model Unique Name']}]({url})**")
        else:
            st.markdown("_ì¶”ì²œ ëª¨ë¸ ì—†ìŒ_")

        st.markdown("### ğŸ“– ê´€ë ¨ ë…¼ë¬¸ Top 5")
        for idx, p in enumerate(papers):
            dist = p.get("dist", 0.0)
            score = p.get("score")
            score_str = f", Score={score:.3f}" if score is not None else ""
            st.markdown(f"**{idx+1}. [{p['Model Unique Name']}]({p['Paper']})** (L2={dist:.3f}{score_str})")

        st.markdown("### ğŸ“ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸")
        st.code(prompt, language="text")

        if st.button(f"ğŸ¤– GPT ì‘ë‹µ ìƒì„± (Query {i})"):
            with st.spinner("LLM ì‘ë‹µ ìƒì„± ì¤‘..."):
                try:
                    rag_prompt = build_rag_prompt(prompt, selected_model["Model Unique Name"])
                    answer = generate_answer_with_feedback(rag_prompt, selected_model["Model Unique Name"])
                    st.success("âœ… GPT ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                    st.markdown(answer)
                except Exception as e:
                    st.error(f"LLM ì˜¤ë¥˜: {e}")
                    answer = ""

            st.session_state["results"].append({
                "query": query,
                "original_model": model_name,
                "recommended_model": selected_model.get("Model Unique Name") if selected_model else None,
                "recommended_papers": [p["Model Unique Name"] for p in papers],
                "prompt": prompt,
                "answer": answer
            })

# ğŸ”¹ ê²°ê³¼ ì €ì¥ (ì‘ë‹µ í¬í•¨)
if st.session_state["results"]:
    if st.button("ğŸ’¾ ì „ì²´ ê²°ê³¼ JSON ì €ì¥"):
        save_path = "/home/cvlab/Desktop/AgentAI/output/llm_rag_results.json"
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(st.session_state["results"], f, indent=2, ensure_ascii=False)
        st.success(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {save_path}")

# ğŸ”¹ í”„ë¡¬í”„íŠ¸ë§Œ ì €ì¥ (TXT + JSON)
if st.button("ğŸ’¾ í”„ë¡¬í”„íŠ¸ë§Œ ì €ì¥ (.txt + .json)"):
    prompt_dir = "/home/cvlab/Desktop/AgentAI/output/prompts"
    os.makedirs(prompt_dir, exist_ok=True)
    all_prompts = []

    for idx, (k, v) in enumerate(st.session_state["cache"].items(), 1):
        prompt_text = v["prompt"]
        model_name = v["model_name"].replace("/", "_").replace(" ", "_")
        query = v["query"]

        txt_path = os.path.join(prompt_dir, f"prompt_{idx:03d}_{model_name}.txt")
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(prompt_text)

        all_prompts.append({
            "index": idx,
            "query": query,
            "original_model": model_name,
            "prompt": prompt_text
        })

    json_path = os.path.join(prompt_dir, "all_prompts.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_prompts, f, indent=2, ensure_ascii=False)

    st.success(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ â†’ {prompt_dir}")