#!/usr/bin/env python3
"""
PROMPT_ROOT ì•ˆì˜ ëª¨ë“  Query*.txt í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì•„
ì§€ì •í•œ LLM(OpenAIÂ·ClaudeÂ·Gemini)ì„ í˜¸ì¶œí•œ ë’¤
ê°™ì€ í´ë”ì— ì‘ë‹µì„ ì €ì¥í•œë‹¤.
"""

import os, glob, time, traceback, sys
import openai, anthropic, google.generativeai as genai

# â”€â”€ í™˜ê²½ ë³€ìˆ˜ë¡œë¶€í„° API í‚¤ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai.api_key      = os.getenv("OPENAI_API_KEY")
anthropic_client    = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# â”€â”€ ì‚¬ìš©ì ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_ROOT = "output/prompts_by_model_query1_bge"      # í”„ë¡¬í”„íŠ¸ ìµœìƒìœ„ í´ë”
LLM_KEYS = {                                           # ê²°ê³¼íŒŒì¼ëª… : model_key
    "claude_sonnet4.txt": "anthropic/claude-sonnet-4-20250514",
    "gemini_pro25.txt":   "gemini/gemini-2.5-pro",
    # "gpt4o.txt":       "openai/gpt-4o-mini",
}
TEMPERATURE = 0.7
PAUSE_SEC   = 1.0                                      # ì—°ì† í˜¸ì¶œ ê°„ ëŒ€ê¸°

# â”€â”€ LLM í˜¸ì¶œ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def call_llm(model_key: str, prompt: str, *, temperature: float = 0.7) -> str:
    """model_key ê·œì¹™ì— ë”°ë¼ LLM í˜¸ì¶œ, ì‘ë‹µ ë¬¸ìì—´ ë°˜í™˜"""

    # --- OpenAI ------------------------------------------------
    if model_key.startswith("openai/"):
        resp = openai.ChatCompletion.create(
            model=model_key.split("/", 1)[1],
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
        )
        return resp.choices[0].message.content.strip()

    # --- Anthropic (Claude) -----------------------------------
    if model_key.startswith(("anthropic/", "claude")):
        name = model_key.split("/", 1)[1] if "/" in model_key else model_key
        resp = anthropic_client.messages.create(
            model=name,
            max_tokens=2048,
            temperature=temperature,
            system="You are a helpful assistant.",
            messages=[{"role": "user", "content": [{"type": "text", "text": prompt}]}],
        )
        return resp.content[0].text.strip()

    # --- Google Gemini ---------------------------------------
    if model_key.startswith("gemini/"):
        real_name = model_key.split("/", 1)[1]
        model     = genai.GenerativeModel(real_name)
        try:
            resp = model.generate_content(
                prompt,
                generation_config={
                    "temperature": temperature,
                    "max_output_tokens": 4096,
                },
            )
            if resp and resp.parts:
                return resp.text.strip()
            # ì‘ë‹µì´ ë¹„ì—ˆê±°ë‚˜ ì•ˆì „ í•„í„°ì— ê±¸ë¦° ê²½ìš°
            msg = "(ë¹ˆ ì‘ë‹µ ë˜ëŠ” Safety Filter)"
            if resp.prompt_feedback and resp.prompt_feedback.safety_ratings:
                msg += f" ì•ˆì „ë“±ê¸‰: {resp.prompt_feedback.safety_ratings}"
            print(f"âš ï¸  Gemini ê²½ê³ : {msg}")
            return ""
        except Exception as e:
            print(f"âŒ Gemini í˜¸ì¶œ ì˜¤ë¥˜({model_key}): {e}")
            traceback.print_exc()
            return ""

    # --- ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ í‚¤ -------------------------------
    raise ValueError(f"[call_llm] ì§€ì›ë˜ì§€ ì•ŠëŠ” model_key: {model_key}")

# â”€â”€ í”„ë¡¬í”„íŠ¸ í•˜ë‚˜ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_on_prompt(prompt_path: str) -> None:
    with open(prompt_path, encoding="utf-8") as f:
        prompt = f.read()

    folder = os.path.dirname(prompt_path)

    for out_fname, mkey in LLM_KEYS.items():
        out_path = os.path.join(folder, out_fname)
        if os.path.exists(out_path):
            print("â­ï¸  ì´ë¯¸ ì¡´ì¬, ê±´ë„ˆëœ€:", out_path)
            continue
        try:
            print(f"ğŸ”¸ {mkey} â†’ {out_path}")
            answer = call_llm(mkey, prompt, temperature=TEMPERATURE)
            with open(out_path, "w", encoding="utf-8") as fo:
                fo.write(answer)
            time.sleep(PAUSE_SEC)
        except Exception as e:
            print(f"âŒ {mkey} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:", e)
            traceback.print_exc()

# â”€â”€ ë°°ì¹˜ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(root: str) -> None:
    pattern = os.path.join(root, "**", "Query*.txt")
    prompts = glob.glob(pattern, recursive=True)
    print(f"â–¶ï¸  {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë°œê²¬")
    for p in prompts:
        run_on_prompt(p)

# â”€â”€ CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import argparse
    ap = argparse.ArgumentParser(description="CNAPS LLM ë°°ì¹˜ ì‹¤í–‰")
    ap.add_argument("--root", default=PROMPT_ROOT,
                    help="Query*.txt íŒŒì¼ë“¤ì´ ìˆëŠ” ìµœìƒìœ„ í´ë” ê²½ë¡œ")
    args = ap.parse_args()

    PROMPT_ROOT =  "/home/cvlab/Desktop/AgentAI/output/prompts_by_model_query1_bge/Colorization-DISCO-c0_2"
    main(PROMPT_ROOT)