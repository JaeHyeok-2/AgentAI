[
  {
    "Model": null,
    "Model Unique Name": "SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation",
    "Category": "GNN",
    "Detailed Category": null,
    "Dataset": "Amazon-Book, Gowalla, MovieLens, Yelp",
    "Paper": "https://arxiv.org/pdf/2405.20878",
    "GitHub": "https://github.com/HKUDS/SelfGNN",
    "HuggingFace": null,
    "Summary": "SelfGNN is a self-supervised graph neural network designed for sequential recommendation. It builds short-term collaborative graphs to capture local user-item interactions, integrates multi-level long-term sequence modeling using attention and GRU, and applies a personalized self-augmentation mechanism to filter out noisy user behaviors. The method significantly outperforms existing baselines on multiple real-world datasets, showing both improved accuracy and robustness."
  },
  {
    "Model": null,
    "Model Unique Name": "AnimateDiff-Lightning: Cross-Model Diffusion Distillation",
    "Category": "Text-to-Video",
    "Detailed Category": null,
    "Dataset": "WebVid-10M, Pandas-70M",
    "Paper": "https://arxiv.org/pdf/2403.12706",
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/ByteDance/AnimateDiff-Lightning",
    "Summary": "AnimateDiff‑Lightning is a lightweight, few‑step video-generation model optimized via a cross-model distillation framework. It takes a robust teacher model (AnimateDiff) and distills it into a streamlined student architecture capable of producing high-quality video sequences in just 1–4 inference steps. A key innovation is the cross-model distillation mechanism, where a shared motion module is trained jointly across multiple base diffusion models to ensure consistent style and dynamics. This approach achieves performance competitive with state-of-the-art models like AnimateLCM, while drastically reducing computational cost and inference time. The implementation is released for the research community."
  },
  {
    "Model": null,
    "Model Unique Name": "AnimateLCM: Computation-Efficient Personalized Style Video Generation without Personalized Video Data",
    "Category": "Text-to-Video",
    "Detailed Category": null,
    "Dataset": "UCF101",
    "Paper": "https://arxiv.org/pdf/2402.00769",
    "GitHub": "https://github.com/G-U-N/AnimateLCM",
    "HuggingFace": "https://huggingface.co/wangfuyun/AnimateLCM",
    "Summary": "AnimateLCM introduces a fast, efficient framework for generating videos in a personalized style without needing personalized video data. It decouples two learning objectives: (1) capturing video style from (non-personalized) style data, and (2) accelerating both image generation and motion generation. By separating image and motion priors during training, the method reduces inference time drastically—from ~25 seconds to ~1 second per clip—while maintaining high visual quality. The model supports adapter modules (e.g., ControlNet), remaining lightweight and highly adaptable. Experiments show that AnimateLCM achieves performance comparable to heavier diffusion methods, but in a fraction of the time . Code and pretrained weights are publicly available ."
  },
  {
    "Model": null,
    "Model Unique Name": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA",
    "Category": "Feature Extraction",
    "Detailed Category": null,
    "Dataset": "MTEB",
    "Paper": "https://arxiv.org/pdf/2409.10173",
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/jinaai/jina-embeddings-v3",
    "Summary": "jina‑embeddings‑v3 introduces a 570M‑parameter transformer-based embedding model optimized via task-specific LoRA adapters. It supports multilingual (over 100 languages) and long-context (up to 8192 tokens) scenarios, while remaining efficient enough for on-edge deployment. Key techniques include adapter tuning for retrieval, clustering, classification, and text matching, Matryoshka Representation Learning for flexible dimensionality (from 1024 down to 32), and synthetic data augmentation to improve retrieval robustness. On the MTEB benchmark, it outperforms proprietary models (e.g. OpenAI, Cohere) in English and exceeds multilingual‑e5‑large‑instruct across all multilingual tasks."
  },
  {
    "Model": null,
    "Model Unique Name": "JINA CLIP: Your CLIP Model Is Also Your Text Retriever",
    "Category": "Feature Extraction",
    "Detailed Category": null,
    "Dataset": "LAION, MTEB",
    "Paper": "https://arxiv.org/pdf/2405.20204",
    "GitHub": null,
    "HuggingFace": null,
    "Summary": "This paper introduces Jina CLIP, a unified contrastive learning model trained jointly on both image-caption and text-text pairs. By combining cross-modal and text-only objectives in a multi-task framework, Jina CLIP closes the performance gap between CLIP and specialized text retrievers. Experiments show that its text encoder rivals dedicated text models on MTEB benchmarks, while its multimodal capability matches or exceeds CLIP (e.g., achieving ~85.8% Recall@5 across Flickr8k, Flickr30K, MS-COCO), on par with EVA‑CLIP, and outperforming OpenAI’s CLIP on text retrieval . Jina CLIP is fully open-sourced and supports both text-to-text and text-to-image retrieval within a single, efficient model ."
  },
  {
    "Model": null,
    "Model Unique Name": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "Category": "Fill-Mask",
    "Detailed Category": null,
    "Dataset": "GLUE, BEIR, MLDR, CodeSearchNet, StackQA",
    "Paper": "https://arxiv.org/pdf/2412.13663",
    "GitHub": "https://github.com/AnswerDotAI/ModernBERT",
    "HuggingFace": "https://huggingface.co/answerdotai/ModernBERT-large",
    "Summary": "ModernBERT is a thoroughly modernized encoder-only transformer model that incorporates state-of-the-art architecture optimizations—such as RoPE positional embeddings, alternating local-global attention, Flash Attention, and unpadding—for significant efficiency gains. Pre-trained on a massive 2 trillion tokens, including code and long-context data (up to 8,192 tokens), it achieves state-of-the-art results across diverse classification and retrieval tasks—especially single- and multi-vector semantic search, including code retrieval. The model offers excellent performance–size trade-offs, strong speed and memory efficiency on standard GPUs, and is available in base (~149 M parameters) and large (~395 M parameters) sizes."
  },
  {
    "Model": null,
    "Model Unique Name": "M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
    "Category": "Sentence Similarity",
    "Detailed Category": null,
    "Dataset": "MTEB, MIRACL, MKQA, BEIR, C-MTEB, MLDR",
    "Paper": "https://arxiv.org/pdf/2402.03216",
    "GitHub": "https://github.com/FlagOpen/FlagEmbedding",
    "HuggingFace": "https://huggingface.co/BAAI/bge-m3",
    "Summary": "BGE M3‑Embedding (M3‑Embedding) is a unified text embedding model designed for versatility across 100+ languages, three retrieval functions (dense, multi-vector, sparse), and input lengths up to 8,192 tokens. It employs a novel self-knowledge distillation technique, using ensemble-style teacher signals derived from its own multi-modal retrieval heads, alongside an optimized batching strategy for large-scale training. M3‑Embedding offers state-of-the-art performance on multilingual, cross-lingual, and long-document retrieval benchmarks (e.g., MIRACL, MKQA), and its model, code, and data have been made publicly available ."
  },
  {
    "Model": null,
    "Model Unique Name": "DEPTH PRO : Sharp Monocular Metric Depth In Less Than a Second",
    "Category": "Depth Estimation",
    "Detailed Category": null,
    "Dataset": "AM-2k, DIS-5k",
    "Paper": "https://arxiv.org/pdf/2410.02073",
    "GitHub": "https://github.com/apple/ml-depth-pro",
    "HuggingFace": "https://huggingface.co/apple/DepthPro",
    "Summary": "Depth Pro is a fast and accurate model for monocular depth estimation that delivers sharp, metric-scale depth maps in under a second. Unlike previous models that require camera intrinsics or extensive fine-tuning, Depth Pro works zero-shot across diverse scenes and devices. It uses a multi-scale Vision Transformer to preserve fine boundary details and structural sharpness in high-resolution outputs."
  },
  {
    "Model": null,
    "Model Unique Name": "Depth-Anything-V2",
    "Category": "Depth Estimation",
    "Detailed Category": null,
    "Dataset": "BlendedMVS, Hypersim, IRS, TartanAir, KITTI",
    "Paper": "https://arxiv.org/pdf/2406.09414",
    "GitHub": "https://github.com/DepthAnything/Depth-Anything-V2",
    "HuggingFace": "https://huggingface.co/depth-anything/Depth-Anything-V2-Large",
    "Summary": "Depth Anything V2 improves performance by leveraging three core ideas: using synthetic-only labeled data, training a large DINOv2-G based teacher model, and distilling it into smaller student models using pseudo-labels on 62M real images."
  },
  {
    "Model": null,
    "Model Unique Name": "FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION",
    "Category": "Text-to-Image",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2406.06858",
    "GitHub": "https://github.com/black-forest-labs/flux",
    "HuggingFace": "https://huggingface.co/black-forest-labs/FLUX.1-dev",
    "Summary": "FLUX introduces a novel software-level GPU optimization designed to hide communication latency by tightly fusing fine-grained communication and computation steps into unified kernels. Instead of treating communication and computation separately, FLUX over-decomposes operations and fuses them into larger GPU kernels, enabling up to 96% overlap of communication with work that would otherwise sit idle. In practice, this achieves up to 1.24× speedups in distributed training with Megatron‑LM on 128 GPUs, and up to 1.66×/1.30× speedups in prefill and decoding inference with vLLM on 8-GPU clusters. This approach delivers significant efficiency gains without altering the model architecture or compromising kernel performance ."
  },
  {
    "Model": null,
    "Model Unique Name": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
    "Category": "Image-to-Image",
    "Detailed Category": null,
    "Dataset": "Emu Edit, MagicBrush",
    "Paper": "https://arxiv.org/abs/2504.20690",
    "GitHub": "https://github.com/River-Zhang/ICEdit?tab=readme-ov-file",
    "HuggingFace": "https://huggingface.co/RiverZ/normal-lora",
    "Summary": "In‑Context Edit introduces a unified and efficient framework for instruction-guided image editing that requires no structural model modifications or extensive data. It leverages the native contextual capabilities of a large-scale Diffusion Transformer (DiT) to perform zero-shot edits based on textual instructions. The framework enhances adaptability by integrating a LoRA-MoE hybrid tuning method that activates modular experts dynamically, and it employs an early-filter mechanism using vision-language models to refine initial noise samples and improve output quality. Evaluated against state-of-the-art methods, it achieves superior editing precision while using only 0.5 % of training data and 1 % of trainable parameters compared to conventional baselines . Code and demos are available via its project page ."
  },
  {
    "Model": null,
    "Model Unique Name": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "Category": "Image-to-Video",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/abs/2503.20314",
    "GitHub": "https://github.com/Wan-Video/Wan2.1",
    "HuggingFace": "https://huggingface.co/Wan-AI",
    "Summary": "Wan is a suite of open-source video foundation models built on diffusion transformer architecture, engineered to push the boundaries of video generation. The suite includes two models—a compact 1.3 B parameter version optimized for consumer-grade GPUs (≈8 GB VRAM) and a larger 14 B parameter variant—for balanced efficiency and performance. Key innovations include a novel diffusion-aware VAE, scalable pretraining on billions of image/video samples, curated large-scale datasets, and automated evaluation protocols. Across multiple benchmarks, Wan consistently outperforms existing open-source and commercial models, demonstrating clear scaling benefits. By openly releasing all model checkpoints and code, Wan aims to accelerate innovation in video generative modeling ."
  },
  {
    "Model": null,
    "Model Unique Name": "SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation",
    "Category": "Image-to-Video",
    "Detailed Category": null,
    "Dataset": "ObjaverseDSy, Consistent4D, DAVIS",
    "Paper": "https://arxiv.org/pdf/2503.16396",
    "GitHub": "https://github.com/Stability-AI/generative-models",
    "HuggingFace": "https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt",
    "Summary": "SV4D 2.0 is a unified diffusion-based framework that transforms a single-view input video into high-fidelity novel-view videos and optimizes a 4D dynamic 3D asset. By redesigning the network architecture to remove dependency on separate multi-view references and adopting a blend of 3D and temporal attention, it achieves robust performance even with occlusions and large motion. The model is further enhanced through progressive multi-stage training, improved data curation, and a two-stage refinement process with progressive frame sampling. Compared to its predecessor, SV4D 2.0 significantly enhances visual detail sharpness (14% LPIPS reduction) and spatio-temporal consistency (44% FV4D improvement), and delivers pronounced gains across both synthetic and real-world benchmarks (e.g., ObjaverseDy, DAVIS) ."
  },
  {
    "Model": null,
    "Model Unique Name": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters",
    "Category": "Image-to-Video",
    "Detailed Category": null,
    "Dataset": "LatentSync-filtered AV data, CelebV-HQ, HDTF, Self-Collected Full-Body Wild",
    "Paper": "https://arxiv.org/pdf/2505.20156",
    "GitHub": "https://github.com/Tencent-Hunyuan/HunyuanVideo-Avatar",
    "HuggingFace": "https://huggingface.co/tencent/HunyuanVideo-Avatar",
    "Summary": "HunyuanVideo‑Avatar is a diffusion transformer model that generates highly dynamic, multi-character dialogue videos driven solely by audio inputs. It overcomes character identity drift by injecting reference images channel-wise instead of via naive addition, enabling both expressiveness and consistency. An Audio Emotion Module ensures that generated facial expressions accurately reflect emotional tone, while a Face‑Aware Audio Adapter isolates audio-driven motion to specific face regions, enabling synchronized multi-character performance. Evaluated on standard benchmarks and a new wild dataset, the model demonstrates marked improvements in motion realism, lip-sync accuracy, and emotion alignment, outperforming prior audio-driven animation methods across key metrics ."
  },
  {
    "Model": null,
    "Model Unique Name": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
    "Category": "Zero-Shot Image Classification",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2303.00915",
    "GitHub": "https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
    "HuggingFace": "https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
    "Summary": "BiomedCLIP introduces PMC‑15M, a large-scale biomedical dataset consisting of 15 million image–caption pairs extracted from 4.4 million articles in PubMed Central. By fine-tuning both image and text encoders—leveraging domain-specialized architectures like PubMedBERT—it adapts CLIP-style contrastive training to the biomedical domain. The resulting model sets new state-of-the-art results in cross-modal retrieval, zero-shot image classification, and visual question answering across eight standard biomedical benchmarks. Notably, BiomedCLIP even surpasses radiology-specific models such as BioViL on pneumonia detection, highlighting the benefits of pretraining on diverse biomedical image types ."
  },
  {
    "Model": null,
    "Model Unique Name": "Scaling Open-Vocabulary Object Detection",
    "Category": "Zero-Shot Object Detection",
    "Detailed Category": null,
    "Dataset": "WebLI, LVIS, ODinW13",
    "Paper": "https://arxiv.org/pdf/2306.09683",
    "GitHub": "https://github.com/inuwamobarak/OWLv2",
    "HuggingFace": "https://huggingface.co/google/owlv2-large-patch14-ensemble",
    "Summary": "This paper presents OWLv2 and its OWL‑ST self‑training recipe that unlock web-scale weak supervision for open‑vocabulary object detection by generating pseudo‑box labels on massive image–text datasets. OWLv2 enhances training efficiency through optimized architectures and data augmentation techniques, allowing it to surpass prior state-of-the-art detectors using only ~10 million examples. OWL‑ST further scales training to over 1 billion image–text pairs, improving zero-shot detection average precision (AP) on rare LVIS classes from 31.2% to 44.6% using a ViT-L/14 backbone. The approach requires no architectural changes or extra annotation, demonstrating that scaling self-training on weak supervision matches trends in image-level models and significantly advances open-world localization ."
  },
  {
    "Model": null,
    "Model Unique Name": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
    "Category": "Zero-Shot Object Detection",
    "Detailed Category": null,
    "Dataset": "OpenImages v5 subset, Localized Narratives (OpenImages), LVIS subset, RefCOCO, RefCOCO+, RefCOCOg, gRefCOCO",
    "Paper": "https://arxiv.org/pdf/2303.05499",
    "GitHub": "https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file",
    "HuggingFace": "https://huggingface.co/IDEA-Research/grounding-dino-tiny",
    "Summary": "Grounding DINO enhances the DETR-based DINO object detector by incorporating language-aware grounded pre-training, enabling zero-shot detection of arbitrary concepts specified via text. The model integrates language cues into key detector stages—feature enhancement, query selection, and a cross-modality decoder—to achieve tight multimodal fusion. It is pre-trained on detection, grounding, and caption datasets, and excels in open-set and referring-expression detection tasks. Empirical results show that Grounding DINO achieves a zero-shot COCO average precision (AP) of 52.5 and sets new records in the ODinW benchmark, while maintaining strong performance on LVIS and RefCOCO datasets ."
  },
  {
    "Model": null,
    "Model Unique Name": "Structured 3D Latents for Scalable and Versatile 3D Generation∗",
    "Category": "Text-to-3D",
    "Detailed Category": null,
    "Dataset": "ObjaverseXL (sketchfab), ObjaverseXL (github), ABO, 3D-FUTURE, HSSD",
    "Paper": "https://arxiv.org/pdf/2412.01506",
    "GitHub": "https://github.com/Microsoft/TRELLIS",
    "HuggingFace": "https://huggingface.co/microsoft/TRELLIS-text-xlarge",
    "Summary": "This paper introduces a unified structured latent (SLAT) representation that combines a sparse 3D grid with dense visual features from a vision foundation model, enabling versatile decoding into various 3D formats including radiance fields, 3D Gaussians, and meshes. Leveraging rectified flow transformers, the authors train multi-billion parameter models (up to 2B) on a diverse dataset of half a million 3D assets. The resulting system, named TRELLIS, generates high-fidelity 3D geometry and textures in approximately 10 seconds from either text or image prompts, surpassing prior methods at similar scale. Additionally, TRELLIS supports local, prompt-driven editing without retraining, offering an efficient and flexible foundation for scalable, versatile 3D content creation ."
  },
  {
    "Model": null,
    "Model Unique Name": "TripoSR: Fast 3D Object Reconstruction from a Single Image",
    "Category": "Image-to-3D",
    "Detailed Category": null,
    "Dataset": "Objaverse",
    "Paper": "https://arxiv.org/pdf/2403.02151",
    "GitHub": "https://github.com/VAST-AI-Research/TripoSR",
    "HuggingFace": "https://huggingface.co/stabilityai/stable-fast-3d",
    "Summary": "TripoSR is a transformer-based model designed for fast, feed-forward 3D mesh reconstruction from a single image, achieving inference in under 0.5 seconds on high-end GPUs without per-shape optimization. Building on the LRM architecture, TripoSR integrates advances in data curation, encoder-decoder design, and training techniques. It outputs textured 3D triplane-based neural radiance fields, and evaluations across standard benchmarks demonstrate state-of-the-art performance—improving Chamfer distance and F-score over prior open-source methods—while offering significantly faster throughput. Released under the MIT license with code, demo, and pretrained models publicly available, TripoSR supports practical deployment and further research ."
  },
  {
    "Model": null,
    "Model Unique Name": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
    "Category": "Image-to-3D",
    "Detailed Category": null,
    "Dataset": "Objaverse, Objaverse-XL",
    "Paper": "https://arxiv.org/abs/2501.12202",
    "GitHub": "https://github.com/Tencent-Hunyuan/Hunyuan3D-2",
    "HuggingFace": "https://huggingface.co/tencent/Hunyuan3D-2",
    "Summary": "Hunyuan3D 2.0 delivers an open-source, two-stage diffusion-based system for generating high-resolution textured 3D assets from images. The first stage, Hunyuan3D‑DiT, employs a flow-based diffusion transformer to generate detailed geometry conditioned on input images, while the second stage, Hunyuan3D‑Paint, synthesizes vibrant texture maps using geometric and diffusion priors. Together with a user-friendly production platform—Hunyuan3D‑Studio—users can easily generate, edit, stylize, and animate meshes. Extensive quantitative and qualitative evaluations, including user studies, confirm that Hunyuan3D 2.0 surpasses both open- and closed-source baselines in geometry fidelity, texture quality, and condition alignment, offering a significant tool for scalable 3D content creation ."
  },
  {
    "Model": null,
    "Model Unique Name": "VGGT: Visual Geometry Grounded Transformer",
    "Category": "Image-to-3D",
    "Detailed Category": null,
    "Dataset": "Co3Dv2, BlendMVS, DL3DV, MegaDepth, Kubric, WildRGB, ScanNet, HyperSim, Mapillary Metropolis, Habitat, Replica, MVS-Synth, PointOdyssey, Virtual KITTI, Aria Synthetic Environments, Aria Digital Twin, Objaverse",
    "Paper": "https://arxiv.org/pdf/2503.11651",
    "GitHub": "https://github.com/facebookresearch/vggt",
    "HuggingFace": "https://huggingface.co/facebook/VGGT-1B",
    "Summary": "VGGT is a feed-forward transformer designed to directly infer comprehensive 3D scene attributes—camera parameters, depth maps, point maps, and 3D point tracks—from one or hundreds of input views in under a second. By forgoing explicit geometric post-processing (like bundle adjustment) and applying a standard transformer with alternating frame-level and global attention, VGGT consistently matches or exceeds traditional optimization-based methods on multiple 3D benchmarks. The model’s learned features also effectively enhance downstream tasks such as novel-view synthesis and non-rigid point tracking. With publicly released code and pretrained weights, VGGT establishes a fast, unified foundation for versatile, multi-task 3D vision that scales seamlessly with increased input views ."
  },
  {
    "Model": null,
    "Model Unique Name": "BGE: One-Stop Retrieval Toolkit For Search and RAG",
    "Category": "Text Classification",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2412.14475",
    "GitHub": "https://github.com/FlagOpen/FlagEmbedding/tree/master?tab=readme-ov-file",
    "HuggingFace": "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "Summary": "MegaPairs proposes a scalable data synthesis pipeline that combines vision-language models (CLIP and DINO) with open-domain image corpora to generate heterogeneous triplets of images and textual instructions. By mining correlated image pairs with varied relationships and using multimodal and language models to produce descriptive, open-ended instructions, the method creates over 26 million high-quality training instances. Models trained on MegaPairs, named MMRet, outperform baselines trained on 70× more private data, achieving state-of-the-art zero-shot performance on composed image retrieval (CIR) benchmarks and across 36 datasets in the MMEB suite. MegaPairs and its trained retrievers are fully open-sourced to support further research and applications ."
  },
  {
    "Model": null,
    "Model Unique Name": "GLiNER: Generalist and Lightweight Model for Named Entity Recognition",
    "Category": "Token Classification",
    "Detailed Category": null,
    "Dataset": "Pile‑NER",
    "Paper": "https://arxiv.org/pdf/2311.08526",
    "GitHub": "https://github.com/urchade/GLiNER",
    "HuggingFace": "https://huggingface.co/urchade/gliner_multi_pii-v1",
    "Summary": "GLiNER is a compact, bidirectional transformer model that reframes named entity recognition as a matching problem between textual span embeddings and entity-type embeddings, rather than sequential token generation. By encoding both entity labels and candidate text spans in a shared latent space, it enables efficient parallel extraction of arbitrary entity types. Despite its smaller size, GLiNER outperforms larger LLMs like ChatGPT and fine-tuned models on zero-shot benchmarks, and demonstrates robust multilingual capabilities—even on languages unseen during training. It serves as a resource-efficient and accurate solution for flexible, instruction-driven NER tasks ."
  },
  {
    "Model": null,
    "Model Unique Name": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
    "Category": "Feature Extraction",
    "Detailed Category": null,
    "Dataset": "MTEB, synthetic text relevance dataset (~150M pairs), high-quality supervised dataset (~7M pairs)",
    "Paper": "https://arxiv.org/pdf/2506.05176",
    "GitHub": "https://github.com/QwenLM/Qwen3-Embedding",
    "HuggingFace": "https://huggingface.co/Qwen/Qwen3-Embedding-0.6B",
    "Summary": "Qwen3 Embedding presents a series of embedding models (0.6B, 4B, and 8B parameters) derived from the Qwen3 foundation models, designed for both text embedding and reranking tasks. The authors introduce a multi-stage training pipeline with large-scale unsupervised pretraining followed by supervised fine-tuning on high-quality relevance datasets, and apply model-merging strategies to improve robustness. They further leverage the Qwen3 instruct model to synthesize a diverse, multilingual text relevance dataset for supervised training. Evaluations show that Qwen3 Embedding achieves state-of-the-art performance across multiple benchmarks, outperforming its predecessor (GTE‑Qwen) and matching or surpassing other contemporary embedding models in both efficiency and effectiveness ."
  },
  {
    "Model": null,
    "Model Unique Name": "DeepSeek-R1",
    "Category": null,
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2501.12948",
    "GitHub": "https://github.com/deepseek-ai/DeepSeek-R1",
    "HuggingFace": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",
    "Summary": "DeepSeek‑R1 presents a novel paradigm for enhancing large language model reasoning through pure reinforcement learning (RL), first demonstrating that a base model (DeepSeek‑R1‑Zero) can develop chain-of-thought reasoning and self-reflection without any supervised fine-tuning. To improve readability and reduce language mixing, the authors introduce DeepSeek‑R1, which adds a cold-start supervised stage followed by additional RL and supervised fine-tuning. This multi-stage approach achieves reasoning performance on par with OpenAI’s o1‑1217 model across benchmarks like AIME, MATH‑500, MMLU, GPQA, Codeforces, and more. They also showcase effective distillation, transferring reasoning capabilities into smaller models (1.5B–70B), where a 14B student outperforms open‑source baselines and larger models—highlighting RL + distillation as a cost-efficient path to producing strong reasoning systems. All models and code are open-sourced ."
  },
  {
    "Model": null,
    "Model Unique Name": "VGGT: Visual Geometry Grounded Transformer",
    "Category": " 3D Vision",
    "Detailed Category": null,
    "Dataset": "Co3Dv2, Objaverse, NYUv2",
    "Paper": "https://arxiv.org/pdf/2503.11651",
    "GitHub": "https://github.com/facebookresearch/vggt",
    "HuggingFace": null,
    "Summary": "VGGT is a large feed-forward transformer that directly infers comprehensive 3D scene attributes—including camera intrinsics/extrinsics, depth maps, point maps, and 3D point tracks—from anywhere between one to hundreds of input views, in under a second, eliminating the need for traditional geometry post-processing like bundle adjustment. It achieves state-of-the-art performance across multiple tasks—camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and point tracking—often matching or surpassing methods that rely on optimization, while also enhancing downstream applications such as feed-forward novel-view synthesis and non-rigid point tracking. Built with minimal 3D-specific inductive biases (aside from alternating frame-level and global attention), VGGT leverages a transformer backbone trained on large-scale annotated 3D data, and its code and models are publicly released ."
  },
  {
    "Model": null,
    "Model Unique Name": "MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors",
    "Category": " 3D Vision",
    "Detailed Category": null,
    "Dataset": "TUM-RGBD, 7-Scenes, EuRoC, ETH3D",
    "Paper": "https://arxiv.org/pdf/2412.12392",
    "GitHub": "https://github.com/rmurai0610/MASt3R-SLAM",
    "HuggingFace": null,
    "Summary": "MASt3R‑SLAM introduces a real‑time monocular dense SLAM system built atop a two-view 3D reconstruction prior from MASt3R, enabling robust and in‑the‑wild performance without requiring fixed camera intrinsics beyond a single camera center. The system fuses fine-grained pointmap matching, efficient camera tracking, local volumetric fusion, graph construction with loop closure, and second-order global optimization into a unified framework running at around 15 FPS. When camera calibration is known, it achieves state‑of‑the‑art accuracy in both trajectory estimation and dense geometry recovery across standard benchmarks. By integrating a learned geometric prior directly into the SLAM pipeline, MASt3R‑SLAM provides a plug‑and‑play solution for globally consistent monocular dense mapping and localization ."
  },
  {
    "Model": null,
    "Model Unique Name": "UniK3D: Universal Camera Monocular 3D Estimation",
    "Category": "depth estimation",
    "Detailed Category": null,
    "Dataset": "KITTI, NYUv2, MegaDepth, NianticMapFree, Mapillary",
    "Paper": "https://arxiv.org/pdf/2503.16591",
    "GitHub": "https://github.com/lpiccinelli-eth/UniK3D",
    "HuggingFace": null,
    "Summary": "UniK3D introduces a breakthrough in monocular 3D estimation by supporting any camera model—from standard pinhole to fisheye and panoramic—without requiring calibration or rectification. The model leverages a spherical representation for both scene geometry and camera projection, where rays are encoded via learned spherical harmonics. An angular loss is incorporated to prevent geometric contraction in wide-field scenarios. Evaluated zero-shot across 13 diverse datasets, UniK3D achieves state-of-the-art performance in metric depth, 3D point-cloud reconstruction, and camera parameter estimation, particularly excelling in extreme field-of-view conditions while matching performance on standard narrow views. The method is fully open-sourced, with code and pretrained models available."
  },
  {
    "Model": null,
    "Model Unique Name": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
    "Category": "depth estimation",
    "Detailed Category": null,
    "Dataset": "Sintel, ScanNet, KITTI, Bonn, NYU-v2",
    "Paper": "https://arxiv.org/pdf/2409.02095",
    "GitHub": "https://github.com/Tencent/DepthCrafter",
    "HuggingFace": null,
    "Summary": "DepthCrafter adapts a pre-trained image-to-video diffusion model to generate temporally coherent, long-range depth sequences for diverse real-world videos without relying on camera poses or optical flow. Through a carefully designed three-stage training pipeline using both synthetic and real paired video-depth data, the method enables zero-shot depth generation of up to 110 frames at once. At inference, it processes longer clips via segment-wise estimation with seamless stitching. It delivers state-of-the-art performance in zero-shot open-world video depth estimation, with rich geometry and temporal consistency, and supports downstream tasks such as depth-based visual effects and conditional video editing ."
  },
  {
    "Model": null,
    "Model Unique Name": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
    "Category": "depth estimation",
    "Detailed Category": null,
    "Dataset": "KITTI, Bonn, ScanNet, Sinte l, DAVIS",
    "Paper": "https://arxiv.org/pdf/2501.12375",
    "GitHub": "https://github.com/DepthAnything/Video-Depth-Anything",
    "HuggingFace": null,
    "Summary": "Video Depth Anything extends the image-trained Depth Anything V2 model by replacing its head with a lightweight spatio-temporal module and introducing a simple temporal-gradient matching loss to enforce depth consistency across time, all without requiring optical flow or camera pose priors. The system also implements a key-frame–based inference strategy that enables zero-shot depth estimation on arbitrarily long videos—spanning several minutes—while maintaining both spatial accuracy and temporal coherence. Evaluated across multiple video benchmarks, it establishes new state-of-the-art performance in zero-shot video depth estimation, balancing quality, efficiency, and scalability, with the smallest variant running in real-time (~30 FPS) ."
  },
  {
    "Model": null,
    "Model Unique Name": "Interpreting Object-level Foundation Models via Visual Precision Search",
    "Category": "explainability and interpretability",
    "Detailed Category": null,
    "Dataset": "MS COCO, RefCOCO, LVIS",
    "Paper": "https://arxiv.org/pdf/2411.16198",
    "GitHub": "https://github.com/RuoyuChen10/VPS",
    "HuggingFace": null,
    "Summary": "This paper introduces Visual Precision Search (VPS), a novel, gradient-free method for generating accurate, instance-specific saliency maps to interpret object-level foundation models like Grounding DINO and Florence‑2. VPS works by segmenting input images into sparse superpixel regions and ranking them using consistency and collaboration scores via a submodular selection mechanism—resulting in precise localization of critical decision regions. Evaluations on MS COCO, RefCOCO, and LVIS show that VPS greatly improves attribution faithfulness—up to 31.6% for Grounding DINO and 102.9% for Florence‑2—and reliably identifies factors contributing to detection failures. The method bypasses internal gradient reliance common in multimodal detectors, offers theoretical guarantees on region selection, and opens a path for more trustworthy interpretation of vision-language models ."
  },
  {
    "Model": null,
    "Model Unique Name": "Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders",
    "Category": "explainability and interpretability",
    "Detailed Category": null,
    "Dataset": "GazeFollow, VideoAttentionTarget, ChildPlay, GOO-Real",
    "Paper": "https://arxiv.org/pdf/2412.09586",
    "GitHub": "https://github.com/fkryan/gazelle",
    "HuggingFace": null,
    "Summary": "Gaze‑LLE simplifies gaze target estimation by harnessing a frozen DINOv2 visual encoder to extract a unified scene representation, then injecting person‑specific positional prompts to decode gaze targets using a lightweight transformer decoder. This streamlined approach eliminates the need for complex multi‑branch pipelines and auxiliary inputs like depth or pose, while achieving state‑of‑the‑art performance on multiple gaze benchmarks. Despite using one to two orders of magnitude fewer trainable parameters, Gaze‑LLE delivers efficient inference (~50 FPS on an RTX 4090), strong cross‑dataset generalization, and rapid training (≈1.5 GPU hours), all with publicly released code and models ."
  },
  {
    "Model": null,
    "Model Unique Name": "MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
    "Category": "generative models",
    "Detailed Category": null,
    "Dataset": "AudioSet, Freesound, VGGSound, AudioCaps, WavCaps",
    "Paper": "https://arxiv.org/pdf/2412.15322",
    "GitHub": "https://github.com/hkchengrex/MMAudio",
    "HuggingFace": null,
    "Summary": "MMAudio presents a unified framework for video-to-audio synthesis that is jointly trained on paired video–audio data and large-scale text–audio corpora, enabling high-quality and semantically aligned audio generation. The model enhances audio–visual synchrony using a novel conditional synchronization module that aligns audio latents with video frames at a fine-grained level. With only 157M parameters and using a flow-matching objective, MMAudio achieves state-of-the-art performance in terms of audio fidelity, semantic relevance, and temporal alignment, while keeping inference time low (~1.23 s for an 8‑second clip). Interestingly, joint training does not compromise single-modality performance, as MMAudio also demonstrates competitive results in text-to-audio tasks ."
  },
  {
    "Model": null,
    "Model Unique Name": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
    "Category": "generative models",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2403.09055",
    "GitHub": "https://github.com/ironjr/semantic-draw",
    "HuggingFace": null,
    "Summary": "SemanticDraw introduces a transformative interactive image-generation tool that allows content creators to define multiple user-drawn regions, each tied to separate text prompts, and synthesize high-quality images in real time (≈0.64 s per 512×512 image) on a single RTX 2080 Ti GPU. It tackles the incompatibility between region-based control and accelerated diffusion sampling by integrating latent pre-averaging, mask-centering bootstrapping, and mask quantization, enabling LCM-compatible processing in just a few steps. Further, a multi-prompt streaming pipeline batches region-based prompts through continuous diffusion steps, achieving ~1.57 FPS in panorama generation—over 10× faster than prior MultiDiffusion approaches. This enables a new “semantic palette” interface where users can paint with text-linked brushes interactively. The method is model-agnostic, supports any existing diffusion models and schedulers, and is supported by open-source code ."
  },
  {
    "Model": null,
    "Model Unique Name": "MINIMA: Modality Invariant Image Matching",
    "Category": "image matching",
    "Detailed Category": null,
    "Dataset": "MD-syn (MegaDepth-Syn), LLVIP, M3FD, MSRS, METU-VisTIR, MMIM, DIODE, DSEC (RGB-Event)",
    "Paper": "https://arxiv.org/pdf/2412.19412",
    "GitHub": "https://github.com/LSXI7/MINIMA",
    "HuggingFace": null,
    "Summary": "MINIMA introduces a unified image matching framework that generalizes across diverse modalities—such as RGB, depth, infrared, event cameras, and artistic styles—by generating large-scale multimodal training data from inexpensive RGB sources. The core innovation is a “data engine” that converts richly annotated RGB datasets into synthetic multimodal pairs (MD‑syn) using generative models, effectively scaling paired data with accurate pixel-wise correspondence labels. Trained on MD‑syn, MINIMA achieves remarkable cross-modal performance and strong zero-shot transfer, outperforming modality-specific methods on 19 real-world matching tasks. The code and dataset are publicly available to support future multimodal matching research ."
  },
  {
    "Model": null,
    "Model Unique Name": "Layered Image Vectorization via Semantic Simplification",
    "Category": "image vectorization",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2406.05404",
    "GitHub": "https://github.com/SZUVIZ/layered_vectorization",
    "HuggingFace": null,
    "Summary": "This paper introduces a progressive, two-stage image vectorization pipeline that reconstructs raster images as layered vector graphics by employing semantic simplification. First, a diffusion-based Score Distillation Sampling method simplifies the image in stages—from coarse semantic structures down to fine details—producing a sequence of abstraction levels. Then, those simplified images guide a structured vector-fitting process that builds clean, semantic-aligned SVG layers. The resulting vector output is both visually faithful and compact, enabling intuitive editing and semantic-aware manipulation. Experiments across diverse image types demonstrate that this method outperforms traditional single-pass vectorization approaches in terms of layer coherence, editability, and file size efficiency ."
  },
  {
    "Model": null,
    "Model Unique Name": "DEIM: DETR with Improved Matching for Fast Convergence",
    "Category": "object detection",
    "Detailed Category": null,
    "Dataset": "COCO",
    "Paper": "https://arxiv.org/pdf/2412.04234",
    "GitHub": "https://github.com/ShihuaHuang95/DEIM",
    "HuggingFace": null,
    "Summary": "DEIM enhances Transformer-based DETR object detectors by replacing sparse one-to-one (O2O) matching with Dense O2O, which generates more positive samples per image via standard data augmentations. To address noise from lower-quality matches, the framework introduces a Matchability-Aware Loss (MAL) that weights matches based on confidence, improving training stability and speed. Experiments on COCO show DEIM reduces training time by 50% and achieves 53.2% AP on RT-DETRv2 after just one day of training on a 4090 GPU. Real-time detectors trained with DEIM—such as DEIM-D-FINE-L and DEIM-D-FINE-X—outperform state-of-the-art models at up to 124 FPS, without extra data or architectural changes ."
  },
  {
    "Model": null,
    "Model Unique Name": "MITracker: Multi-View Integration for Visual Object Tracking",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "MVTrack, GOT-10k, GMTD",
    "Paper": "https://arxiv.org/pdf/2502.20111",
    "GitHub": "https://github.com/XuM007/MITracker",
    "HuggingFace": null,
    "Summary": "MITracker presents an innovative framework that addresses occlusion and target loss by integrating synchronized multi-view inputs into a unified 3D feature volume projected into bird’s-eye-view (BEV) space. Key to its design is an attention mechanism that fuses geometric consistency from this paired 3D representation to refine per-view tracking results, enabling robust, class-agnostic object tracking over arbitrary-length sequences. Alongside, the authors introduce MVTrack—a high-quality multi-view dataset featuring 234,000 annotated frames of 27 distinct objects across diverse scenes—which supports both training and evaluation. MITracker achieves state-of-the-art performance on the MVTrack and GMTD benchmarks, demonstrating superior robustness to occlusion and long-term consistency compared to leading single-view methods  ￼.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Multiple Object Tracking as ID Prediction",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "DanceTrack, SportsMOT, MOT17",
    "Paper": "https://arxiv.org/pdf/2403.16848",
    "GitHub": "https://github.com/MCG-NJU/MOTIP",
    "HuggingFace": null,
    "Summary": "This paper recasts multi-object tracking as a direct ID-prediction task, using a transformer-based ID Decoder that learns to assign consistent identities across frames rather than relying on tracking-by-detection pipelines. The ID Decoder employs a dynamic multi-layer structure—optimized through ablation studies—to handle appearance changes and occlusions, achieving a HOTA score improvement from 54.3 to 60.5 by moving from 1 to 6 layers. The method is fully feed-forward and demonstrates strong performance across standard MOT benchmarks without requiring complex association or motion models. By unifying detection and tracking through identity prediction, the approach simplifies the tracking framework while maintaining competitive accuracy ."
  },
  {
    "Model": null,
    "Model Unique Name": "EdgeTAM: On-Device Track Anything Model",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "DAVIS 2017, MOSE, SA-V (val/test), YouTube-VOS",
    "Paper": "https://arxiv.org/pdf/2501.07256",
    "GitHub": "https://github.com/facebookresearch/EdgeTAM",
    "HuggingFace": null,
    "Summary": "EdgeTAM adapts SAM 2 for on-device video segmentation and tracking by introducing a 2D Spatial Perceiver module that compresses frame‑level memory features while preserving spatial structure, and employs feature‑level distillation from the full SAM 2 model. As a result, it runs at 16 FPS on iPhone 15 Pro Max—about 22× faster than SAM 2—while delivering comparable or better accuracy on DAIVS‐2017, MOSE, SA‑V, and YouTubeVOS benchmarks (e.g., 87.7 J&F on DAVIS’17). Training involved aligning both encoder and memory-attention features between teacher and student, yielding a lightweight yet high-performing on-device system ."
  },
  {
    "Model": null,
    "Model Unique Name": "A Distractor-Aware Memory for Visual Object Tracking with SAM2",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "DiDi, VOT2020, VOT2022, VOTChallenge, LaSOT, GOT-10k",
    "Paper": "https://arxiv.org/pdf/2411.17576",
    "GitHub": "https://github.com/jovanavidenovic/DAM4SAM",
    "HuggingFace": null,
    "Summary": "This work enhances memory-based visual object tracking—particularly with SAM2—by introducing a novel Distractor-Aware Memory (DAM) that separates recent target appearances (RAM) from hard-to-distinguish distractors (DRM). The key innovation is an introspection-driven update mechanism: when SAM2’s predicted mask diverges from an alternative segmentation, the frame is added to DRM to boost robustness. They also release DiDi, a distractor-distilled dataset designed to expose real-world tracking failures. Without additional training, the resulting SAM2.1++ significantly outperforms SAM2.1 and related variants across seven benchmarks, setting a new state-of-the-art on six of them."
  },
  {
    "Model": null,
    "Model Unique Name": "From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "Market1501, SYSU‑MM01, Occluded‑ReID",
    "Paper": "https://arxiv.org/pdf/2503.00938",
    "GitHub": "https://github.com/yuanc3/Pose2ID",
    "HuggingFace": null,
    "Summary": "Pose2ID introduces a training‑free person Re‑ID approach that enhances identity feature stability by centralizing embeddings around per‑identity feature centers. The method comprises two key components: an identity‑guided pedestrian generator that synthesizes diverse view samples to enrich identity representation, and neighbor feature centralization that aggregates proximal neighbors in embedding space to reduce noise from occlusion and background interference. Surprisingly, without any Re‑ID model fine‑tuning, Pose2ID achieves strong performance—52.8 mAP / 78.9 Rank‑1 on Market1501—and sets new benchmarks across cross‑modality and occluded Re‑ID tasks ."
  },
  {
    "Model": null,
    "Model Unique Name": "Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models",
    "Category": "open-world detection",
    "Detailed Category": null,
    "Dataset": "Anomaly-Instruct-125k, VisA-D&R, MVTec AD, AITEX, ELPV, BTAD, MPDD, BrainMRI, HeadCT, Br35H",
    "Paper": "https://arxiv.org/pdf/2502.07601",
    "GitHub": "https://github.com/honda-research-institute/Anomaly-OneVision",
    "HuggingFace": null,
    "Summary": "This paper introduces Anomaly‑Instruct‑125k, the first large-scale visual-instruction tuning dataset for zero-shot anomaly detection (ZSAD) and reasoning, paired with a comprehensive benchmark called VisA‑D&R. By evaluating current multimodal large language models (MLLMs) like GPT‑4o, the authors demonstrate these models struggle to detect and explain fine-grained visual anomalies. To address this, they propose Anomaly‑OneVision (Anomaly‑OV), a specialist system that leverages a Look‑Twice Feature Matching mechanism to adaptively select and emphasize abnormal visual tokens for precise detection and human-readable reasoning. Anomaly‑OV significantly outperforms generalist models on detection accuracy and anomaly interpretation, and extends effectively to industrial, medical, and 3D anomaly scenarios ."
  },
  {
    "Model": null,
    "Model Unique Name": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
    "Category": "pose estimation",
    "Detailed Category": null,
    "Dataset": "Human3.6M, MPI-INF-3DHP, COCO, MPII, AI Challenger, AVA, InstaVariety, MOYO",
    "Paper": "https://arxiv.org/pdf/2503.21751",
    "GitHub": "https://github.com/IsshikiHugh/HSMR",
    "HuggingFace": null,
    "Summary": "This work introduces HSMR (Human Skeleton and Mesh Recovery), a novel transformer-based model that reconstructs a full-body 3D mesh with a biomechanically accurate skeleton, unlike standard models like SMPL. HSMR is trained using a self-generated pseudo-ground-truth pipeline: it synthesizes SKEL-model parameters for real images and iteratively refines them through training loops to improve accuracy. The result delivers anatomically plausible joint predictions and high-fidelity surface meshes from a single image. By combining SKEL’s biomechanical rig with robust image-to-parameter regression, HSMR enables realistic and physically consistent 3D human modeling and is publicly released with project code ."
  },
  {
    "Model": null,
    "Model Unique Name": "MatAnyone: Stable Video Matting with Consistent Memory Propagation",
    "Category": "segmentation",
    "Detailed Category": null,
    "Dataset": "VM800, YoutubeMatte",
    "Paper": "https://arxiv.org/pdf/2501.14677",
    "GitHub": "https://github.com/pq-yang/MatAnyone",
    "HuggingFace": null,
    "Summary": "MatAnyone introduces a memory-based video matting framework that enables temporally stable and semantically consistent matte extraction for a user-specified target across diverse scenes. By incorporating a region‑adaptive memory fusion module, the system selectively integrates previous-frame embeddings to maintain core object consistency while preserving high-fidelity boundary details. The authors also curated a high-quality, diverse matting dataset and devised a segmentation-assisted training strategy leveraging large-scale segmentation data to bolster model robustness. MatAnyone significantly outperforms prior auxiliary-free matting methods, particularly in complex or cluttered backgrounds, and is backed by an accompanying dataset and open-source implementation ."
  },
  {
    "Model": null,
    "Model Unique Name": "FoundationStereo: Zero-Shot Stereo Matching",
    "Category": "stereo matching",
    "Detailed Category": null,
    "Dataset": "FoundationStereo Dataset (1M synthetic stereo pairs), Scene Flow, Middlebury, ETH3D, KITTI",
    "Paper": "https://arxiv.org/pdf/2501.09898",
    "GitHub": "https://github.com/NVlabs/FoundationStereo",
    "HuggingFace": null,
    "Summary": "FoundationStereo presents a foundation-model approach for stereo depth estimation that achieves strong zero-shot generalization across diverse real-world domains without requiring per-domain fine-tuning. The model is trained on a curated 1 million synthetic stereo image pair dataset and enhanced with architectural components like a side-tuning feature backbone to adapt monocular priors, attentive long-range cost-volume filtering, and iterative refinement. These strategies together close the sim-to-real gap and deliver state-of-the-art robustness and accuracy on challenging scenarios—such as reflective surfaces, thin structures, and extreme lighting—establishing a new zero-shot benchmark for stereo matching. Though not yet real-time (≈0.7 s per image on A100), it sets the stage for deployable, generalizable stereo depth models ."
  },
  {
    "Model": null,
    "Model Unique Name": "Towards Universal Soccer Video Understanding",
    "Category": "video understanding",
    "Detailed Category": null,
    "Dataset": "SoccerReplay-1988, SoccerNet-v2, SoccerNet",
    "Paper": "https://arxiv.org/pdf/2412.01820",
    "GitHub": "https://github.com/jyrao/UniSoccer",
    "HuggingFace": null,
    "Summary": "This work introduces SoccerReplay-1988, the largest multi-modal soccer dataset comprising nearly 2,000 full matches with rich annotations—including timestamped events and match commentary—generated via an automated pipeline. The authors also present MatchVision, the first soccer-specific visual-language foundation model that extends visual backbones with spatiotemporal attention to excel across tasks like event classification, commentary generation, and multi-view foul recognition. MatchVision achieves state-of-the-art performance on both existing benchmarks and newly introduced evaluation tasks, demonstrating the effectiveness of large-scale data collection and unified modeling for comprehensive sports video understanding ."
  },
  {
    "Model": null,
    "Model Unique Name": "Magma: A Foundation Model for Multimodal AI Agents",
    "Category": "visual agents",
    "Detailed Category": null,
    "Dataset": "SeeClick, Vision2UI, Ego4D, EpicKitchen, Something‑Something v2, Open‑X‑Embodiment",
    "Paper": "https://arxiv.org/pdf/2502.13130",
    "GitHub": "https://github.com/microsoft/Magma",
    "HuggingFace": null,
    "Summary": "Magma is a unified multimodal AI foundation model that bridges verbal intelligence (understanding visual-language inputs) and spatial-temporal intelligence (planning and executing actions in digital and physical environments). Through joint pretraining on heterogeneous datasets—including labeled UIs with actionable “Set-of-Mark” annotations and videos with “Trace-of-Mark” motion traces—it learns to ground actions and plan multi-step sequences across tasks like GUI navigation and robotic manipulation. Magma achieves state-of-the-art performance on both specialized agentic benchmarks and general vision-language tasks, despite not using larger datasets, and offers unified, zero-shot adaptability across modalities. Code and model checkpoints are publicly available."
  },
  {
    "Model": null,
    "Model Unique Name": "Semantic-SAM: Segment and Recognize Anything at Any Granularity",
    "Category": "segmentation",
    "Detailed Category": null,
    "Dataset": "SA-1B, Objects365, COCO panoptic, ADE20k panoptic, PASCAL Part, PACO, PartImageNet",
    "Paper": "https://arxiv.org/pdf/2307.04767",
    "GitHub": "https://github.com/UX-Decoder/Semantic-SAM?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "Semantic‑SAM extends the Segment‑Anything paradigm by jointly learning semantic awareness and multi-granularity mask generation in an interactive model. It consolidates generic object, part, and SA‑1B segmentation datasets into a unified training pipeline with decoupled classifiers that distinguish both object-level and part-level semantics. By integrating a multi-choice learning strategy, the model produces multiple valid masks for each user click, spanning from entire objects down to fine-grained parts. This first attempt at combined training on SA‑1B, generic, and part datasets yields a model capable of interactive segmentation at any desired level of granularity, and training with SA‑1B further boosts its performance on panoptic and part segmentation tasks  ￼. The project is open-sourced with demos and full code available .\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Segment Everything Everywhere All at Once",
    "Category": "segmentation",
    "Detailed Category": null,
    "Dataset": "MS COCO, DAVIS",
    "Paper": "https://arxiv.org/pdf/2304.06718",
    "GitHub": "https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "SEEM presents a promptable, interactive segmentation model that supports diverse user inputs—including points, boxes, scribbles, masks, text prompts, and even referring regions from other images—by encoding them into a unified visual-semantic space. It employs a Transformer encoder-decoder architecture with memory prompts to retain segmentation history, enabling efficient multi-round interaction without re-running the image encoder. Designed for versatility, compositionality, interactivity, and open-vocabulary segmentation, SEEM delivers competitive performance across nine segmentation tasks—interactive, generic, referring, and video segmentation—often with minimal supervision (as little as 1%)  ￼. Its model-agnostic design and compositional prompting allow zero-shot generalization to novel combinations of prompts and modalities."
  },
  {
    "Model": null,
    "Model Unique Name": "SAM 2: Segment Anything in Images and Videos",
    "Category": "segmentation",
    "Detailed Category": null,
    "Dataset": "SA-1B, SA-V",
    "Paper": "https://arxiv.org/pdf/2408.00714",
    "GitHub": "https://github.com/facebookresearch/sam2?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "SAM 2 is a unified transformer-based foundation model that enables promptable segmentation for both images and videos by introducing streaming memory to carry prompt context across frames. The authors also developed a data engine to create SA‑V, the largest video segmentation dataset to date, collecting 35.5 million masks across 50,900 videos. Compared to its predecessor, SAM 2 achieves around 3× fewer user interactions for video segmentation and is 6× faster on image tasks, while maintaining or improving accuracy. It offers state-of-the-art performance in interactive video segmentation benchmarks and strong zero-shot generalization across 17 video and 37 image datasets. All model weights, dataset, and code are publicly released  ￼."
  },
  {
    "Model": null,
    "Model Unique Name": "Grounding DINO",
    "Category": "Zero-Shot Object Detection",
    "Detailed Category": null,
    "Dataset": "OpenImages v5 subset, Localized Narratives (OpenImages), LVIS subset, RefCOCO, RefCOCO+, RefCOCOg, gRefCOCO, COCO, Objects365 v1 & v2, V3Det, GRIT, GQA, Flickr30K Entities, ODinW13/35",
    "Paper": "https://arxiv.org/pdf/2303.05499",
    "GitHub": "https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "Grounding DINO integrates language understanding into the transformer-based DINO object detector, enabling detection of any text-specified concept—be it category names or complex referring expressions—without additional model modifications. It introduces a tight cross-modal fusion design with language-aware feature enhancements, language-guided query selection, and a cross-modality decoder, and is pre-trained across detection, grounding, and caption datasets. The model achieves state-of-the-art zero-shot open-vocabulary performance: 52.5 AP on COCO and 26.1 AP on the ODinW benchmark without seeing COCO labels during pre-training. It also demonstrates strong referring-expression comprehension and supports diverse downstream applications like grounding-aware image editing ."
  },
  {
    "Model": null,
    "Model Unique Name": "One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer",
    "Category": "3D Reconstruction",
    "Detailed Category": null,
    "Dataset": "COCO-WholeBody, MPII, Human3.6M, UBody, AGORA, EHF",
    "Paper": "https://arxiv.org/pdf/2303.16160",
    "GitHub": "https://github.com/IDEA-Research/OSX",
    "HuggingFace": null,
    "Summary": "OSX introduces a single-stage transformer-based pipeline that recovers detailed whole-body 3D meshes—including body, hands, and face—from a single image, without separate networks or manual post-processing. The model uses a Component-Aware Transformer, where a global body encoder provides a shared feature map for local face and hand decoders that apply feature-level upsample-crop and keypoint-guided deformable attention. This joint design preserves inter-part coherence, avoids noticeable artifacts in skeletal structure, and achieves top performance (first place) on the AGORA SMPL-X benchmark. The authors also contribute a new UBody dataset containing high-quality 2D and 3D annotations for partially visible upper-body images collected “in the wild.”"
  },
  {
    "Model": null,
    "Model Unique Name": "Recognize Anything Model",
    "Category": "Image Tagging",
    "Detailed Category": null,
    "Dataset": "COCO, Visual Genome, Conceptual Captions, SBU Captions, Conceptual 12M",
    "Paper": "https://arxiv.org/pdf/2306.03514",
    "GitHub": "https://github.com/OPPOMKLab/recognize-anything?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "Recognize Anything (RAM) is a foundation-level model for general-purpose image tagging that achieves strong zero-shot performance by training on 14 million image–text pairs without manual labels. RAM’s pipeline uses automatic text semantic parsing to generate large-scale tag annotations, a unified model architecture that combines captioning and tagging, and a data engine that refines tags through expansion and cleaning. The model outperforms CLIP, BLIP, and even fully supervised tagging models—including commercial APIs—across multiple benchmarks, recognizing over 6,400 common categories, and is released with open-source weights and code ."
  },
  {
    "Model": null,
    "Model Unique Name": "VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking",
    "Category": "3D detection & tracking",
    "Detailed Category": null,
    "Dataset": "nuScenes, Waymo, Argoverse2, KITTI",
    "Paper": "https://arxiv.org/pdf/2303.11301",
    "GitHub": "https://github.com/dvlab-research/VoxelNeXt?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "VoxelNeXt introduces a clean, fully sparse 3D object detection and tracking framework based entirely on sparse voxel features—eliminating anchors, center proxies, sparse-to-dense conversion, and NMS. Built on sparse convolutional networks, it achieves top-tier speed–accuracy trade-offs on benchmarks like nuScenes, Waymo, and Argoverse2, and outperforms all previous LiDAR-based methods in nuScenes tracking—it even set the leaderboard’s best result  ￼."
  }
]