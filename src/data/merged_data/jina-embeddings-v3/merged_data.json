[
  {
    "Model": null,
    "Model Unique Name": "Classification-HuggingFace-falconsai-nsfw_image_detection",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-21k",
    "Paper": "https://arxiv.org/pdf/2010.11929",
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/Falconsai/nsfw_image_detection",
    "Summary": "This paper proposes Vision Transformer (ViT), which applies a pure Transformer architecture directly to image classification by treating image patches as tokens. The model is pre-trained on large-scale image datasets and then fine-tuned on smaller benchmarks. The ViT model demonstrates superior performance compared to traditional CNN-based architectures when trained on sufficiently large datasets, outperforming them with substantially lower computational resources. Key experiments show that ViT achieves state-of-the-art results on several standard image classification benchmarks."
  },
  {
    "Model": null,
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-18",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "GitHub": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-18",
    "Summary": "This paper introduces a novel deep residual learning framework aimed at addressing the optimization difficulties encountered in training very deep neural networks. Instead of fitting stacked layers to approximate a direct mapping, the authors propose learning residual mappings via identity shortcut connections. The proposed architecture, ResNet, significantly mitigates the degradation problem, allowing the successful training of networks with substantially increased depth. Experiments on ImageNet demonstrate that ResNet models significantly outperform previous architectures, achieving state-of-the-art results with reduced complexity"
  },
  {
    "Model": null,
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-50",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "GitHub": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-50",
    "Summary": "This paper introduces a novel deep residual learning framework aimed at addressing the optimization difficulties encountered in training very deep neural networks. Instead of fitting stacked layers to approximate a direct mapping, the authors propose learning residual mappings via identity shortcut connections. The proposed architecture, ResNet, significantly mitigates the degradation problem, allowing the successful training of networks with substantially increased depth. Experiments on ImageNet demonstrate that ResNet models significantly outperform previous architectures, achieving state-of-the-art results with reduced complexity"
  },
  {
    "Model": null,
    "Model Unique Name": "Classification-HuggingFace-microsoft-resnet-101",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "ImageNet-1k",
    "Paper": "https://arxiv.org/pdf/1512.03385",
    "GitHub": "https://github.com/KaimingHe/deep-residual-networks",
    "HuggingFace": "https://huggingface.co/microsoft/resnet-101",
    "Summary": "This paper introduces a novel deep residual learning framework aimed at addressing the optimization difficulties encountered in training very deep neural networks. Instead of fitting stacked layers to approximate a direct mapping, the authors propose learning residual mappings via identity shortcut connections. The proposed architecture, ResNet, significantly mitigates the degradation problem, allowing the successful training of networks with substantially increased depth. Experiments on ImageNet demonstrate that ResNet models significantly outperform previous architectures, achieving state-of-the-art results with reduced complexity"
  },
  {
    "Model": null,
    "Model Unique Name": "Classification-HuggingFace-NTQAI-pedestrian_gender_recognition",
    "Category": "Img2Txt",
    "Detailed Category": "Classification",
    "Dataset": "PETA",
    "Paper": null,
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/NTQAI/pedestrian_gender_recognition",
    "Summary": "This model card describes a Vision Transformer (microsoft/beit-base-patch16-224-pt22k-ft22k) fine-tuned for 5 epochs on the PETA dataset to recognise pedestrian gender. The resulting checkpoint (≈ 86 M parameters) is released in PyTorch, Safetensors, and ONNX formats under the Apache-2.0 license. Reported validation accuracy reaches 0.9107, with a final loss of 0.2170. Training used a learning rate of 2 × 10⁻⁵, batch size 8, the Adam optimizer, and a linear LR schedule. Usage is demonstrated with a simple pipeline(\"image-classification\") snippet."
  },
  {
    "Model": null,
    "Model Unique Name": "Inpainting-LatentDiffusion",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "LAION-400M",
    "Paper": "https://arxiv.org/pdf/2112.10752",
    "GitHub": "https://github.com/CompVis/latent-diffusion",
    "HuggingFace": null,
    "Summary": "The authors introduce Latent Diffusion Models (LDMs), a two-stage framework that first learns a perceptual autoencoder to compress images into a lower-dimensional latent space and then trains a diffusion model in that space. Operating on latents cuts both training and inference cost while preserving detail, enabling megapixel synthesis. Cross-attention layers connect arbitrary conditioning inputs—such as text, semantic maps or bounding boxes—to the UNet backbone, making the generator highly flexible. LDMs set new state-of-the-art scores for image inpainting and class-conditional synthesis, and achieve competitive results on text-to-image, unconditional generation and super-resolution, all with markedly fewer GPU resources than pixel-based diffusion models"
  },
  {
    "Model": null,
    "Model Unique Name": "Colorization-DISCO-c0_2",
    "Category": "Img2Img",
    "Detailed Category": "Colorization",
    "Dataset": "ImageNet,COCO",
    "Paper": "https://menghanxia.github.io/projects/disco/disco_main.pdf",
    "GitHub": "https://github.com/MenghanXia/DisentangledColorization",
    "HuggingFace": null,
    "Summary": "The authors build a dual-branch framework in which a small set of globally located colour “anchors” captures the multimodal colour distribution of an image, while a separate generator propagates those anchor colours across the scene to respect structural affinities. Anchors are found through clustering of super-pixel features that guarantee independence and full image coverage, and the generator refines results with luminance-guided corrections. Experiments on ImageNet, COCO-Stuff and historical photographs show higher perceptual quality, colourfulness and competitive FID compared with prior frameworks, without extra computational burden. "
  },
  {
    "Model": null,
    "Model Unique Name": "Colorization-DISCO-rand",
    "Category": "Img2Img",
    "Detailed Category": "Colorization",
    "Dataset": "ImageNet,COCO",
    "Paper": "https://menghanxia.github.io/projects/disco/disco_main.pdf",
    "GitHub": "https://github.com/MenghanXia/DisentangledColorization",
    "HuggingFace": null,
    "Summary": "The authors build a dual-branch framework in which a small set of globally located colour “anchors” captures the multimodal colour distribution of an image, while a separate generator propagates those anchor colours across the scene to respect structural affinities. Anchors are found through clustering of super-pixel features that guarantee independence and full image coverage, and the generator refines results with luminance-guided corrections. Experiments on ImageNet, COCO-Stuff and historical photographs show higher perceptual quality, colourfulness and competitive FID compared with prior frameworks, without extra computational burden. "
  },
  {
    "Model": null,
    "Model Unique Name": "Deblur-MIMO-UNet-Plus",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "GitHub": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Summary": "The authors propose MIMO-UNet, a single U-shaped network that accepts the blurry image at multiple down-scaled resolutions and outputs deblurred results for those scales in one pass. A multi-input single encoder (MISE) ingests each resolution, while a multi-output single decoder (MOSD) produces matching deblurred images. Cross-scale information is exchanged by an asymmetric feature-fusion module, and training employs a multi-scale content loss combined with a frequency-domain reconstruction loss. Experiments on GoPro and RealBlur benchmarks show that MIMO-UNet delivers higher PSNR with faster inference and far fewer parameters than stacked coarse-to-fine models."
  },
  {
    "Model": null,
    "Model Unique Name": "Deblur-MIMO-UNet-RealBlur",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "GitHub": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Summary": "The authors propose MIMO-UNet, a single U-shaped network that accepts the blurry image at multiple down-scaled resolutions and outputs deblurred results for those scales in one pass. A multi-input single encoder (MISE) ingests each resolution, while a multi-output single decoder (MOSD) produces matching deblurred images. Cross-scale information is exchanged by an asymmetric feature-fusion module, and training employs a multi-scale content loss combined with a frequency-domain reconstruction loss. Experiments on GoPro and RealBlur benchmarks show that MIMO-UNet delivers higher PSNR with faster inference and far fewer parameters than stacked coarse-to-fine models."
  },
  {
    "Model": null,
    "Model Unique Name": "Deblur-MIMO-UNet",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro,RealBlur",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.pdf",
    "GitHub": "https://github.com/chosj95/MIMO-UNet",
    "HuggingFace": null,
    "Summary": "The authors propose MIMO-UNet, a single U-shaped network that accepts the blurry image at multiple down-scaled resolutions and outputs deblurred results for those scales in one pass. A multi-input single encoder (MISE) ingests each resolution, while a multi-output single decoder (MOSD) produces matching deblurred images. Cross-scale information is exchanged by an asymmetric feature-fusion module, and training employs a multi-scale content loss combined with a frequency-domain reconstruction loss. Experiments on GoPro and RealBlur benchmarks show that MIMO-UNet delivers higher PSNR with faster inference and far fewer parameters than stacked coarse-to-fine models."
  },
  {
    "Model": null,
    "Model Unique Name": "Deblur-MSSNet-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "GitHub": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model": null,
    "Model Unique Name": "Deblur-MSSNet-L-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "GitHub": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model": null,
    "Model Unique Name": "Deblur-MSSNet-S-GoPro",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "GitHub": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model": null,
    "Model Unique Name": "Deblur-MSSNet-RealBlurJ",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "GitHub": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model": null,
    "Model Unique Name": "Deblur-MSSNet-RealBlurR",
    "Category": "Img2Img",
    "Detailed Category": "Deblur",
    "Dataset": "GoPro",
    "Paper": "https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf",
    "GitHub": "https://github.com/kky7/MSSNet",
    "HuggingFace": null,
    "Summary": "The authors revisit coarse-to-fine deblurring and diagnose three flaws in past multi-scale networks: scale-agnostic sub-networks, poor feature transfer across scales, and information loss from down-sampling. They introduce MSSNet, which assigns deeper UNet stages to finer scales, propagates rich residual features (not images) between scales, and replaces down-sampling with pixel-unshuffle/shuffle so no detail is discarded. Cross-stage and cross-scale fusion further boost information flow. Experiments on GoPro and RealBlur show MSSNet surpasses previous CNN and Transformer baselines in PSNR/SSIM while using fewer parameters and faster inference."
  },
  {
    "Model": null,
    "Model Unique Name": "Denoise-SwinIR-Noise15",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "The paper introduces SwinIR, an image-restoration network built on the hierarchical Swin Transformer. It consists of a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a reconstruction head that merges shallow and deep features. Trained with task-specific losses, SwinIR attains superior PSNR and SSIM across six restoration benchmarks—including classical, lightweight, and real-world super-resolution, grayscale and color denoising, and compression-artifact reduction—while using up to 67 % fewer parameters than previous methods reported in the paper."
  },
  {
    "Model": null,
    "Model Unique Name": "Denoise-SwinIR-Noise25",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "The paper introduces SwinIR, an image-restoration network built on the hierarchical Swin Transformer. It consists of a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a reconstruction head that merges shallow and deep features. Trained with task-specific losses, SwinIR attains superior PSNR and SSIM across six restoration benchmarks—including classical, lightweight, and real-world super-resolution, grayscale and color denoising, and compression-artifact reduction—while using up to 67 % fewer parameters than previous methods reported in the paper."
  },
  {
    "Model": null,
    "Model Unique Name": "Denoise-SwinIR-Noise50",
    "Category": "Img2Img",
    "Detailed Category": "Denoise",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "The paper introduces SwinIR, an image-restoration network built on the hierarchical Swin Transformer. It consists of a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a reconstruction head that merges shallow and deep features. Trained with task-specific losses, SwinIR attains superior PSNR and SSIM across six restoration benchmarks—including classical, lightweight, and real-world super-resolution, grayscale and color denoising, and compression-artifact reduction—while using up to 67 % fewer parameters than previous methods reported in the paper."
  },
  {
    "Model": null,
    "Model Unique Name": "Img2Txt-HuggingFace-Salesforce-blip-image-captioning-large",
    "Category": "Img2Txt",
    "Detailed Category": "Img2Txt",
    "Dataset": "COCO",
    "Paper": "https://arxiv.org/pdf/2201.12086",
    "GitHub": "https://github.com/salesforce/BLIP",
    "HuggingFace": "https://huggingface.co/Salesforce/blip-image-captioning-large",
    "Summary": "The paper introduces BLIP, a two-part framework comprising a Multimodal Mixture of Encoder-Decoder (MED) architecture and a Captioning-and-Filtering (CapFilt) data bootstrapping strategy. MED can operate as an image encoder, an image-grounded text encoder, or an image-grounded text decoder, and is jointly pre-trained with image–text contrastive, image–text matching, and image-conditioned language-modeling losses. CapFilt first uses a fine-tuned captioner to generate synthetic captions for web images, then applies a fine-tuned filter to discard noisy original and synthetic captions, producing a cleaner training set. Experiments show state-of-the-art results on image–text retrieval, captioning, VQA, NLVR2, visual dialog, and zero-shot transfers to video-language tasks."
  },
  {
    "Model": null,
    "Model Unique Name": "ImgTxt2Txt-HuggingFace-dandelin-vilt-b32-finetuned-vqa",
    "Category": "ImgTxt2Txt",
    "Detailed Category": "ImgTxt2Txt",
    "Dataset": "GCC,SBU,VG,COCO,Flickr30k,VQAv2,NLVR2",
    "Paper": "https://arxiv.org/pdf/2102.03334",
    "GitHub": "https://github.com/dandelin/vilt",
    "HuggingFace": "https://huggingface.co/dandelin/vilt-b32-finetuned-vqa",
    "Summary": "The paper proposes ViLT, a single-stream Transformer that handles images and text with the same lightweight patch projection used for textual tokens, completely removing CNNs and object detectors. Visual inputs are split into 32 × 32 patches, linearly embedded, concatenated with word embeddings, and processed by a 12-layer ViT-based Transformer initialized from ViT-B/32 weights. Pre-training combines image–text contrastive alignment, image–text matching with word-patch alignment, and masked language modeling (with whole-word masking). ViLT attains competitive or superior results on VQAv2, NLVR2, MS-COCO, and Flickr30K while running tens of times faster and using fewer parameters than previous VLP models."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-CARN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "GitHub": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-CARN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "GitHub": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-CARN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "GitHub": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-CARN-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "GitHub": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-CARN-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "GitHub": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-CARN-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1803.08664",
    "GitHub": "https://github.com/nmhkahn/CARN-pytorch",
    "HuggingFace": null,
    "Summary": "The authors propose Cascading Residual Network (CARN) and its lightweight variant CARN-M for efficient and accurate image super-resolution. CARN introduces both local and global cascading mechanisms within a residual network to capture multi-level features and enhance information flow. CARN-M incorporates efficient residual blocks using group convolutions and recursive weight sharing to reduce computation and model size. Extensive experiments on standard benchmarks show that both models outperform or match state-of-the-art methods with significantly fewer parameters and operations, making them suitable for deployment in mobile and real-time systems."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-ESRT-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "GitHub": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Summary": "ESRT-2× couples a Lightweight CNN Backbone (LCB) that contracts the feature maps with a Lightweight Transformer Backbone (LTB) built from Efficient Transformers (ET) employing Efficient Multi-Head Attention (EMHA). A pixel-shuffle head upsamples features by a factor of two. Training on DIV2K yields PSNR/SSIM competitive with deeper CNNs at a fraction of their parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-ESRT-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "GitHub": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Summary": "Using the same LCB + LTB core as the 2× model, ESRT-3× incorporates a three-step pixel-shuffle reconstruction module that upsamples features by 3. Extensive DIV2K training and fine-tuning demonstrate that the model preserves edge sharpness better than CNN baselines of similar size. "
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-ESRT-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": null,
    "GitHub": "https://github.com/luissen/ESRT",
    "HuggingFace": null,
    "Summary": "ESRT-4× deepens the reconstruction head with an additional upsampling stage but keeps the LCB and LTB unchanged. It achieves favourable PSNR/SSIM on DIV2K and Urban100 with fewer parameters and FLOPs than SwinIR or RCAN, validating the efficiency of EMHA."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-HAN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "GitHub": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Summary": "The authors present Holistic Attention Network (HAN), which enhances single-image super-resolution by combining two complementary attention mechanisms. A Layer Attention Module adaptively weights features across residual groups, and a Channel-Spatial Attention Module jointly learns channel and spatial importance within each map. These modules, inserted into a residual channel-attention backbone, allow the network to focus on informative features at every depth and position. Trained on DIV2K, HAN outperforms previous methods across ×2, ×3, ×4, and ×8 scales without relying on perceptual or adversarial losses."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-HAN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "GitHub": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Summary": "The authors present Holistic Attention Network (HAN), which enhances single-image super-resolution by combining two complementary attention mechanisms. A Layer Attention Module adaptively weights features across residual groups, and a Channel-Spatial Attention Module jointly learns channel and spatial importance within each map. These modules, inserted into a residual channel-attention backbone, allow the network to focus on informative features at every depth and position. Trained on DIV2K, HAN outperforms previous methods across ×2, ×3, ×4, and ×8 scales without relying on perceptual or adversarial losses."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-HAN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "GitHub": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Summary": "The authors present Holistic Attention Network (HAN), which enhances single-image super-resolution by combining two complementary attention mechanisms. A Layer Attention Module adaptively weights features across residual groups, and a Channel-Spatial Attention Module jointly learns channel and spatial importance within each map. These modules, inserted into a residual channel-attention backbone, allow the network to focus on informative features at every depth and position. Trained on DIV2K, HAN outperforms previous methods across ×2, ×3, ×4, and ×8 scales without relying on perceptual or adversarial losses."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-HAN-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2008.08767",
    "GitHub": "https://github.com/wwlCape/HAN",
    "HuggingFace": null,
    "Summary": "The authors present Holistic Attention Network (HAN), which enhances single-image super-resolution by combining two complementary attention mechanisms. A Layer Attention Module adaptively weights features across residual groups, and a Channel-Spatial Attention Module jointly learns channel and spatial importance within each map. These modules, inserted into a residual channel-attention backbone, allow the network to focus on informative features at every depth and position. Trained on DIV2K, HAN outperforms previous methods across ×2, ×3, ×4, and ×8 scales without relying on perceptual or adversarial losses."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-DRN-S-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "GitHub": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Summary": "The authors propose a dual-regression scheme that trains two coupled networks: a primal model that upsamples LR to HR, and a lightweight dual model that down-samples the generated HR back to the original LR, forming a closed loop. The dual path enforces consistency—super-resolved output must downsample to the input—thus regularising the primal mapping. A unified loss combines the standard HR reconstruction term with the dual LR reconstruction term, weighted by λ. For unpaired data, the same closed loop allows the model to learn directly from LR images while still leveraging synthetic LR-HR pairs. Extensive experiments show that their Dual Regression Network (DRN) surpasses state-of-the-art CNN and Transformer SR models on ×2–×8 tasks and adapts robustly to real video frames."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-DRN-S-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "GitHub": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Summary": "The authors propose a dual-regression scheme that trains two coupled networks: a primal model that upsamples LR to HR, and a lightweight dual model that down-samples the generated HR back to the original LR, forming a closed loop. The dual path enforces consistency—super-resolved output must downsample to the input—thus regularising the primal mapping. A unified loss combines the standard HR reconstruction term with the dual LR reconstruction term, weighted by λ. For unpaired data, the same closed loop allows the model to learn directly from LR images while still leveraging synthetic LR-HR pairs. Extensive experiments show that their Dual Regression Network (DRN) surpasses state-of-the-art CNN and Transformer SR models on ×2–×8 tasks and adapts robustly to real video frames."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-DRN-L-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "GitHub": "https://github.com/guoyongcs/DRN",
    "HuggingFace": null,
    "Summary": "The authors propose a dual-regression scheme that trains two coupled networks: a primal model that upsamples LR to HR, and a lightweight dual model that down-samples the generated HR back to the original LR, forming a closed loop. The dual path enforces consistency—super-resolved output must downsample to the input—thus regularising the primal mapping. A unified loss combines the standard HR reconstruction term with the dual LR reconstruction term, weighted by λ. For unpaired data, the same closed loop allows the model to learn directly from LR images while still leveraging synthetic LR-HR pairs. Extensive experiments show that their Dual Regression Network (DRN) surpasses state-of-the-art CNN and Transformer SR models on ×2–×8 tasks and adapts robustly to real video frames."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-DRN-L-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2003.07018",
    "GitHub": "https://github.com/guoyongcs/DRNN",
    "HuggingFace": null,
    "Summary": "The authors introduce a Dual Regression Network (DRN) that pairs a conventional upsampler (primal network) with a lightweight downsampler (dual network) to form a closed loop. During training, the HR reconstruction loss drives the primal network when ground-truth HR is available, while a dual consistency loss ensures the downsampled output matches the LR input for every sample. This dual constraint regularises learning, narrows the search space, and allows the same framework to exploit both paired synthetic data and unpaired real LR imagery. Empirical results across ×2–×8 upscaling show that DRN surpasses prior CNN and Transformer baselines in PSNR/SSIM and adapts effectively to real video frames without HR references."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-RCAN-it-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "GitHub": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Summary": "The authors revisit RCAN and create RCAN-it, a minimally modified version that replaces ReLU with SiLU and is trained with modern large-batch, large-patch, cosine-annealed schedules. Extensive ablations reveal that RCAN’s main limitation is underfitting: simply extending training iterations, increasing batch size, and using a stronger optimizer improve performance, while typical regularizers hurt. With these updates, RCAN-it matches or exceeds nearly all CNN-based SR models published after RCAN on standard benchmarks and narrows the gap to recent Transformer approaches. Training efficiency is also improved—comparable accuracy is reached in one quarter of the original training time."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-RCAN-it-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "GitHub": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Summary": "The authors revisit RCAN and create RCAN-it, a minimally modified version that replaces ReLU with SiLU and is trained with modern large-batch, large-patch, cosine-annealed schedules. Extensive ablations reveal that RCAN’s main limitation is underfitting: simply extending training iterations, increasing batch size, and using a stronger optimizer improve performance, while typical regularizers hurt. With these updates, RCAN-it matches or exceeds nearly all CNN-based SR models published after RCAN on standard benchmarks and narrows the gap to recent Transformer approaches. Training efficiency is also improved—comparable accuracy is reached in one quarter of the original training time."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-RCAN-it-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2201.11279",
    "GitHub": "https://github.com/zudi-lin/rcan-it",
    "HuggingFace": null,
    "Summary": "The authors revisit RCAN and create RCAN-it, a minimally modified version that replaces ReLU with SiLU and is trained with modern large-batch, large-patch, cosine-annealed schedules. Extensive ablations reveal that RCAN’s main limitation is underfitting: simply extending training iterations, increasing batch size, and using a stronger optimizer improve performance, while typical regularizers hurt. With these updates, RCAN-it matches or exceeds nearly all CNN-based SR models published after RCAN on standard benchmarks and narrows the gap to recent Transformer approaches. Training efficiency is also improved—comparable accuracy is reached in one quarter of the original training time."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Swin2SR-Classical-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "GitHub": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Swin2SR-Classical-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "GitHub": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Swin2SR-Compressed-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "GitHub": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Swin2SR-LightWeight-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "GitHub": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Swin2SR-Real-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2209.11345",
    "GitHub": "https://github.com/mv-lab/swin2sr",
    "HuggingFace": null,
    "Summary": "Swin2SR revisits the SwinIR framework and replaces its Swin-Transformer layers with the newer SwinV2 blocks, inheriting their post-norm design, scaled-cosine attention, and log-spaced continuous positional bias. The model follows a three-stage pipeline: a shallow convolution extracts low-frequency features; a deep stack of six Residual SwinV2 Transformer Blocks models local and long-range interactions; a reconstruction head fuses shallow and deep features and upsamples with pixel shuffle. An auxiliary bicubic branch adds the network’s output to a simple interpolated image, and two extra loss terms—downsample consistency and high-frequency similarity—encourage robustness to JPEG artifacts. Trained on DIV2K (optionally with Flickr2K) and validated on multiple benchmarks, Swin2SR attains state-of-the-art PSNR/SSIM for JPEG artifact removal, classical and lightweight SR, and ranks among the top methods in the AIM 2022 compressed-image SR challenge while cutting training iterations by roughly one-third compared with SwinIR."
  },
  {
    "Model": null,
    "Model Unique Name": "ObjDet-HuggingFace-facebook-detr-resnet-50",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "COCO",
    "Paper": "https://arxiv.org/pdf/2005.12872",
    "GitHub": "https://github.com/facebookresearch/detr",
    "HuggingFace": "https://huggingface.co/facebook/detr-resnet-50",
    "Summary": "The authors present DETR, a detector that frames object detection as direct set prediction. A convolutional backbone first extracts a compact feature map; a Transformer encoder–decoder then reasons globally over the image and a fixed number of learned object queries; finally, a shared feed-forward head emits class probabilities (including a “no-object” class) and normalized box coordinates for every query. Training uses a bipartite-matching loss that pairs each ground-truth object with one unique prediction and penalizes classification and box regression errors. DETR matches or surpasses a strong Faster R-CNN baseline on the COCO benchmark, excels on large objects, and extends cleanly to panoptic segmentation by adding a lightweight mask head."
  },
  {
    "Model": null,
    "Model Unique Name": "ObjDet-HuggingFace-hustvl-yolos-small",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "ImageNet-1k",
    "Paper": null,
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/hustvl/yolos-small",
    "Summary": "YOLOS-Small adapts a 12-layer Vision Transformer (ViT-Small, 16 × 16 patches, 384 hidden dimensions) to object detection. It appends a lightweight prediction head that converts the final patch embeddings into fixed-length sets of class logits and bounding-box coordinates. Trained on ImageNet-1K for classification and then fine-tuned on detection data, YOLOS-Small demonstrates that even a modest transformer backbone can localize and classify objects without auxiliary detection machinery. Although accuracy trails large detector backbones, the model is compact, conceptually simple, and free of domain-specific heuristics.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "ObjDet-HuggingFace-valentinafeve-yolos-fashionpedia",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Object Detection",
    "Dataset": "Fashionpedia",
    "Paper": null,
    "GitHub": "https://github.com/valentinafeve/fine_tunning_YOLOS_for_fashion",
    "HuggingFace": "https://huggingface.co/valentinafeve/yolos-fashionpedia",
    "Summary": "YOLOS-Fashionpedia fine-tunes the small Vision-Transformer backbone from the original YOLOS on Fashionpedia’s annotated runway and street images. The network treats an image as a sequence of patches plus a small set of learnable detection tokens; through global self-attention it reasons about apparel context and outputs fixed-length sets of class logits and bounding-box coordinates. Training uses the Hungarian matching loss to align each ground-truth garment with exactly one prediction, encouraging direct set output. The resulting model detects diverse clothing items—from trench coats to belts—in a single pass and is released under an open licence for fashion-domain research and applications."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-DeepLabV3-MobileNet-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "GitHub": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "The model couples DeepLabV3’s atrous spatial-pyramid pooling (ASPP) head with a lightweight MobileNet-v2 backbone. Trained from scratch—or fine-tuned from ImageNet weights—on PASCAL VOC 2012, it segments 21 categories in a single forward pass while maintaining a small memory footprint."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-DeepLabV3-ResNet50-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "GitHub": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "Replacing the heavy ResNet-101 in the original DeepLabV3 with ResNet-50 cuts inference time roughly in half while preserving most of the mean Intersection-over-Union (mIoU) score on VOC 2012."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-DeepLabV3-ResNet101-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "GitHub": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "Using a high-capacity ResNet-101 backbone with dilated convolutions allows finer receptive fields and higher mIoU than the ResNet-50 or MobileNet versions, albeit with greater compute cost."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-DeepLabV3Plus-MobileNet-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "GitHub": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "DeepLabV3+ augments the ASPP head with a lightweight decoder path. Combining that decoder with MobileNet-v2 yields a fast, edge-aware model suitable for embedded use."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-DeepLabV3Plus-ResNet50-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "GitHub": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "Integrating the DeepLabV3+ decoder with a ResNet-50 backbone improves mIoU and edge fidelity relative to its V3 counterpart, making it suitable for real-time desktop applications."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-DeepLabV3Plus-ResNet101-VOC",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "VOC",
    "Paper": null,
    "GitHub": "https://github.com/VainF/DeepLabV3Plus-Pytorch",
    "HuggingFace": null,
    "Summary": "A ResNet-101 encoder, ASPP module, and decoder path together reach the highest VOC mIoU among the listed variants, though at the cost of increased compute."
  },
  {
    "Model": null,
    "Model Unique Name": "Restoration-SwinIR-Jpeg10",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR couples a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a task-specific reconstruction head. The shifted-window self-attention inside each RSTB yields both local detail handling and cross-window interaction, while a long skip connection feeds low-frequency features directly to the reconstructor. Trained on DIV2K (and optionally Flickr2K) with standard pixel or Charbonnier losses, SwinIR sets new benchmarks on classical, lightweight, and real-world super-resolution, grayscale and color denoising, and JPEG artifact removal—gaining up to 0.45 dB PSNR over previous best CNNs with up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "Restoration-SwinIR-Jpeg20",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR couples a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a task-specific reconstruction head. The shifted-window self-attention inside each RSTB yields both local detail handling and cross-window interaction, while a long skip connection feeds low-frequency features directly to the reconstructor. Trained on DIV2K (and optionally Flickr2K) with standard pixel or Charbonnier losses, SwinIR sets new benchmarks on classical, lightweight, and real-world super-resolution, grayscale and color denoising, and JPEG artifact removal—gaining up to 0.45 dB PSNR over previous best CNNs with up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "Restoration-SwinIR-Jpeg30",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR couples a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a task-specific reconstruction head. The shifted-window self-attention inside each RSTB yields both local detail handling and cross-window interaction, while a long skip connection feeds low-frequency features directly to the reconstructor. Trained on DIV2K (and optionally Flickr2K) with standard pixel or Charbonnier losses, SwinIR sets new benchmarks on classical, lightweight, and real-world super-resolution, grayscale and color denoising, and JPEG artifact removal—gaining up to 0.45 dB PSNR over previous best CNNs with up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "Restoration-SwinIR-Jpeg40",
    "Category": "Img2Img",
    "Detailed Category": "Restoration",
    "Dataset": "DIV2K,Flickr2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR couples a shallow convolutional feature extractor, a deep stack of Residual Swin Transformer Blocks (RSTBs), and a task-specific reconstruction head. The shifted-window self-attention inside each RSTB yields both local detail handling and cross-window interaction, while a long skip connection feeds low-frequency features directly to the reconstructor. Trained on DIV2K (and optionally Flickr2K) with standard pixel or Charbonnier losses, SwinIR sets new benchmarks on classical, lightweight, and real-world super-resolution, grayscale and color denoising, and JPEG artifact removal—gaining up to 0.45 dB PSNR over previous best CNNs with up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-HuggingFace-facebook-maskformer-swin-base-coco",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "ADE20k,Cityspaces,COCO,Mapillary Vistas",
    "Paper": "https://arxiv.org/pdf/2107.06278",
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/facebook/maskformer-swin-base-coco",
    "Summary": "MaskFormer converts any fully convolutional network into a mask-classification system. A pixel decoder produces per-pixel embeddings; a Transformer decoder generates a fixed set of query embeddings, each yielding one class prediction and one mask embedding. The binary masks are recovered by a dot product between per-pixel and mask embeddings, supervised with a classification loss and a focal-plus-dice mask loss under Hungarian matching. Without altering architecture or losses, the same model attains state-of-the-art scores on semantic (e.g., 55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO), particularly excelling when class vocabularies are large."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-HuggingFace-jonathandinu-face-parsing",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": "CelebAMask-HQ",
    "Paper": null,
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/jonathandinu/face-parsing",
    "Summary": "SegFormer-FaceParsing fine-tunes the SegFormer MiT-B0 backbone on the CelebAMask-HQ dataset, predicting 19 facial classes plus background. The hierarchical Transformer encoder captures global context, and the all-MLP decoder fuses multi-scale features without convolutions. With roughly 3.7 M parameters, the model runs in real time on standard GPUs and achieves high mean IoU across facial regions, accurately separating subtle boundaries such as eye shadow and inner-lip areas."
  },
  {
    "Model": null,
    "Model Unique Name": "Segmentation-HuggingFace-nvidia-segformer-b3-finetuned-ade-512-512",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Segmentation",
    "Dataset": null,
    "Paper": null,
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/nvidia/segformer-b3-finetuned-ade-512-512",
    "Summary": "SegFormer-B3 couples a MixVision Transformer (MiT-B3) encoder with an all-MLP decoder that fuses multi-scale features into dense per-pixel predictions. Fine-tuned on the ADE20K dataset at 512 × 512 resolution, the model segments 150 classes in a single forward pass, achieving strong mean IoU while maintaining moderate parameter count and fast inference. Thanks to overlapping patch embeddings and efficient local–global self-attention, it captures both fine details and global context without needing feature pyramids or dilated convolutions."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-IMDN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "GitHub": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Summary": "The authors introduce the Information Multi-Distillation Network (IMDN), built from cascaded Information Multi-Distillation Blocks (IMDBs). Each IMDB progressively splits features, keeps the informative part, and further refines the rest, then fuses all distilled features with a contrast-aware channel-attention layer. A lightweight sub-pixel upsampler produces the final high-resolution image. To extend one model to any scale, they develop an adaptive cropping strategy that divides an input into stride-aligned patches, super-resolves each patch, and stitches the results. Experiments on standard benchmarks show that IMDN matches or exceeds previous lightweight methods in PSNR/SSIM while using under one million parameters and achieving real-time speed on a single GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-IMDN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "GitHub": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Summary": "The authors introduce the Information Multi-Distillation Network (IMDN), built from cascaded Information Multi-Distillation Blocks (IMDBs). Each IMDB progressively splits features, keeps the informative part, and further refines the rest, then fuses all distilled features with a contrast-aware channel-attention layer. A lightweight sub-pixel upsampler produces the final high-resolution image. To extend one model to any scale, they develop an adaptive cropping strategy that divides an input into stride-aligned patches, super-resolves each patch, and stitches the results. Experiments on standard benchmarks show that IMDN matches or exceeds previous lightweight methods in PSNR/SSIM while using under one million parameters and achieving real-time speed on a single GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-IMDN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1909.11856v1",
    "GitHub": "https://github.com/Zheng222/IMDN",
    "HuggingFace": null,
    "Summary": "The authors introduce the Information Multi-Distillation Network (IMDN), built from cascaded Information Multi-Distillation Blocks (IMDBs). Each IMDB progressively splits features, keeps the informative part, and further refines the rest, then fuses all distilled features with a contrast-aware channel-attention layer. A lightweight sub-pixel upsampler produces the final high-resolution image. To extend one model to any scale, they develop an adaptive cropping strategy that divides an input into stride-aligned patches, super-resolves each patch, and stitches the results. Experiments on standard benchmarks show that IMDN matches or exceeds previous lightweight methods in PSNR/SSIM while using under one million parameters and achieving real-time speed on a single GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-LatticeNet-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "GitHub": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Summary": "The authors introduce the Lattice Block (LB), an economical unit that replaces a pair of residual blocks with two butterfly-style combination paths. Learnable “combination coefficients,” produced by a channel-attention mechanism, adaptively mix the two residual branches, allowing LB to mimic multiple residual-block topologies yet use roughly half the parameters. Stacking LBs and feeding their outputs to a Backward Feature Fusion Module (BFM) yields LatticeNet, a lightweight SISR network. Trained on DIV2K and evaluated on Set5, Set14, B100, and Urban100 at ×2–×4 scales, LatticeNet surpasses existing compact models such as CARN and IMDN in PSNR/SSIM while remaining under one million parameters and achieving real-time inference."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-LatticeNet-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "GitHub": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Summary": "The authors introduce the Lattice Block (LB), an economical unit that replaces a pair of residual blocks with two butterfly-style combination paths. Learnable “combination coefficients,” produced by a channel-attention mechanism, adaptively mix the two residual branches, allowing LB to mimic multiple residual-block topologies yet use roughly half the parameters. Stacking LBs and feeding their outputs to a Backward Feature Fusion Module (BFM) yields LatticeNet, a lightweight SISR network. Trained on DIV2K and evaluated on Set5, Set14, B100, and Urban100 at ×2–×4 scales, LatticeNet surpasses existing compact models such as CARN and IMDN in PSNR/SSIM while remaining under one million parameters and achieving real-time inference."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-LatticeNet-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670273.pdf",
    "GitHub": "https://github.com/ymff0592/super-resolution",
    "HuggingFace": null,
    "Summary": "The authors introduce the Lattice Block (LB), an economical unit that replaces a pair of residual blocks with two butterfly-style combination paths. Learnable “combination coefficients,” produced by a channel-attention mechanism, adaptively mix the two residual branches, allowing LB to mimic multiple residual-block topologies yet use roughly half the parameters. Stacking LBs and feeding their outputs to a Backward Feature Fusion Module (BFM) yields LatticeNet, a lightweight SISR network. Trained on DIV2K and evaluated on Set5, Set14, B100, and Urban100 at ×2–×4 scales, LatticeNet surpasses existing compact models such as CARN and IMDN in PSNR/SSIM while remaining under one million parameters and achieving real-time inference."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-RCAN-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "GitHub": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Summary": "The authors present RCAN, a 400-plus-layer network for single-image super-resolution. Its core is a Residual-in-Residual (RIR) structure that nests residual groups inside a long skip connection so low-frequency signals bypass the main pathway, easing optimisation. Within each group, Residual Channel-Attention Blocks (RCABs) use a lightweight attention module that rates each channel by global statistics and rescales feature maps accordingly. Trained on DIV2K and evaluated on five benchmarks, RCAN surpasses earlier CNNs and even larger models such as EDSR, achieving up to 0.55 dB PSNR gain while using fewer parameters and delivering real-time speed on a GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-RCAN-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "GitHub": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Summary": "The authors present RCAN, a 400-plus-layer network for single-image super-resolution. Its core is a Residual-in-Residual (RIR) structure that nests residual groups inside a long skip connection so low-frequency signals bypass the main pathway, easing optimisation. Within each group, Residual Channel-Attention Blocks (RCABs) use a lightweight attention module that rates each channel by global statistics and rescales feature maps accordingly. Trained on DIV2K and evaluated on five benchmarks, RCAN surpasses earlier CNNs and even larger models such as EDSR, achieving up to 0.55 dB PSNR gain while using fewer parameters and delivering real-time speed on a GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-RCAN-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "GitHub": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Summary": "The authors present RCAN, a 400-plus-layer network for single-image super-resolution. Its core is a Residual-in-Residual (RIR) structure that nests residual groups inside a long skip connection so low-frequency signals bypass the main pathway, easing optimisation. Within each group, Residual Channel-Attention Blocks (RCABs) use a lightweight attention module that rates each channel by global statistics and rescales feature maps accordingly. Trained on DIV2K and evaluated on five benchmarks, RCAN surpasses earlier CNNs and even larger models such as EDSR, achieving up to 0.55 dB PSNR gain while using fewer parameters and delivering real-time speed on a GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-RCAN-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/1807.02758",
    "GitHub": "https://github.com/yulunzhang/RCAN",
    "HuggingFace": null,
    "Summary": "The authors present RCAN, a 400-plus-layer network for single-image super-resolution. Its core is a Residual-in-Residual (RIR) structure that nests residual groups inside a long skip connection so low-frequency signals bypass the main pathway, easing optimisation. Within each group, Residual Channel-Attention Blocks (RCABs) use a lightweight attention module that rates each channel by global statistics and rescales feature maps accordingly. Trained on DIV2K and evaluated on five benchmarks, RCAN surpasses earlier CNNs and even larger models such as EDSR, achieving up to 0.55 dB PSNR gain while using fewer parameters and delivering real-time speed on a GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Classical-DF2K-64-M-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DF2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Classical-DIV2K-48-M-8x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-3x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Lightweight-DIV2K-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": "DIV2K",
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Real-DFO-64-M-2x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Real-DFO-64-M-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-SwinIR-Real-DFOWMFC-64-L-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2108.10257",
    "GitHub": "https://github.com/JingyunLiang/SwinIR",
    "HuggingFace": null,
    "Summary": "SwinIR is a three-stage framework: a shallow 3 × 3 convolution extracts low-frequency features, a deep stack of Residual Swin Transformer Blocks (RSTBs) models local and global interactions through shifted-window self-attention, and a reconstruction head fuses shallow and deep features to produce the restored image. The same backbone supports classical, lightweight, and real-world super-resolution as well as grayscale/color denoising and artifact removal. Trained on DIV2K (optionally with Flickr2K), SwinIR surpasses previous methods by up to 0.45 dB PSNR while using up to 67 % fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-AbsoluteReality",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "AbsoluteReality is a LoRA-style fine-tune of Stable Diffusion v1.5 that targets high-fidelity realism. Curated training images featuring varied lighting, ethnicities, and depth cues guide the model toward accurate anatomy, natural skin, and physically plausible materials. AbsoluteReality retains SD v1.5’s 512 × 512 latent structure, runs in the same pipelines, and supports negative-prompt guidance to minimise artifacts. Users report strong “out-of-the-box” realism with minimal prompt length—e.g., “portrait photo of a woman, 35 mm lens, cinematic lighting”—yielding coherent facial features, sharp textures, and neutral color grading."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-majicMix-sombre",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "majicMix-sombre is a LoRA-style fine-tune of SD v1.5 trained on a curated set of under-exposed portraits, foggy cityscapes, and chiaroscuro interiors. It preserves SD’s latent resolution (512 × 512) and sampling settings but biases the diffusion priors toward shadow-rich, high-contrast compositions. Users report that phrases such as “moody portrait, cinematic shadows” yield coherent faces, subtle film grain, and muted tones without additional prompt engineering."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-SimpleMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "SimpleMix blends several specialist SD v1.5 LoRA weights—photographic realism, soft digital-art shading, and light anime linework—into one checkpoint, balancing them through iterative weight averaging. The resulting model responds smoothly to style keywords (“cinematic photo”, “digital painting”, “anime illustration”) while keeping anatomy fidelity and color consistency."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ToonYou",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "ToonYou is a DreamBooth fine-tune of SD v1.5 trained on thousands of stylised head-and-shoulder illustrations featuring thick outlines, flat colors, and expressive eyes. The model excels at single-character close-ups and supports diverse angles while avoiding photoreal textures. Prompts like “toon style portrait of a warrior, cel-shade, thick lines” generate high-resolution PNG-friendly art ready for comics or stickers."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Any-LIIF-EDSR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": "DIV2K,CelebAHQ,Benchmark",
    "Paper": "https://arxiv.org/pdf/2012.09161",
    "GitHub": "https://github.com/yinboc/liif",
    "HuggingFace": null,
    "Summary": "The authors propose Local Implicit Image Function (LIIF), which views an image as a grid of latent codes and learns a shared multilayer-perceptron decoder that maps any continuous 2-D coordinate, together with nearby latent codes, to an RGB value. An encoder is jointly trained with LIIF through a self-supervised super-resolution task using randomly sampled up-scaling factors between ×1 and ×4. The resulting representation can be queried at extreme magnifications (up to ×30) far beyond training scales and still recover high-frequency detail. LIIF also enables learning tasks where ground-truth images have varied resolutions, outperforming methods that force all targets to a single size."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Any-LIIF-RDN",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": "DIV2K,CelebAHQ,Benchmark",
    "Paper": "https://arxiv.org/pdf/2012.09161",
    "GitHub": "https://github.com/yinboc/liif",
    "HuggingFace": null,
    "Summary": "The authors propose Local Implicit Image Function (LIIF), which views an image as a grid of latent codes and learns a shared multilayer-perceptron decoder that maps any continuous 2-D coordinate, together with nearby latent codes, to an RGB value. An encoder is jointly trained with LIIF through a self-supervised super-resolution task using randomly sampled up-scaling factors between ×1 and ×4. The resulting representation can be queried at extreme magnifications (up to ×30) far beyond training scales and still recover high-frequency detail. LIIF also enables learning tasks where ground-truth images have varied resolutions, outperforming methods that force all targets to a single size."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Any-LTE-EDSR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "GitHub": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Summary": "The authors propose the Local Texture Estimator (LTE), a module that predicts a set of dominant Fourier components for every spatial location and feeds them into an implicit decoder. An encoder (e.g., EDSR-baseline, RDN, or SwinIR without its upsampling head) extracts a latent feature map; LTE transforms a query coordinate and its neighbouring latent codes into amplitude, frequency, and phase signals; a small MLP then reconstructs the RGB value, and a bilinearly upsampled LR skip connection supplies low frequencies. Trained on DIV2K with randomly sampled scale factors between ×1 and ×4, the resulting networks outperform previous arbitrary-scale SR approaches (MetaSR, LIIF) on Set5, Set14, B100, Urban100, and DIV2K, while requiring less memory and runtime."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Any-LTE-RDN",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "GitHub": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Summary": "The authors propose the Local Texture Estimator (LTE), a module that predicts a set of dominant Fourier components for every spatial location and feeds them into an implicit decoder. An encoder (e.g., EDSR-baseline, RDN, or SwinIR without its upsampling head) extracts a latent feature map; LTE transforms a query coordinate and its neighbouring latent codes into amplitude, frequency, and phase signals; a small MLP then reconstructs the RGB value, and a bilinearly upsampled LR skip connection supplies low frequencies. Trained on DIV2K with randomly sampled scale factors between ×1 and ×4, the resulting networks outperform previous arbitrary-scale SR approaches (MetaSR, LIIF) on Set5, Set14, B100, Urban100, and DIV2K, while requiring less memory and runtime."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Any-LTE-SwinIR-LIIF",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "GitHub": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Summary": "The authors propose the Local Texture Estimator (LTE), a module that predicts a set of dominant Fourier components for every spatial location and feeds them into an implicit decoder. An encoder (e.g., EDSR-baseline, RDN, or SwinIR without its upsampling head) extracts a latent feature map; LTE transforms a query coordinate and its neighbouring latent codes into amplitude, frequency, and phase signals; a small MLP then reconstructs the RGB value, and a bilinearly upsampled LR skip connection supplies low frequencies. Trained on DIV2K with randomly sampled scale factors between ×1 and ×4, the resulting networks outperform previous arbitrary-scale SR approaches (MetaSR, LIIF) on Set5, Set14, B100, Urban100, and DIV2K, while requiring less memory and runtime."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-Any-LTE-SwinIR",
    "Category": "Img2Img",
    "Detailed Category": "SISR Any",
    "Dataset": null,
    "Paper": "https://ipl.dgist.ac.kr/LTE_cvpr.pdf",
    "GitHub": "https://github.com/jaewon-lee-b/lte",
    "HuggingFace": null,
    "Summary": "The authors propose the Local Texture Estimator (LTE), a module that predicts a set of dominant Fourier components for every spatial location and feeds them into an implicit decoder. An encoder (e.g., EDSR-baseline, RDN, or SwinIR without its upsampling head) extracts a latent feature map; LTE transforms a query coordinate and its neighbouring latent codes into amplitude, frequency, and phase signals; a small MLP then reconstructs the RGB value, and a bilinearly upsampled LR skip connection supplies low frequencies. Trained on DIV2K with randomly sampled scale factors between ×1 and ×4, the resulting networks outperform previous arbitrary-scale SR approaches (MetaSR, LIIF) on Set5, Set14, B100, Urban100, and DIV2K, while requiring less memory and runtime."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-1",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Version 1.1 is the first general-release Stable Diffusion checkpoint. A variational auto-encoder compresses 512 × 512 RGB images to a latent space; a UNet denoiser, conditioned on CLIP-ViT-L/14 textual embeddings, iteratively refines latent noise into an image representation; the decoder converts the final latent back to RGB. Trained on hundreds of millions of LAION-filtered captioned images, the model produces diverse, photorealistic or illustrative outputs from short prompts on a single consumer GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-2",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Version 1.2 continues training v1.1 with an expanded, quality-filtered image set and improved caption pre-processing. Aesthetic-score filtering and language-detector pruning raise the proportion of high-grade training pairs. The same architecture and latent resolution are retained, enabling seamless replacement in existing pipelines. Users observe crisper edges, fewer duplicated limbs, and tighter adherence to negative prompts compared with v1.1."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-4",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Version 1.4 restarts training from scratch with a two-stage curriculum: (1) 900 k steps on 2.3 B captioned images filtered for resolution, watermark absence, and language; (2) 100 k high-resolution refinement steps with aggressive caption de-duplication. The resulting checkpoint delivers clearer eyes, better composition, and fewer artifacts in complex scenes. It became the de-facto “stable-diffusion-v1-4.ckpt” used by most community UIs.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-5",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "v1.5 performs an additional 200 k training steps on a hand-curated, “aesthetic-score ≥ 6” subset plus targeted face datasets. Low-resolution noise augmentation and slightly stronger unconditional dropout improve negative-prompt responsiveness. The checkpoint inherits full compatibility with prior v1 models, runs at identical speed, and shows noticeably sharper small objects, cleaner text, and more accurate anatomy. It is the recommended base for photoreal LoRA and DreamBooth training."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ArteYou",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "ArteYou is a fine-tuned SD v1.5 checkpoint focused on rich brush textures, bold color blocking, and story-book lighting. Short prompts such as “vivid oil-paint fantasy landscape” yield coherent compositions with visible stroke patterns and balanced contrast, reducing reliance on negative-prompt tricks.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Artius_v1.5",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Artius v1.5 refines SD v1.5 on carefully lit portrait and landscape frames inspired by high-end cinema. Prompts like “cinematic 50 mm portrait, moody rim-light” return sharp eyes, correct skin tone, and gentle teal-and-orange hues, reducing the need for elaborate prompt engineering."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-CCDDA_ArtStyle",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "CC DDA ArtStyle merges SD v1.5 with a dataset of experimental artworks—hand-inked sketches, analog glitches, and paper collages. Prompts such as “esoteric ink collage, asymmetrical composition” produce layered, grainy textures, unexpected color overlaps, and controlled chaos suitable for album covers or poster art.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-colorful",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "colorful is a fine-tuned v1.5 checkpoint that emphasises high-chroma hues, bold contrast, and smooth gradient transitions. With short prompts such as “neon cityscape at dusk,” the model delivers punchy reds, luminous cyans, and clean highlight–shadow separation while minimising colour banding."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ConsistentFactor",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "ConsistentFactor refines v1.5 on repeat-appearance data—photo bursts and multi-view studio shoots—so it learns stronger correlations between facial structure, hairstyle, and apparel. After a single reference prompt (or an embedded trigger token), subsequent prompts yield matching characters with high structural consistency, reducing the need for external face-reference tools."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-CyberRealistic",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "CyberRealistic mixes portrait photography, high-end fashion editorials, and sci-fi concept renders. Prompts like “cyber-realistic half-body portrait, soft rim light, futuristic jacket” yield lifelike skin, believable fabric micro-details, and restrained teal-and-magenta accents—bridging realism with sci-fi flair."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-DreamShaper",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "DreamShaper fuses several specialised v1.5 LoRA checkpoints—realistic portraits, light anime, and ethereal fantasy art—via staged weight averaging, followed by a short stabilisation fine-tune. The model responds to style cues like “dreamshaper realistic,” “dreamshaper anime,” or “dreamshaper fantasy,” adjusting brushwork, line thickness, and colour vibrancy without losing anatomical correctness."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-epiCRealism_newEra",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "The checkpoint is a DreamBooth continuation of SD v1.5 on thousands of DSLR and mirror-less portraits plus 8-K product shots. It excels at fine pores, accurate eye reflections, and believable depth of field while keeping anatomy stable. Prompts such as “studio portrait, 85 mm lens, epic realism” produce magazine-quality imagery with minimal negative prompting."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-GhostMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "GhostMix blends several LoRA checkpoints—cinematic realism, pastel painting, light anime—then stabilises the mix with a short DreamBooth run. Prompts like “ethereal portrait, ghostmix style” yield velvety skin, softened edges, and gentle film-grain bloom, suitable for fantasy covers or romantic scenes."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-henmixReal",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "The model merges a photoreal LoRA with a semi-illustrative LoRA, then finetunes on mixed portraiture. It responds to nuances in prompts, smoothly sliding from realistic to lightly stylised without losing detail."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ICBINP",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Fine-tuned on high-resolution, studio-lit imagery, ICBINP produces strikingly lifelike faces, fabrics, and metals. Prompts like “product shot, icbinp style, 50 mm, f-1.8” yield catalogue-ready visuals with minimal artifacts."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-IDSM",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "IDSM is trained on art featuring mirrored faces, glitch splits, and colour-inverted halves. Prompts such as “idsm style, dual persona, half neon, half charcoal” generate symmetric or bifurcated compositions with high stylistic coherence."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-ImpressionismOil",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "ImpressionismOil is a DreamBooth-style continuation of SD v1.5 trained on high-resolution photographs of Monet, Renoir, Sisley, and modern oil studies. The model delivers soft edges, visible impasto, and pastel palettes from short prompts such as “impressionist oil landscape at sunrise”."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-LemonTeaMix",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "LemonTeaMix blends three specialist LoRAs (semi-real anime, oil-paint portrait, atmospheric concept art) and stabilises the merge with a brief DreamBooth run. Prompts like “lemonteamix, vivid fantasy heroine” yield clean lines, layered brush strokes, and a gentle 2.5-D parallax feel."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-majicMix-realistic",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "majicMix-realistic is a fine-tuned v1.5 checkpoint focused on balanced lighting, neutral colour grading, and micro-detail preservation. Simple prompts such as “majicmix realistic portrait, natural light” return magazine-grade faces, crisp hair strands, and believable backgrounds with minimal artifacts."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-NextPhoto",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "NextPhoto is a DreamBooth continuation of SD v1.5 trained on thousands of daylight portraits, lifestyle scenes, and travel snaps. Prompts such as “nextphoto candid street portrait, 50 mm” yield lifelike skin texture, gentle bokeh, and accurate ambient light without extensive negative prompting."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-PirsusEpicRealism",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Trained on a balanced mix of studio portraits, period costume photography, and high-budget film stills, this model yields striking depth, controlled highlights, and precise anatomy. Prompts like “epic realism medium-shot, rim-light, golden hour” return magazine-cover quality with minimal artifacts."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-QGO-PromptingReal",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "This checkpoint blends lightly fine-tuned realism weights with the base model, then undergoes a “prompt perturbation” training phase where captions are randomly shuffled, lengthened, or truncated. The result is a model that keeps facial accuracy, natural colours, and clean backgrounds even when prompts are casual or unordered."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Realisian",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Realisian is a DreamBooth continuation of SD v1.5 trained on curated 6 K DSLR portraits and lifestyle scenes. It preserves anatomy, adds fine pores and fabric weave, and suppresses HDR-style halos. Prompts such as “realisian natural-light portrait, 85 mm lens” produce lifelike images with believable depth of field and minimal artefacts."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-Reliberate",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "Reliberate starts from v1.5, receives a light realism fine-tune, then undergoes a “prompt-shuffle” phase in which caption tokens are randomly permuted, truncated, or expanded. The model learns to map diverse linguistic patterns to consistent visual output. Users can write relaxed prompts—“woman wearing red coat snowy forest”—and obtain sharp, coherent scenes without special keywords."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-StableDiffusionV1-RunDiffusionFX",
    "Category": "Txt2Img",
    "Detailed Category": "Txt2Img",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/Kameronski/stable-diffusion-1.5",
    "HuggingFace": null,
    "Summary": "The model is a DreamBooth continuation of v1.5 on professionally lit fashion, automotive and still-life photographs. It enhances specular highlights, deep shadows and micro-contrast. Prompts like “rundiffusion fx, studio car shot, rim-light” yield crisp edges, realistic reflections and controlled bokeh with minimal colour banding.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Txt-HuggingFace-facebook-bart-large-cnn",
    "Category": "Txt2Txt",
    "Detailed Category": "Txt2Txt",
    "Dataset": "CNN Daily Mail",
    "Paper": "https://arxiv.org/pdf/1910.13461",
    "GitHub": "https://github.com/facebookresearch/fairseq/tree/main/examples/bart",
    "HuggingFace": "https://huggingface.co/facebook/bart-large-cnn",
    "Summary": "BART is a transformer encoder–decoder trained as a denoising auto-encoder: text is corrupted with arbitrary noising schemes, and the model learns to reconstruct the original. Its design unifies BERT-style bidirectional encoding and GPT-style autoregressive decoding within one framework. Experiments show BART equals RoBERTa on GLUE and SQuAD while setting new records on summarization, dialogue, and abstractive QA, and even boosts machine-translation quality when used as a target-side language model. Ablations confirm that span infilling plus sentence shuffling provide the strongest pre-training signal.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Txt-HuggingFace-microsoft-Promptist",
    "Category": "Txt2Txt",
    "Detailed Category": "Txt2Txt",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/microsoft/LMOps/tree/main/promptist",
    "HuggingFace": "https://huggingface.co/microsoft/Promptist",
    "Summary": "Promptist is a lightweight, instruction-tuned language model that converts ordinary user descriptions into detailed prompts tailored for diffusion-based image generators. Fine-tuned from FLAN-T5-XL on approximately 300 K curated prompt pairs, the model learns to inject artist references, camera settings, lighting cues, aspect ratios, and negative keywords while preserving the user’s core intent. Offline evaluations and user studies show that images produced with Promptist-enhanced prompts score markedly higher in realism, relevance, and aesthetic appeal than those from raw inputs. The model is released under the MIT license and runs in under 2 GB of VRAM, making it easy to embed in existing creative pipelines."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Voice-HuggingFace-espnet-fastspeech2_conformer_with_hifigan",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/espnet/fastspeech2_conformer_with_hifigan",
    "Summary": "The pipeline couples a non-autoregressive FastSpeech 2 acoustic model—enhanced with Conformer blocks for richer local–global context—and a parallel HiFi-GAN vocoder. Alignment is obtained from an external CTC/ASR model, enabling the system to predict mel-spectrograms in one shot and convert them to 24-kHz waveform audio in real time."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Img-HuggingFace-prompthero-openjourney-v4",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/prompthero/openjourney-v4",
    "Summary": "OpenJourney v4 is a fine-tuned SD v1.5 checkpoint trained on high-ranking Midjourney community images. It delivers moody lighting, dramatic compositions, and painterly brushwork from concise prompts like “openjourney cyberpunk streetscape”."
  },
  {
    "Model": null,
    "Model Unique Name": "Txt2Voice-HuggingFace-suno-bark",
    "Category": "Txt2Snd",
    "Detailed Category": "Txt2Voice",
    "Dataset": null,
    "Paper": null,
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/suno/bark",
    "Summary": "Bark is a GPT-style audio language model trained to jointly generate tokens representing speech, non-speech vocalisations, and background music. It supports zero-shot voice cloning, multilingual synthesis, and emotion control from plain text plus optional reference audio."
  },
  {
    "Model": null,
    "Model Unique Name": "Voice2Txt-nvidia-parakeet-tdt-1.1b",
    "Category": "Snd2Txt",
    "Detailed Category": "Voice2Txt",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/NVIDIA/NeMo",
    "HuggingFace": "https://huggingface.co/nvidia/parakeet-tdt-1.1b",
    "Summary": "Parakeet TDT is a 1.1-billion-parameter Conformer-Transducer optimized for real-time transcription across accents and noisy conditions. It integrates Neural Residual Vector Quantization bottlenecks for compression and domain adversarial training for noise robustness, achieving near-LibriSpeech performance with < 300 ms end-pointer latency."
  },
  {
    "Model": null,
    "Model Unique Name": "WeatherRemoval-CLAIO-DeHaze",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "GitHub": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Summary": "The model is one component of the CL All-In-One framework and is trained exclusively for de-hazing. It learns to map hazy inputs from the OTS set to their haze-free references and is evaluated on Rain100H and Snow100K for cross-weather generalisation."
  },
  {
    "Model": null,
    "Model Unique Name": "WeatherRemoval-CLAIO-DeRain",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "GitHub": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Summary": "As part of the CL All-In-One suite, this model is trained on the Rain100H subset and validated on OTS and Snow100K to test generalisability. It targets both heavy streak removal and detail sharpening in a one-shot feed-forward pass."
  },
  {
    "Model": null,
    "Model Unique Name": "WeatherRemoval-CLAIO-DeSnow",
    "Category": "Img2Img",
    "Detailed Category": "Weather Removal",
    "Dataset": "OTS,Rain100H,Snow100K",
    "Paper": null,
    "GitHub": "https://github.com/xiaojihh/cl_all-in-one",
    "HuggingFace": null,
    "Summary": "This variant of CL All-In-One is trained on the Snow100K dataset and cross-tested on OTS and Rain100H. It aims to clear both translucent flake overlays and dense snow blobs in one inference step."
  },
  {
    "Model": null,
    "Model Unique Name": "Inpainting-CTSDG-CelebA",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "CelebA",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "GitHub": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Summary": "The authors present a two-stream generator in which texture-constrained structure reconstruction and structure-constrained texture synthesis are performed jointly. Each stream has its own encoder–decoder path, but the decoders borrow features from the opposite encoder, creating bidirectional guidance. A Bi-directional Gated Feature Fusion (Bi-GFF) module further mixes the decoded features, while a Contextual Feature Aggregation (CFA) module refines long-range dependencies through multi-scale self-patch matching. A two-branch discriminator separately evaluates texture realism and structural plausibility. Trained on CelebA, Paris StreetView, and Places2 with irregular masks, the model achieves lower LPIPS and higher PSNR/SSIM than previous methods and produces visually sharper edges with coherent textures."
  },
  {
    "Model": null,
    "Model Unique Name": "Inpainting-CTSDG-Paris",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Paris StreetView",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "GitHub": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Summary": "The authors present a two-stream generator in which texture-constrained structure reconstruction and structure-constrained texture synthesis are performed jointly. Each stream has its own encoder–decoder path, but the decoders borrow features from the opposite encoder, creating bidirectional guidance. A Bi-directional Gated Feature Fusion (Bi-GFF) module further mixes the decoded features, while a Contextual Feature Aggregation (CFA) module refines long-range dependencies through multi-scale self-patch matching. A two-branch discriminator separately evaluates texture realism and structural plausibility. Trained on CelebA, Paris StreetView, and Places2 with irregular masks, the model achieves lower LPIPS and higher PSNR/SSIM than previous methods and produces visually sharper edges with coherent textures."
  },
  {
    "Model": null,
    "Model Unique Name": "Inpainting-CTSDG-Places2",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Places2",
    "Paper": "https://arxiv.org/pdf/2108.09760",
    "GitHub": "https://github.com/xiefan-guo/ctsdg",
    "HuggingFace": null,
    "Summary": "The authors present a two-stream generator in which texture-constrained structure reconstruction and structure-constrained texture synthesis are performed jointly. Each stream has its own encoder–decoder path, but the decoders borrow features from the opposite encoder, creating bidirectional guidance. A Bi-directional Gated Feature Fusion (Bi-GFF) module further mixes the decoded features, while a Contextual Feature Aggregation (CFA) module refines long-range dependencies through multi-scale self-patch matching. A two-branch discriminator separately evaluates texture realism and structural plausibility. Trained on CelebA, Paris StreetView, and Places2 with irregular masks, the model achieves lower LPIPS and higher PSNR/SSIM than previous methods and produces visually sharper edges with coherent textures."
  },
  {
    "Model": null,
    "Model Unique Name": "Inpainting-MISF-CelebA",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "CelebA",
    "Paper": "https://arxiv.org/pdf/2203.06304",
    "GitHub": "https://github.com/tsingqguo/misf",
    "HuggingFace": null,
    "Summary": "MISF introduces a two-branch architecture in which a Kernel Prediction Branch (KPB) predicts spatially adaptive filtering kernels and a Semantic & Image Filtering Branch (SIFB) applies those kernels at both feature and pixel levels. The branches are interactively linked: SIFB’s multi-level features condition KPB, and KPB’s kernels guide SIFB’s filtering, enabling mutual refinement of structure and texture. A Bi-directional Gated Feature Fusion module merges the two streams, and a Contextual Feature Aggregation block propagates non-local information with multi-scale patch matching. Trained on CelebA, Places2 and Dunhuang with irregular masks, MISF surpasses state-of-the-art baselines in PSNR, SSIM, L1 and LPIPS, particularly under large missing regions."
  },
  {
    "Model": null,
    "Model Unique Name": "Inpainting-MISF-Places2",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": "Places2",
    "Paper": "https://arxiv.org/pdf/2203.06304",
    "GitHub": "https://github.com/tsingqguo/misf",
    "HuggingFace": null,
    "Summary": "MISF introduces a two-branch architecture in which a Kernel Prediction Branch (KPB) predicts spatially adaptive filtering kernels and a Semantic & Image Filtering Branch (SIFB) applies those kernels at both feature and pixel levels. The branches are interactively linked: SIFB’s multi-level features condition KPB, and KPB’s kernels guide SIFB’s filtering, enabling mutual refinement of structure and texture. A Bi-directional Gated Feature Fusion module merges the two streams, and a Contextual Feature Aggregation block propagates non-local information with multi-scale patch matching. Trained on CelebA, Places2 and Dunhuang with irregular masks, MISF surpasses state-of-the-art baselines in PSNR, SSIM, L1 and LPIPS, particularly under large missing regions."
  },
  {
    "Model": null,
    "Model Unique Name": "Inpainting-ResShift-Face",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "GitHub": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift constructs a Markov chain that moves directly between a high-resolution (HR) image and its low-resolution (LR) counterpart by shifting their residual rather than diffusing from Gaussian noise. A learnable UNet (trained in the latent space of a VQ-GAN) predicts the clean HR image from each intermediate state. A flexible noise schedule controls both the residual-shifting speed and per-step noise strength, letting the user balance fidelity and perceptual realism. With only 15 sampling steps, ResShift matches or surpasses state-of-the-art SR methods—including latent-diffusion models sped up by DDIM—on synthetic ImageNet and several real-world datasets."
  },
  {
    "Model": null,
    "Model Unique Name": "Inpainting-ResShift-ImageNet",
    "Category": "ImgMsk2Img",
    "Detailed Category": "Inpainting",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "GitHub": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift constructs a Markov chain that moves directly between a high-resolution (HR) image and its low-resolution (LR) counterpart by shifting their residual rather than diffusing from Gaussian noise. A learnable UNet (trained in the latent space of a VQ-GAN) predicts the clean HR image from each intermediate state. A flexible noise schedule controls both the residual-shifting speed and per-step noise strength, letting the user balance fidelity and perceptual realism. With only 15 sampling steps, ResShift matches or surpasses state-of-the-art SR methods—including latent-diffusion models sped up by DDIM—on synthetic ImageNet and several real-world datasets."
  },
  {
    "Model": null,
    "Model Unique Name": "ImgTxt2Img-HuggingFace-alaa-lab-InstructCV",
    "Category": "ImgTxt2Img",
    "Detailed Category": "ImgTxt2Img",
    "Dataset": "NYUV2,MS-COCO,ADE20k,Oxford-IIIT,SUNRGBD,Pascal VOC2012",
    "Paper": "https://arxiv.org/pdf/2310.00390",
    "GitHub": "https://github.com/AlaaLab/InstructCV",
    "HuggingFace": "https://huggingface.co/alaa-lab/InstructCV",
    "Summary": "The authors cast multiple vision tasks as text-conditioned image-generation problems where the input image and a task instruction guide the model to output a visually encoded solution. They pool standard datasets, build a multi-modal, multi-task corpus of image pairs, and use a large language model to paraphrase prompt templates, yielding varied instructions. Using the InstructPix2Pix framework, they instruction-tune a Stable Diffusion checkpoint so it learns to produce task outputs instead of purely generative imagery. Experiments on ADE20K, MS-COCO, NYUv2, and Oxford-IIIT Pets show that the resulting model, InstructCV, achieves performance on par with specialist and other generalist systems while generalizing to unseen datasets, categories, and user-written prompts."
  },
  {
    "Model": null,
    "Model Unique Name": "ImgTxt2Img-HuggingFace-timbrooks-instruct-pix2pix",
    "Category": "ImgTxt2Img",
    "Detailed Category": "ImgTxt2Img",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2211.09800",
    "GitHub": "https://github.com/timothybrooks/instruct-pix2pix",
    "HuggingFace": "https://huggingface.co/timbrooks/instruct-pix2pix",
    "Summary": "The authors create a large paired dataset by combining GPT-3 and Stable Diffusion: GPT-3 rewrites LAION captions into editing instructions and corresponding “after-edit” captions, while Stable Diffusion with Prompt-to-Prompt generates before/after image pairs that match those captions. A latent diffusion model initialized from Stable Diffusion is then fine-tuned on these synthetic triplets so that, given an image and an instruction, it denoises toward the desired edit. Classifier-free guidance is extended to balance faithfulness to the input image and to the instruction. Trained on roughly 450 k examples at 256×256, the resulting model generalizes zero-shot to real photos and user-written instructions, producing diverse edits in seconds."
  },
  {
    "Model": null,
    "Model Unique Name": "HDR-DeepHDRR",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://people.engr.tamu.edu/nimak/Papers/SIGGRAPH2020_HDR/files/SIGGRAPH2020Final.pdf",
    "GitHub": "https://github.com/marcelsan/Deep-HdrReconstruction",
    "HuggingFace": null,
    "Summary": "The authors propose a U-Net–style CNN that (1) masks intermediate feature maps so activations from saturated areas are down-weighted, and (2) is trained with a VGG-based perceptual loss adapted to HDR values. To overcome limited HDR training data, the network is first pretrained on large-scale inpainting, then fine-tuned for HDR using simulated LDR–HDR pairs; a patch-sampling strategy focuses this second stage on challenging textured regions. Quantitative tests on synthetic images and qualitative comparisons on real photographs show lower MSE, higher HDR-VDP-2 scores, and fewer artefacts than previous state-of-the-art approaches."
  },
  {
    "Model": null,
    "Model Unique Name": "HDR-FHDR-I1",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1912.11463",
    "GitHub": "https://github.com/mukulkhanna/FHDR",
    "HuggingFace": null,
    "Summary": "FHDR adopts a feedback network in which the output of a dense feedback block is recurrently fed back to guide low-level features, creating a virtually deeper model without increasing physical depth. Running for four iterations, the network produces an HDR prediction at each step, improving quality progressively. Feature masking and a global residual skip pass low-level LDR information directly to the reconstruction head, while a mixed L1 + perceptual loss on µ-law–-tonemapped HDR values sharpens textures. On City Scene and a curated high-resolution HDR dataset, FHDR yields higher PSNR, SSIM, and HDR-VDP-2 scores than prior CNN and inverse-tonemapping baselines, despite using fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "HDR-FHDR-I2",
    "Category": "Img2Img",
    "Detailed Category": "HDR",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1912.11463",
    "GitHub": "https://github.com/mukulkhanna/FHDR",
    "HuggingFace": null,
    "Summary": "FHDR adopts a feedback network in which the output of a dense feedback block is recurrently fed back to guide low-level features, creating a virtually deeper model without increasing physical depth. Running for four iterations, the network produces an HDR prediction at each step, improving quality progressively. Feature masking and a global residual skip pass low-level LDR information directly to the reconstruction head, while a mixed L1 + perceptual loss on µ-law–-tonemapped HDR values sharpens textures. On City Scene and a curated high-resolution HDR dataset, FHDR yields higher PSNR, SSIM, and HDR-VDP-2 scores than prior CNN and inverse-tonemapping baselines, despite using fewer parameters."
  },
  {
    "Model": null,
    "Model Unique Name": "FaceReplacement-ResShift",
    "Category": "Img2Img",
    "Detailed Category": "Face Replacement",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "GitHub": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift links the low- and high-resolution images through a short Markov chain that gradually shifts their residual instead of diffusing from Gaussian noise. Starting near the LR image, only 15 sampling steps suffice to reach the HR target. A learnable UNet operating in latent space predicts the clean HR image at each step. A flexible noise schedule, parameterised by a residual-shift rate and a noise-strength factor, lets users trade fidelity for realism. On synthetic ImageNet and multiple real-world datasets, ResShift matches or surpasses state-of-the-art GAN and diffusion baselines while cutting inference time by roughly 4× relative to latent diffusion accelerated by DDIM."
  },
  {
    "Model": null,
    "Model Unique Name": "Enhancement-low-light-img-enhancer",
    "Category": "Img2Img",
    "Detailed Category": "Low Light Enhancement",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/dblasko/low-light-event-img-enhancer",
    "HuggingFace": "https://huggingface.co/dblasko/mirnet-low-light-img-enhancement",
    "Summary": "This model adapts the original MIRNet architecture—designed for general image restoration—to the specific task of low-light enhancement. The network contains a multi-scale residual encoder–decoder in which information is exchanged across three spatial resolutions via attention-guided feature fusion. Trained on paired low/normal-exposure images, the model learns to suppress noise, boost luminance, and correct color casts simultaneously. A publicly released checkpoint (≈38 M parameters) runs at full resolution on commodity GPUs and substantially improves PSNR, SSIM, and NIQE scores over classic histogram equalization or Retinex-based methods."
  },
  {
    "Model": null,
    "Model Unique Name": "Harmonization-INR-RAW-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "GitHub": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "Harmonization-INR-RAW-iHarmony4",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "iHarmony4",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "GitHub": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "Harmonization-INR-Res256-iHarmony4",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "iHarmony4",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "GitHub": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "Harmonization-INR-Res1024-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "GitHub": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "Harmonization-INR-Res2048-HAdobe5K",
    "Category": "Img2Img",
    "Detailed Category": "Harmonization",
    "Dataset": "HAdobe5k",
    "Paper": "https://arxiv.org/pdf/2303.01681",
    "GitHub": "https://github.com/WindVChen/INR-Harmonization",
    "HuggingFace": null,
    "Summary": "The authors propose HINet, the first dense harmonization framework built on implicit neural representation (INR). A CNN encoder predicts weights for a decoder composed of coordinate-conditioned MLPs that map (x, y) positions, mask values, and low-res RGB hints to harmonized RGB outputs. Inspired by Retinex theory, decoder MLPs are split into content (local) and appearance (global) branches. A three-level Low-Resolution Image Prior (LRIP) feeds progressively finer coordinate batches, preventing boundary discontinuities while reducing memory. Random Step Crop (RSC) training and tiled inference let the same model handle resolutions from 256² up to 6 K with fixed VRAM. An optional 3-D LUT head offers user-controllable color tweaks. HINet sets new state-of-the-art PSNR/SSIM on iHarmony4 and HAdobe5K at both 256² and 2048², and is the first pixel-to-pixel method to process full-resolution iHarmony4 images on a single GPU."
  },
  {
    "Model": null,
    "Model Unique Name": "NST-fast-neural-style-candy",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Summary": "Each listed checkpoint is a separate feed-forward transform network trained to reproduce one reference painting’s style while preserving the content of any input photograph. Once trained, the model runs in ∼20 ms for a 512 px image on a modern GPU and below 100 ms on many smartphones, enabling live video stylisation. The four provided styles correspond to:\n\t•\tCandy – a bright, saturated candy-wrapper painting (Sato)."
  },
  {
    "Model": null,
    "Model Unique Name": "NST-fast-neural-style-mosaic",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Summary": "Each listed checkpoint is a separate feed-forward transform network trained to reproduce one reference painting’s style while preserving the content of any input photograph. Once trained, the model runs in ∼20 ms for a 512 px image on a modern GPU and below 100 ms on many smartphones, enabling live video stylisation. The four provided styles correspond to:\n\t•\tMosaic – a Cubist, tile-like Paul Klee mosaic."
  },
  {
    "Model": null,
    "Model Unique Name": "NST-fast-neural-style-rain-princess",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Summary": "Each listed checkpoint is a separate feed-forward transform network trained to reproduce one reference painting’s style while preserving the content of any input photograph. Once trained, the model runs in ∼20 ms for a 512 px image on a modern GPU and below 100 ms on many smartphones, enabling live video stylisation. The four provided styles correspond to:\n\t•\tRain Princess – Leonid Afremov’s palette-knife “Rain Princess”."
  },
  {
    "Model": null,
    "Model Unique Name": "NST-fast-neural-style-udnie",
    "Category": "Img2Img",
    "Detailed Category": "NST",
    "Dataset": null,
    "Paper": null,
    "GitHub": "https://github.com/rrmina/fast-neural-style-pytorch",
    "HuggingFace": null,
    "Summary": "Each listed checkpoint is a separate feed-forward transform network trained to reproduce one reference painting’s style while preserving the content of any input photograph. Once trained, the model runs in ∼20 ms for a 512 px image on a modern GPU and below 100 ms on many smartphones, enabling live video stylisation. The four provided styles correspond to:\n\t•\tUdnie – Francis Picabia’s abstract “Udnie (Young American Girl)”."
  },
  {
    "Model": null,
    "Model Unique Name": "PoseEstimation-OpenPose",
    "Category": "Img2ImgTxt",
    "Detailed Category": "Pose Estimation",
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/1812.08008",
    "GitHub": "https://github.com/CMU-Perceptual-Computing-Lab/openpose",
    "HuggingFace": null,
    "Summary": "The authors introduce a bottom-up framework that first predicts body-part locations and then groups them into individual skeletons. A fully convolutional network outputs two sets of heatmaps: (1) confidence maps for part locations and (2) Part Affinity Fields (PAFs) that encode both position and orientation of limbs. A greedy parsing algorithm uses PAF scores to assemble parts into full-body poses for all people simultaneously. Because the network processes the image only once and parsing is lightweight, the system runs in real time on a single GPU while maintaining high accuracy on standard benchmarks.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-ResShift-BICSR-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "GitHub": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift shortens the diffusion chain to ∼15 steps by shifting the LR–HR residual instead of starting from Gaussian noise. A learned reverse kernel progressively removes scaled residual noise in latent space, enabling ×4 SR that runs 4× faster than latent-diffusion baselines while matching or surpassing their visual metrics. Separate checkpoints are provided for bicubic (BICSR) and three real-world RealSR variants (v1, v2, v3)."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-ResShift-RealSR-v1-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "GitHub": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift shortens the diffusion chain to ∼15 steps by shifting the LR–HR residual instead of starting from Gaussian noise. A learned reverse kernel progressively removes scaled residual noise in latent space, enabling ×4 SR that runs 4× faster than latent-diffusion baselines while matching or surpassing their visual metrics. Separate checkpoints are provided for bicubic (BICSR) and three real-world RealSR variants (v1, v2, v3)."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-ResShift-RealSR-v2-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "GitHub": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift shortens the diffusion chain to ∼15 steps by shifting the LR–HR residual instead of starting from Gaussian noise. A learned reverse kernel progressively removes scaled residual noise in latent space, enabling ×4 SR that runs 4× faster than latent-diffusion baselines while matching or surpassing their visual metrics. Separate checkpoints are provided for bicubic (BICSR) and three real-world RealSR variants (v1, v2, v3)."
  },
  {
    "Model": null,
    "Model Unique Name": "SISR-ResShift-RealSR-v3-4x",
    "Category": "Img2Img",
    "Detailed Category": "SISR",
    "Dataset": null,
    "Paper": "https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac2eac5098dba08208807b65c5851cc-Paper-Conference.pdf",
    "GitHub": "https://github.com/zsyOAOA/ResShift",
    "HuggingFace": null,
    "Summary": "ResShift shortens the diffusion chain to ∼15 steps by shifting the LR–HR residual instead of starting from Gaussian noise. A learned reverse kernel progressively removes scaled residual noise in latent space, enabling ×4 SR that runs 4× faster than latent-diffusion baselines while matching or surpassing their visual metrics. Separate checkpoints are provided for bicubic (BICSR) and three real-world RealSR variants (v1, v2, v3)."
  },
  {
    "Model": null,
    "Model Unique Name": "SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation",
    "Category": "GNN",
    "Detailed Category": null,
    "Dataset": "Amazon-Book, Gowalla, MovieLens, Yelp",
    "Paper": "https://arxiv.org/pdf/2405.20878",
    "GitHub": "https://github.com/HKUDS/SelfGNN",
    "HuggingFace": null,
    "Summary": "SelfGNN is a self-supervised graph neural network designed for sequential recommendation. It builds short-term collaborative graphs to capture local user-item interactions, integrates multi-level long-term sequence modeling using attention and GRU, and applies a personalized self-augmentation mechanism to filter out noisy user behaviors. The method significantly outperforms existing baselines on multiple real-world datasets, showing both improved accuracy and robustness."
  },
  {
    "Model": null,
    "Model Unique Name": "AnimateDiff-Lightning: Cross-Model Diffusion Distillation",
    "Category": "Text-to-Video",
    "Detailed Category": null,
    "Dataset": "WebVid-10M, Pandas-70M",
    "Paper": "https://arxiv.org/pdf/2403.12706",
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/ByteDance/AnimateDiff-Lightning",
    "Summary": "AnimateDiff‑Lightning is a lightweight, few‑step video-generation model optimized via a cross-model distillation framework. It takes a robust teacher model (AnimateDiff) and distills it into a streamlined student architecture capable of producing high-quality video sequences in just 1–4 inference steps. A key innovation is the cross-model distillation mechanism, where a shared motion module is trained jointly across multiple base diffusion models to ensure consistent style and dynamics. This approach achieves performance competitive with state-of-the-art models like AnimateLCM, while drastically reducing computational cost and inference time. The implementation is released for the research community."
  },
  {
    "Model": null,
    "Model Unique Name": "AnimateLCM: Computation-Efficient Personalized Style Video Generation without Personalized Video Data",
    "Category": "Text-to-Video",
    "Detailed Category": null,
    "Dataset": "UCF101",
    "Paper": "https://arxiv.org/pdf/2402.00769",
    "GitHub": "https://github.com/G-U-N/AnimateLCM",
    "HuggingFace": "https://huggingface.co/wangfuyun/AnimateLCM",
    "Summary": "AnimateLCM introduces a fast, efficient framework for generating videos in a personalized style without needing personalized video data. It decouples two learning objectives: (1) capturing video style from (non-personalized) style data, and (2) accelerating both image generation and motion generation. By separating image and motion priors during training, the method reduces inference time drastically—from ~25 seconds to ~1 second per clip—while maintaining high visual quality. The model supports adapter modules (e.g., ControlNet), remaining lightweight and highly adaptable. Experiments show that AnimateLCM achieves performance comparable to heavier diffusion methods, but in a fraction of the time . Code and pretrained weights are publicly available ."
  },
  {
    "Model": null,
    "Model Unique Name": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA",
    "Category": "Feature Extraction",
    "Detailed Category": null,
    "Dataset": "MTEB",
    "Paper": "https://arxiv.org/pdf/2409.10173",
    "GitHub": null,
    "HuggingFace": "https://huggingface.co/jinaai/jina-embeddings-v3",
    "Summary": "jina‑embeddings‑v3 introduces a 570M‑parameter transformer-based embedding model optimized via task-specific LoRA adapters. It supports multilingual (over 100 languages) and long-context (up to 8192 tokens) scenarios, while remaining efficient enough for on-edge deployment. Key techniques include adapter tuning for retrieval, clustering, classification, and text matching, Matryoshka Representation Learning for flexible dimensionality (from 1024 down to 32), and synthetic data augmentation to improve retrieval robustness. On the MTEB benchmark, it outperforms proprietary models (e.g. OpenAI, Cohere) in English and exceeds multilingual‑e5‑large‑instruct across all multilingual tasks."
  },
  {
    "Model": null,
    "Model Unique Name": "JINA CLIP: Your CLIP Model Is Also Your Text Retriever",
    "Category": "Feature Extraction",
    "Detailed Category": null,
    "Dataset": "LAION, MTEB",
    "Paper": "https://arxiv.org/pdf/2405.20204",
    "GitHub": null,
    "HuggingFace": null,
    "Summary": "This paper introduces Jina CLIP, a unified contrastive learning model trained jointly on both image-caption and text-text pairs. By combining cross-modal and text-only objectives in a multi-task framework, Jina CLIP closes the performance gap between CLIP and specialized text retrievers. Experiments show that its text encoder rivals dedicated text models on MTEB benchmarks, while its multimodal capability matches or exceeds CLIP (e.g., achieving ~85.8% Recall@5 across Flickr8k, Flickr30K, MS-COCO), on par with EVA‑CLIP, and outperforming OpenAI’s CLIP on text retrieval . Jina CLIP is fully open-sourced and supports both text-to-text and text-to-image retrieval within a single, efficient model ."
  },
  {
    "Model": null,
    "Model Unique Name": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "Category": "Fill-Mask",
    "Detailed Category": null,
    "Dataset": "GLUE, BEIR, MLDR, CodeSearchNet, StackQA",
    "Paper": "https://arxiv.org/pdf/2412.13663",
    "GitHub": "https://github.com/AnswerDotAI/ModernBERT",
    "HuggingFace": "https://huggingface.co/answerdotai/ModernBERT-large",
    "Summary": "ModernBERT is a thoroughly modernized encoder-only transformer model that incorporates state-of-the-art architecture optimizations—such as RoPE positional embeddings, alternating local-global attention, Flash Attention, and unpadding—for significant efficiency gains. Pre-trained on a massive 2 trillion tokens, including code and long-context data (up to 8,192 tokens), it achieves state-of-the-art results across diverse classification and retrieval tasks—especially single- and multi-vector semantic search, including code retrieval. The model offers excellent performance–size trade-offs, strong speed and memory efficiency on standard GPUs, and is available in base (~149 M parameters) and large (~395 M parameters) sizes."
  },
  {
    "Model": null,
    "Model Unique Name": "M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
    "Category": "Sentence Similarity",
    "Detailed Category": null,
    "Dataset": "MTEB, MIRACL, MKQA, BEIR, C-MTEB, MLDR",
    "Paper": "https://arxiv.org/pdf/2402.03216",
    "GitHub": "https://github.com/FlagOpen/FlagEmbedding",
    "HuggingFace": "https://huggingface.co/BAAI/bge-m3",
    "Summary": "BGE M3‑Embedding (M3‑Embedding) is a unified text embedding model designed for versatility across 100+ languages, three retrieval functions (dense, multi-vector, sparse), and input lengths up to 8,192 tokens. It employs a novel self-knowledge distillation technique, using ensemble-style teacher signals derived from its own multi-modal retrieval heads, alongside an optimized batching strategy for large-scale training. M3‑Embedding offers state-of-the-art performance on multilingual, cross-lingual, and long-document retrieval benchmarks (e.g., MIRACL, MKQA), and its model, code, and data have been made publicly available ."
  },
  {
    "Model": null,
    "Model Unique Name": "DEPTH PRO : Sharp Monocular Metric Depth In Less Than a Second",
    "Category": "Depth Estimation",
    "Detailed Category": null,
    "Dataset": "AM-2k, DIS-5k",
    "Paper": "https://arxiv.org/pdf/2410.02073",
    "GitHub": "https://github.com/apple/ml-depth-pro",
    "HuggingFace": "https://huggingface.co/apple/DepthPro",
    "Summary": "Depth Pro is a fast and accurate model for monocular depth estimation that delivers sharp, metric-scale depth maps in under a second. Unlike previous models that require camera intrinsics or extensive fine-tuning, Depth Pro works zero-shot across diverse scenes and devices. It uses a multi-scale Vision Transformer to preserve fine boundary details and structural sharpness in high-resolution outputs."
  },
  {
    "Model": null,
    "Model Unique Name": "Depth-Anything-V2",
    "Category": "Depth Estimation",
    "Detailed Category": null,
    "Dataset": "BlendedMVS, Hypersim, IRS, TartanAir, KITTI",
    "Paper": "https://arxiv.org/pdf/2406.09414",
    "GitHub": "https://github.com/DepthAnything/Depth-Anything-V2",
    "HuggingFace": "https://huggingface.co/depth-anything/Depth-Anything-V2-Large",
    "Summary": "Depth Anything V2 improves performance by leveraging three core ideas: using synthetic-only labeled data, training a large DINOv2-G based teacher model, and distilling it into smaller student models using pseudo-labels on 62M real images."
  },
  {
    "Model": null,
    "Model Unique Name": "FLUX: FAST SOFTWARE-BASED COMMUNICATION OVERLAP ON GPUS THROUGH KERNEL FUSION",
    "Category": "Text-to-Image",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2406.06858",
    "GitHub": "https://github.com/black-forest-labs/flux",
    "HuggingFace": "https://huggingface.co/black-forest-labs/FLUX.1-dev",
    "Summary": "FLUX introduces a novel software-level GPU optimization designed to hide communication latency by tightly fusing fine-grained communication and computation steps into unified kernels. Instead of treating communication and computation separately, FLUX over-decomposes operations and fuses them into larger GPU kernels, enabling up to 96% overlap of communication with work that would otherwise sit idle. In practice, this achieves up to 1.24× speedups in distributed training with Megatron‑LM on 128 GPUs, and up to 1.66×/1.30× speedups in prefill and decoding inference with vLLM on 8-GPU clusters. This approach delivers significant efficiency gains without altering the model architecture or compromising kernel performance ."
  },
  {
    "Model": null,
    "Model Unique Name": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
    "Category": "Image-to-Image",
    "Detailed Category": null,
    "Dataset": "Emu Edit, MagicBrush",
    "Paper": "https://arxiv.org/abs/2504.20690",
    "GitHub": "https://github.com/River-Zhang/ICEdit?tab=readme-ov-file",
    "HuggingFace": "https://huggingface.co/RiverZ/normal-lora",
    "Summary": "In‑Context Edit introduces a unified and efficient framework for instruction-guided image editing that requires no structural model modifications or extensive data. It leverages the native contextual capabilities of a large-scale Diffusion Transformer (DiT) to perform zero-shot edits based on textual instructions. The framework enhances adaptability by integrating a LoRA-MoE hybrid tuning method that activates modular experts dynamically, and it employs an early-filter mechanism using vision-language models to refine initial noise samples and improve output quality. Evaluated against state-of-the-art methods, it achieves superior editing precision while using only 0.5 % of training data and 1 % of trainable parameters compared to conventional baselines . Code and demos are available via its project page ."
  },
  {
    "Model": null,
    "Model Unique Name": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "Category": "Image-to-Video",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/abs/2503.20314",
    "GitHub": "https://github.com/Wan-Video/Wan2.1",
    "HuggingFace": "https://huggingface.co/Wan-AI",
    "Summary": "Wan is a suite of open-source video foundation models built on diffusion transformer architecture, engineered to push the boundaries of video generation. The suite includes two models—a compact 1.3 B parameter version optimized for consumer-grade GPUs (≈8 GB VRAM) and a larger 14 B parameter variant—for balanced efficiency and performance. Key innovations include a novel diffusion-aware VAE, scalable pretraining on billions of image/video samples, curated large-scale datasets, and automated evaluation protocols. Across multiple benchmarks, Wan consistently outperforms existing open-source and commercial models, demonstrating clear scaling benefits. By openly releasing all model checkpoints and code, Wan aims to accelerate innovation in video generative modeling ."
  },
  {
    "Model": null,
    "Model Unique Name": "SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation",
    "Category": "Image-to-Video",
    "Detailed Category": null,
    "Dataset": "ObjaverseDSy, Consistent4D, DAVIS",
    "Paper": "https://arxiv.org/pdf/2503.16396",
    "GitHub": "https://github.com/Stability-AI/generative-models",
    "HuggingFace": "https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt",
    "Summary": "SV4D 2.0 is a unified diffusion-based framework that transforms a single-view input video into high-fidelity novel-view videos and optimizes a 4D dynamic 3D asset. By redesigning the network architecture to remove dependency on separate multi-view references and adopting a blend of 3D and temporal attention, it achieves robust performance even with occlusions and large motion. The model is further enhanced through progressive multi-stage training, improved data curation, and a two-stage refinement process with progressive frame sampling. Compared to its predecessor, SV4D 2.0 significantly enhances visual detail sharpness (14% LPIPS reduction) and spatio-temporal consistency (44% FV4D improvement), and delivers pronounced gains across both synthetic and real-world benchmarks (e.g., ObjaverseDy, DAVIS) ."
  },
  {
    "Model": null,
    "Model Unique Name": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters",
    "Category": "Image-to-Video",
    "Detailed Category": null,
    "Dataset": "LatentSync-filtered AV data, CelebV-HQ, HDTF, Self-Collected Full-Body Wild",
    "Paper": "https://arxiv.org/pdf/2505.20156",
    "GitHub": "https://github.com/Tencent-Hunyuan/HunyuanVideo-Avatar",
    "HuggingFace": "https://huggingface.co/tencent/HunyuanVideo-Avatar",
    "Summary": "HunyuanVideo‑Avatar is a diffusion transformer model that generates highly dynamic, multi-character dialogue videos driven solely by audio inputs. It overcomes character identity drift by injecting reference images channel-wise instead of via naive addition, enabling both expressiveness and consistency. An Audio Emotion Module ensures that generated facial expressions accurately reflect emotional tone, while a Face‑Aware Audio Adapter isolates audio-driven motion to specific face regions, enabling synchronized multi-character performance. Evaluated on standard benchmarks and a new wild dataset, the model demonstrates marked improvements in motion realism, lip-sync accuracy, and emotion alignment, outperforming prior audio-driven animation methods across key metrics ."
  },
  {
    "Model": null,
    "Model Unique Name": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
    "Category": "Zero-Shot Image Classification",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2303.00915",
    "GitHub": "https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
    "HuggingFace": "https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
    "Summary": "BiomedCLIP introduces PMC‑15M, a large-scale biomedical dataset consisting of 15 million image–caption pairs extracted from 4.4 million articles in PubMed Central. By fine-tuning both image and text encoders—leveraging domain-specialized architectures like PubMedBERT—it adapts CLIP-style contrastive training to the biomedical domain. The resulting model sets new state-of-the-art results in cross-modal retrieval, zero-shot image classification, and visual question answering across eight standard biomedical benchmarks. Notably, BiomedCLIP even surpasses radiology-specific models such as BioViL on pneumonia detection, highlighting the benefits of pretraining on diverse biomedical image types ."
  },
  {
    "Model": null,
    "Model Unique Name": "Scaling Open-Vocabulary Object Detection",
    "Category": "Zero-Shot Object Detection",
    "Detailed Category": null,
    "Dataset": "WebLI, LVIS, ODinW13",
    "Paper": "https://arxiv.org/pdf/2306.09683",
    "GitHub": "https://github.com/inuwamobarak/OWLv2",
    "HuggingFace": "https://huggingface.co/google/owlv2-large-patch14-ensemble",
    "Summary": "This paper presents OWLv2 and its OWL‑ST self‑training recipe that unlock web-scale weak supervision for open‑vocabulary object detection by generating pseudo‑box labels on massive image–text datasets. OWLv2 enhances training efficiency through optimized architectures and data augmentation techniques, allowing it to surpass prior state-of-the-art detectors using only ~10 million examples. OWL‑ST further scales training to over 1 billion image–text pairs, improving zero-shot detection average precision (AP) on rare LVIS classes from 31.2% to 44.6% using a ViT-L/14 backbone. The approach requires no architectural changes or extra annotation, demonstrating that scaling self-training on weak supervision matches trends in image-level models and significantly advances open-world localization ."
  },
  {
    "Model": null,
    "Model Unique Name": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
    "Category": "Zero-Shot Object Detection",
    "Detailed Category": null,
    "Dataset": "OpenImages v5 subset, Localized Narratives (OpenImages), LVIS subset, RefCOCO, RefCOCO+, RefCOCOg, gRefCOCO",
    "Paper": "https://arxiv.org/pdf/2303.05499",
    "GitHub": "https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file",
    "HuggingFace": "https://huggingface.co/IDEA-Research/grounding-dino-tiny",
    "Summary": "Grounding DINO enhances the DETR-based DINO object detector by incorporating language-aware grounded pre-training, enabling zero-shot detection of arbitrary concepts specified via text. The model integrates language cues into key detector stages—feature enhancement, query selection, and a cross-modality decoder—to achieve tight multimodal fusion. It is pre-trained on detection, grounding, and caption datasets, and excels in open-set and referring-expression detection tasks. Empirical results show that Grounding DINO achieves a zero-shot COCO average precision (AP) of 52.5 and sets new records in the ODinW benchmark, while maintaining strong performance on LVIS and RefCOCO datasets ."
  },
  {
    "Model": null,
    "Model Unique Name": "Structured 3D Latents for Scalable and Versatile 3D Generation∗",
    "Category": "Text-to-3D",
    "Detailed Category": null,
    "Dataset": "ObjaverseXL (sketchfab), ObjaverseXL (github), ABO, 3D-FUTURE, HSSD",
    "Paper": "https://arxiv.org/pdf/2412.01506",
    "GitHub": "https://github.com/Microsoft/TRELLIS",
    "HuggingFace": "https://huggingface.co/microsoft/TRELLIS-text-xlarge",
    "Summary": "This paper introduces a unified structured latent (SLAT) representation that combines a sparse 3D grid with dense visual features from a vision foundation model, enabling versatile decoding into various 3D formats including radiance fields, 3D Gaussians, and meshes. Leveraging rectified flow transformers, the authors train multi-billion parameter models (up to 2B) on a diverse dataset of half a million 3D assets. The resulting system, named TRELLIS, generates high-fidelity 3D geometry and textures in approximately 10 seconds from either text or image prompts, surpassing prior methods at similar scale. Additionally, TRELLIS supports local, prompt-driven editing without retraining, offering an efficient and flexible foundation for scalable, versatile 3D content creation ."
  },
  {
    "Model": null,
    "Model Unique Name": "TripoSR: Fast 3D Object Reconstruction from a Single Image",
    "Category": "Image-to-3D",
    "Detailed Category": null,
    "Dataset": "Objaverse",
    "Paper": "https://arxiv.org/pdf/2403.02151",
    "GitHub": "https://github.com/VAST-AI-Research/TripoSR",
    "HuggingFace": "https://huggingface.co/stabilityai/stable-fast-3d",
    "Summary": "TripoSR is a transformer-based model designed for fast, feed-forward 3D mesh reconstruction from a single image, achieving inference in under 0.5 seconds on high-end GPUs without per-shape optimization. Building on the LRM architecture, TripoSR integrates advances in data curation, encoder-decoder design, and training techniques. It outputs textured 3D triplane-based neural radiance fields, and evaluations across standard benchmarks demonstrate state-of-the-art performance—improving Chamfer distance and F-score over prior open-source methods—while offering significantly faster throughput. Released under the MIT license with code, demo, and pretrained models publicly available, TripoSR supports practical deployment and further research ."
  },
  {
    "Model": null,
    "Model Unique Name": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
    "Category": "Image-to-3D",
    "Detailed Category": null,
    "Dataset": "Objaverse, Objaverse-XL",
    "Paper": "https://arxiv.org/abs/2501.12202",
    "GitHub": "https://github.com/Tencent-Hunyuan/Hunyuan3D-2",
    "HuggingFace": "https://huggingface.co/tencent/Hunyuan3D-2",
    "Summary": "Hunyuan3D 2.0 delivers an open-source, two-stage diffusion-based system for generating high-resolution textured 3D assets from images. The first stage, Hunyuan3D‑DiT, employs a flow-based diffusion transformer to generate detailed geometry conditioned on input images, while the second stage, Hunyuan3D‑Paint, synthesizes vibrant texture maps using geometric and diffusion priors. Together with a user-friendly production platform—Hunyuan3D‑Studio—users can easily generate, edit, stylize, and animate meshes. Extensive quantitative and qualitative evaluations, including user studies, confirm that Hunyuan3D 2.0 surpasses both open- and closed-source baselines in geometry fidelity, texture quality, and condition alignment, offering a significant tool for scalable 3D content creation ."
  },
  {
    "Model": null,
    "Model Unique Name": "VGGT: Visual Geometry Grounded Transformer",
    "Category": "Image-to-3D",
    "Detailed Category": null,
    "Dataset": "Co3Dv2, BlendMVS, DL3DV, MegaDepth, Kubric, WildRGB, ScanNet, HyperSim, Mapillary Metropolis, Habitat, Replica, MVS-Synth, PointOdyssey, Virtual KITTI, Aria Synthetic Environments, Aria Digital Twin, Objaverse",
    "Paper": "https://arxiv.org/pdf/2503.11651",
    "GitHub": "https://github.com/facebookresearch/vggt",
    "HuggingFace": "https://huggingface.co/facebook/VGGT-1B",
    "Summary": "VGGT is a feed-forward transformer designed to directly infer comprehensive 3D scene attributes—camera parameters, depth maps, point maps, and 3D point tracks—from one or hundreds of input views in under a second. By forgoing explicit geometric post-processing (like bundle adjustment) and applying a standard transformer with alternating frame-level and global attention, VGGT consistently matches or exceeds traditional optimization-based methods on multiple 3D benchmarks. The model’s learned features also effectively enhance downstream tasks such as novel-view synthesis and non-rigid point tracking. With publicly released code and pretrained weights, VGGT establishes a fast, unified foundation for versatile, multi-task 3D vision that scales seamlessly with increased input views ."
  },
  {
    "Model": null,
    "Model Unique Name": "BGE: One-Stop Retrieval Toolkit For Search and RAG",
    "Category": "Text Classification",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2412.14475",
    "GitHub": "https://github.com/FlagOpen/FlagEmbedding/tree/master?tab=readme-ov-file",
    "HuggingFace": "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "Summary": "MegaPairs proposes a scalable data synthesis pipeline that combines vision-language models (CLIP and DINO) with open-domain image corpora to generate heterogeneous triplets of images and textual instructions. By mining correlated image pairs with varied relationships and using multimodal and language models to produce descriptive, open-ended instructions, the method creates over 26 million high-quality training instances. Models trained on MegaPairs, named MMRet, outperform baselines trained on 70× more private data, achieving state-of-the-art zero-shot performance on composed image retrieval (CIR) benchmarks and across 36 datasets in the MMEB suite. MegaPairs and its trained retrievers are fully open-sourced to support further research and applications ."
  },
  {
    "Model": null,
    "Model Unique Name": "GLiNER: Generalist and Lightweight Model for Named Entity Recognition",
    "Category": "Token Classification",
    "Detailed Category": null,
    "Dataset": "Pile‑NER",
    "Paper": "https://arxiv.org/pdf/2311.08526",
    "GitHub": "https://github.com/urchade/GLiNER",
    "HuggingFace": "https://huggingface.co/urchade/gliner_multi_pii-v1",
    "Summary": "GLiNER is a compact, bidirectional transformer model that reframes named entity recognition as a matching problem between textual span embeddings and entity-type embeddings, rather than sequential token generation. By encoding both entity labels and candidate text spans in a shared latent space, it enables efficient parallel extraction of arbitrary entity types. Despite its smaller size, GLiNER outperforms larger LLMs like ChatGPT and fine-tuned models on zero-shot benchmarks, and demonstrates robust multilingual capabilities—even on languages unseen during training. It serves as a resource-efficient and accurate solution for flexible, instruction-driven NER tasks ."
  },
  {
    "Model": null,
    "Model Unique Name": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
    "Category": "Feature Extraction",
    "Detailed Category": null,
    "Dataset": "MTEB, synthetic text relevance dataset (~150M pairs), high-quality supervised dataset (~7M pairs)",
    "Paper": "https://arxiv.org/pdf/2506.05176",
    "GitHub": "https://github.com/QwenLM/Qwen3-Embedding",
    "HuggingFace": "https://huggingface.co/Qwen/Qwen3-Embedding-0.6B",
    "Summary": "Qwen3 Embedding presents a series of embedding models (0.6B, 4B, and 8B parameters) derived from the Qwen3 foundation models, designed for both text embedding and reranking tasks. The authors introduce a multi-stage training pipeline with large-scale unsupervised pretraining followed by supervised fine-tuning on high-quality relevance datasets, and apply model-merging strategies to improve robustness. They further leverage the Qwen3 instruct model to synthesize a diverse, multilingual text relevance dataset for supervised training. Evaluations show that Qwen3 Embedding achieves state-of-the-art performance across multiple benchmarks, outperforming its predecessor (GTE‑Qwen) and matching or surpassing other contemporary embedding models in both efficiency and effectiveness ."
  },
  {
    "Model": null,
    "Model Unique Name": "DeepSeek-R1",
    "Category": null,
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2501.12948",
    "GitHub": "https://github.com/deepseek-ai/DeepSeek-R1",
    "HuggingFace": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",
    "Summary": "DeepSeek‑R1 presents a novel paradigm for enhancing large language model reasoning through pure reinforcement learning (RL), first demonstrating that a base model (DeepSeek‑R1‑Zero) can develop chain-of-thought reasoning and self-reflection without any supervised fine-tuning. To improve readability and reduce language mixing, the authors introduce DeepSeek‑R1, which adds a cold-start supervised stage followed by additional RL and supervised fine-tuning. This multi-stage approach achieves reasoning performance on par with OpenAI’s o1‑1217 model across benchmarks like AIME, MATH‑500, MMLU, GPQA, Codeforces, and more. They also showcase effective distillation, transferring reasoning capabilities into smaller models (1.5B–70B), where a 14B student outperforms open‑source baselines and larger models—highlighting RL + distillation as a cost-efficient path to producing strong reasoning systems. All models and code are open-sourced ."
  },
  {
    "Model": null,
    "Model Unique Name": "VGGT: Visual Geometry Grounded Transformer",
    "Category": " 3D Vision",
    "Detailed Category": null,
    "Dataset": "Co3Dv2, Objaverse, NYUv2",
    "Paper": "https://arxiv.org/pdf/2503.11651",
    "GitHub": "https://github.com/facebookresearch/vggt",
    "HuggingFace": null,
    "Summary": "VGGT is a large feed-forward transformer that directly infers comprehensive 3D scene attributes—including camera intrinsics/extrinsics, depth maps, point maps, and 3D point tracks—from anywhere between one to hundreds of input views, in under a second, eliminating the need for traditional geometry post-processing like bundle adjustment. It achieves state-of-the-art performance across multiple tasks—camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and point tracking—often matching or surpassing methods that rely on optimization, while also enhancing downstream applications such as feed-forward novel-view synthesis and non-rigid point tracking. Built with minimal 3D-specific inductive biases (aside from alternating frame-level and global attention), VGGT leverages a transformer backbone trained on large-scale annotated 3D data, and its code and models are publicly released ."
  },
  {
    "Model": null,
    "Model Unique Name": "MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors",
    "Category": " 3D Vision",
    "Detailed Category": null,
    "Dataset": "TUM-RGBD, 7-Scenes, EuRoC, ETH3D",
    "Paper": "https://arxiv.org/pdf/2412.12392",
    "GitHub": "https://github.com/rmurai0610/MASt3R-SLAM",
    "HuggingFace": null,
    "Summary": "MASt3R‑SLAM introduces a real‑time monocular dense SLAM system built atop a two-view 3D reconstruction prior from MASt3R, enabling robust and in‑the‑wild performance without requiring fixed camera intrinsics beyond a single camera center. The system fuses fine-grained pointmap matching, efficient camera tracking, local volumetric fusion, graph construction with loop closure, and second-order global optimization into a unified framework running at around 15 FPS. When camera calibration is known, it achieves state‑of‑the‑art accuracy in both trajectory estimation and dense geometry recovery across standard benchmarks. By integrating a learned geometric prior directly into the SLAM pipeline, MASt3R‑SLAM provides a plug‑and‑play solution for globally consistent monocular dense mapping and localization ."
  },
  {
    "Model": null,
    "Model Unique Name": "UniK3D: Universal Camera Monocular 3D Estimation",
    "Category": "depth estimation",
    "Detailed Category": null,
    "Dataset": "KITTI, NYUv2, MegaDepth, NianticMapFree, Mapillary",
    "Paper": "https://arxiv.org/pdf/2503.16591",
    "GitHub": "https://github.com/lpiccinelli-eth/UniK3D",
    "HuggingFace": null,
    "Summary": "UniK3D introduces a breakthrough in monocular 3D estimation by supporting any camera model—from standard pinhole to fisheye and panoramic—without requiring calibration or rectification. The model leverages a spherical representation for both scene geometry and camera projection, where rays are encoded via learned spherical harmonics. An angular loss is incorporated to prevent geometric contraction in wide-field scenarios. Evaluated zero-shot across 13 diverse datasets, UniK3D achieves state-of-the-art performance in metric depth, 3D point-cloud reconstruction, and camera parameter estimation, particularly excelling in extreme field-of-view conditions while matching performance on standard narrow views. The method is fully open-sourced, with code and pretrained models available."
  },
  {
    "Model": null,
    "Model Unique Name": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
    "Category": "depth estimation",
    "Detailed Category": null,
    "Dataset": "Sintel, ScanNet, KITTI, Bonn, NYU-v2",
    "Paper": "https://arxiv.org/pdf/2409.02095",
    "GitHub": "https://github.com/Tencent/DepthCrafter",
    "HuggingFace": null,
    "Summary": "DepthCrafter adapts a pre-trained image-to-video diffusion model to generate temporally coherent, long-range depth sequences for diverse real-world videos without relying on camera poses or optical flow. Through a carefully designed three-stage training pipeline using both synthetic and real paired video-depth data, the method enables zero-shot depth generation of up to 110 frames at once. At inference, it processes longer clips via segment-wise estimation with seamless stitching. It delivers state-of-the-art performance in zero-shot open-world video depth estimation, with rich geometry and temporal consistency, and supports downstream tasks such as depth-based visual effects and conditional video editing ."
  },
  {
    "Model": null,
    "Model Unique Name": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
    "Category": "depth estimation",
    "Detailed Category": null,
    "Dataset": "KITTI, Bonn, ScanNet, Sinte l, DAVIS",
    "Paper": "https://arxiv.org/pdf/2501.12375",
    "GitHub": "https://github.com/DepthAnything/Video-Depth-Anything",
    "HuggingFace": null,
    "Summary": "Video Depth Anything extends the image-trained Depth Anything V2 model by replacing its head with a lightweight spatio-temporal module and introducing a simple temporal-gradient matching loss to enforce depth consistency across time, all without requiring optical flow or camera pose priors. The system also implements a key-frame–based inference strategy that enables zero-shot depth estimation on arbitrarily long videos—spanning several minutes—while maintaining both spatial accuracy and temporal coherence. Evaluated across multiple video benchmarks, it establishes new state-of-the-art performance in zero-shot video depth estimation, balancing quality, efficiency, and scalability, with the smallest variant running in real-time (~30 FPS) ."
  },
  {
    "Model": null,
    "Model Unique Name": "Interpreting Object-level Foundation Models via Visual Precision Search",
    "Category": "explainability and interpretability",
    "Detailed Category": null,
    "Dataset": "MS COCO, RefCOCO, LVIS",
    "Paper": "https://arxiv.org/pdf/2411.16198",
    "GitHub": "https://github.com/RuoyuChen10/VPS",
    "HuggingFace": null,
    "Summary": "This paper introduces Visual Precision Search (VPS), a novel, gradient-free method for generating accurate, instance-specific saliency maps to interpret object-level foundation models like Grounding DINO and Florence‑2. VPS works by segmenting input images into sparse superpixel regions and ranking them using consistency and collaboration scores via a submodular selection mechanism—resulting in precise localization of critical decision regions. Evaluations on MS COCO, RefCOCO, and LVIS show that VPS greatly improves attribution faithfulness—up to 31.6% for Grounding DINO and 102.9% for Florence‑2—and reliably identifies factors contributing to detection failures. The method bypasses internal gradient reliance common in multimodal detectors, offers theoretical guarantees on region selection, and opens a path for more trustworthy interpretation of vision-language models ."
  },
  {
    "Model": null,
    "Model Unique Name": "Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders",
    "Category": "explainability and interpretability",
    "Detailed Category": null,
    "Dataset": "GazeFollow, VideoAttentionTarget, ChildPlay, GOO-Real",
    "Paper": "https://arxiv.org/pdf/2412.09586",
    "GitHub": "https://github.com/fkryan/gazelle",
    "HuggingFace": null,
    "Summary": "Gaze‑LLE simplifies gaze target estimation by harnessing a frozen DINOv2 visual encoder to extract a unified scene representation, then injecting person‑specific positional prompts to decode gaze targets using a lightweight transformer decoder. This streamlined approach eliminates the need for complex multi‑branch pipelines and auxiliary inputs like depth or pose, while achieving state‑of‑the‑art performance on multiple gaze benchmarks. Despite using one to two orders of magnitude fewer trainable parameters, Gaze‑LLE delivers efficient inference (~50 FPS on an RTX 4090), strong cross‑dataset generalization, and rapid training (≈1.5 GPU hours), all with publicly released code and models ."
  },
  {
    "Model": null,
    "Model Unique Name": "MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
    "Category": "generative models",
    "Detailed Category": null,
    "Dataset": "AudioSet, Freesound, VGGSound, AudioCaps, WavCaps",
    "Paper": "https://arxiv.org/pdf/2412.15322",
    "GitHub": "https://github.com/hkchengrex/MMAudio",
    "HuggingFace": null,
    "Summary": "MMAudio presents a unified framework for video-to-audio synthesis that is jointly trained on paired video–audio data and large-scale text–audio corpora, enabling high-quality and semantically aligned audio generation. The model enhances audio–visual synchrony using a novel conditional synchronization module that aligns audio latents with video frames at a fine-grained level. With only 157M parameters and using a flow-matching objective, MMAudio achieves state-of-the-art performance in terms of audio fidelity, semantic relevance, and temporal alignment, while keeping inference time low (~1.23 s for an 8‑second clip). Interestingly, joint training does not compromise single-modality performance, as MMAudio also demonstrates competitive results in text-to-audio tasks ."
  },
  {
    "Model": null,
    "Model Unique Name": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
    "Category": "generative models",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2403.09055",
    "GitHub": "https://github.com/ironjr/semantic-draw",
    "HuggingFace": null,
    "Summary": "SemanticDraw introduces a transformative interactive image-generation tool that allows content creators to define multiple user-drawn regions, each tied to separate text prompts, and synthesize high-quality images in real time (≈0.64 s per 512×512 image) on a single RTX 2080 Ti GPU. It tackles the incompatibility between region-based control and accelerated diffusion sampling by integrating latent pre-averaging, mask-centering bootstrapping, and mask quantization, enabling LCM-compatible processing in just a few steps. Further, a multi-prompt streaming pipeline batches region-based prompts through continuous diffusion steps, achieving ~1.57 FPS in panorama generation—over 10× faster than prior MultiDiffusion approaches. This enables a new “semantic palette” interface where users can paint with text-linked brushes interactively. The method is model-agnostic, supports any existing diffusion models and schedulers, and is supported by open-source code ."
  },
  {
    "Model": null,
    "Model Unique Name": "MINIMA: Modality Invariant Image Matching",
    "Category": "image matching",
    "Detailed Category": null,
    "Dataset": "MD-syn (MegaDepth-Syn), LLVIP, M3FD, MSRS, METU-VisTIR, MMIM, DIODE, DSEC (RGB-Event)",
    "Paper": "https://arxiv.org/pdf/2412.19412",
    "GitHub": "https://github.com/LSXI7/MINIMA",
    "HuggingFace": null,
    "Summary": "MINIMA introduces a unified image matching framework that generalizes across diverse modalities—such as RGB, depth, infrared, event cameras, and artistic styles—by generating large-scale multimodal training data from inexpensive RGB sources. The core innovation is a “data engine” that converts richly annotated RGB datasets into synthetic multimodal pairs (MD‑syn) using generative models, effectively scaling paired data with accurate pixel-wise correspondence labels. Trained on MD‑syn, MINIMA achieves remarkable cross-modal performance and strong zero-shot transfer, outperforming modality-specific methods on 19 real-world matching tasks. The code and dataset are publicly available to support future multimodal matching research ."
  },
  {
    "Model": null,
    "Model Unique Name": "Layered Image Vectorization via Semantic Simplification",
    "Category": "image vectorization",
    "Detailed Category": null,
    "Dataset": null,
    "Paper": "https://arxiv.org/pdf/2406.05404",
    "GitHub": "https://github.com/SZUVIZ/layered_vectorization",
    "HuggingFace": null,
    "Summary": "This paper introduces a progressive, two-stage image vectorization pipeline that reconstructs raster images as layered vector graphics by employing semantic simplification. First, a diffusion-based Score Distillation Sampling method simplifies the image in stages—from coarse semantic structures down to fine details—producing a sequence of abstraction levels. Then, those simplified images guide a structured vector-fitting process that builds clean, semantic-aligned SVG layers. The resulting vector output is both visually faithful and compact, enabling intuitive editing and semantic-aware manipulation. Experiments across diverse image types demonstrate that this method outperforms traditional single-pass vectorization approaches in terms of layer coherence, editability, and file size efficiency ."
  },
  {
    "Model": null,
    "Model Unique Name": "DEIM: DETR with Improved Matching for Fast Convergence",
    "Category": "object detection",
    "Detailed Category": null,
    "Dataset": "COCO",
    "Paper": "https://arxiv.org/pdf/2412.04234",
    "GitHub": "https://github.com/ShihuaHuang95/DEIM",
    "HuggingFace": null,
    "Summary": "DEIM enhances Transformer-based DETR object detectors by replacing sparse one-to-one (O2O) matching with Dense O2O, which generates more positive samples per image via standard data augmentations. To address noise from lower-quality matches, the framework introduces a Matchability-Aware Loss (MAL) that weights matches based on confidence, improving training stability and speed. Experiments on COCO show DEIM reduces training time by 50% and achieves 53.2% AP on RT-DETRv2 after just one day of training on a 4090 GPU. Real-time detectors trained with DEIM—such as DEIM-D-FINE-L and DEIM-D-FINE-X—outperform state-of-the-art models at up to 124 FPS, without extra data or architectural changes ."
  },
  {
    "Model": null,
    "Model Unique Name": "MITracker: Multi-View Integration for Visual Object Tracking",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "MVTrack, GOT-10k, GMTD",
    "Paper": "https://arxiv.org/pdf/2502.20111",
    "GitHub": "https://github.com/XuM007/MITracker",
    "HuggingFace": null,
    "Summary": "MITracker presents an innovative framework that addresses occlusion and target loss by integrating synchronized multi-view inputs into a unified 3D feature volume projected into bird’s-eye-view (BEV) space. Key to its design is an attention mechanism that fuses geometric consistency from this paired 3D representation to refine per-view tracking results, enabling robust, class-agnostic object tracking over arbitrary-length sequences. Alongside, the authors introduce MVTrack—a high-quality multi-view dataset featuring 234,000 annotated frames of 27 distinct objects across diverse scenes—which supports both training and evaluation. MITracker achieves state-of-the-art performance on the MVTrack and GMTD benchmarks, demonstrating superior robustness to occlusion and long-term consistency compared to leading single-view methods  ￼.\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Multiple Object Tracking as ID Prediction",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "DanceTrack, SportsMOT, MOT17",
    "Paper": "https://arxiv.org/pdf/2403.16848",
    "GitHub": "https://github.com/MCG-NJU/MOTIP",
    "HuggingFace": null,
    "Summary": "This paper recasts multi-object tracking as a direct ID-prediction task, using a transformer-based ID Decoder that learns to assign consistent identities across frames rather than relying on tracking-by-detection pipelines. The ID Decoder employs a dynamic multi-layer structure—optimized through ablation studies—to handle appearance changes and occlusions, achieving a HOTA score improvement from 54.3 to 60.5 by moving from 1 to 6 layers. The method is fully feed-forward and demonstrates strong performance across standard MOT benchmarks without requiring complex association or motion models. By unifying detection and tracking through identity prediction, the approach simplifies the tracking framework while maintaining competitive accuracy ."
  },
  {
    "Model": null,
    "Model Unique Name": "EdgeTAM: On-Device Track Anything Model",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "DAVIS 2017, MOSE, SA-V (val/test), YouTube-VOS",
    "Paper": "https://arxiv.org/pdf/2501.07256",
    "GitHub": "https://github.com/facebookresearch/EdgeTAM",
    "HuggingFace": null,
    "Summary": "EdgeTAM adapts SAM 2 for on-device video segmentation and tracking by introducing a 2D Spatial Perceiver module that compresses frame‑level memory features while preserving spatial structure, and employs feature‑level distillation from the full SAM 2 model. As a result, it runs at 16 FPS on iPhone 15 Pro Max—about 22× faster than SAM 2—while delivering comparable or better accuracy on DAIVS‐2017, MOSE, SA‑V, and YouTubeVOS benchmarks (e.g., 87.7 J&F on DAVIS’17). Training involved aligning both encoder and memory-attention features between teacher and student, yielding a lightweight yet high-performing on-device system ."
  },
  {
    "Model": null,
    "Model Unique Name": "A Distractor-Aware Memory for Visual Object Tracking with SAM2",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "DiDi, VOT2020, VOT2022, VOTChallenge, LaSOT, GOT-10k",
    "Paper": "https://arxiv.org/pdf/2411.17576",
    "GitHub": "https://github.com/jovanavidenovic/DAM4SAM",
    "HuggingFace": null,
    "Summary": "This work enhances memory-based visual object tracking—particularly with SAM2—by introducing a novel Distractor-Aware Memory (DAM) that separates recent target appearances (RAM) from hard-to-distinguish distractors (DRM). The key innovation is an introspection-driven update mechanism: when SAM2’s predicted mask diverges from an alternative segmentation, the frame is added to DRM to boost robustness. They also release DiDi, a distractor-distilled dataset designed to expose real-world tracking failures. Without additional training, the resulting SAM2.1++ significantly outperforms SAM2.1 and related variants across seven benchmarks, setting a new state-of-the-art on six of them."
  },
  {
    "Model": null,
    "Model Unique Name": "From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization",
    "Category": "Object Tracking",
    "Detailed Category": null,
    "Dataset": "Market1501, SYSU‑MM01, Occluded‑ReID",
    "Paper": "https://arxiv.org/pdf/2503.00938",
    "GitHub": "https://github.com/yuanc3/Pose2ID",
    "HuggingFace": null,
    "Summary": "Pose2ID introduces a training‑free person Re‑ID approach that enhances identity feature stability by centralizing embeddings around per‑identity feature centers. The method comprises two key components: an identity‑guided pedestrian generator that synthesizes diverse view samples to enrich identity representation, and neighbor feature centralization that aggregates proximal neighbors in embedding space to reduce noise from occlusion and background interference. Surprisingly, without any Re‑ID model fine‑tuning, Pose2ID achieves strong performance—52.8 mAP / 78.9 Rank‑1 on Market1501—and sets new benchmarks across cross‑modality and occluded Re‑ID tasks ."
  },
  {
    "Model": null,
    "Model Unique Name": "Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models",
    "Category": "open-world detection",
    "Detailed Category": null,
    "Dataset": "Anomaly-Instruct-125k, VisA-D&R, MVTec AD, AITEX, ELPV, BTAD, MPDD, BrainMRI, HeadCT, Br35H",
    "Paper": "https://arxiv.org/pdf/2502.07601",
    "GitHub": "https://github.com/honda-research-institute/Anomaly-OneVision",
    "HuggingFace": null,
    "Summary": "This paper introduces Anomaly‑Instruct‑125k, the first large-scale visual-instruction tuning dataset for zero-shot anomaly detection (ZSAD) and reasoning, paired with a comprehensive benchmark called VisA‑D&R. By evaluating current multimodal large language models (MLLMs) like GPT‑4o, the authors demonstrate these models struggle to detect and explain fine-grained visual anomalies. To address this, they propose Anomaly‑OneVision (Anomaly‑OV), a specialist system that leverages a Look‑Twice Feature Matching mechanism to adaptively select and emphasize abnormal visual tokens for precise detection and human-readable reasoning. Anomaly‑OV significantly outperforms generalist models on detection accuracy and anomaly interpretation, and extends effectively to industrial, medical, and 3D anomaly scenarios ."
  },
  {
    "Model": null,
    "Model Unique Name": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
    "Category": "pose estimation",
    "Detailed Category": null,
    "Dataset": "Human3.6M, MPI-INF-3DHP, COCO, MPII, AI Challenger, AVA, InstaVariety, MOYO",
    "Paper": "https://arxiv.org/pdf/2503.21751",
    "GitHub": "https://github.com/IsshikiHugh/HSMR",
    "HuggingFace": null,
    "Summary": "This work introduces HSMR (Human Skeleton and Mesh Recovery), a novel transformer-based model that reconstructs a full-body 3D mesh with a biomechanically accurate skeleton, unlike standard models like SMPL. HSMR is trained using a self-generated pseudo-ground-truth pipeline: it synthesizes SKEL-model parameters for real images and iteratively refines them through training loops to improve accuracy. The result delivers anatomically plausible joint predictions and high-fidelity surface meshes from a single image. By combining SKEL’s biomechanical rig with robust image-to-parameter regression, HSMR enables realistic and physically consistent 3D human modeling and is publicly released with project code ."
  },
  {
    "Model": null,
    "Model Unique Name": "MatAnyone: Stable Video Matting with Consistent Memory Propagation",
    "Category": "segmentation",
    "Detailed Category": null,
    "Dataset": "VM800, YoutubeMatte",
    "Paper": "https://arxiv.org/pdf/2501.14677",
    "GitHub": "https://github.com/pq-yang/MatAnyone",
    "HuggingFace": null,
    "Summary": "MatAnyone introduces a memory-based video matting framework that enables temporally stable and semantically consistent matte extraction for a user-specified target across diverse scenes. By incorporating a region‑adaptive memory fusion module, the system selectively integrates previous-frame embeddings to maintain core object consistency while preserving high-fidelity boundary details. The authors also curated a high-quality, diverse matting dataset and devised a segmentation-assisted training strategy leveraging large-scale segmentation data to bolster model robustness. MatAnyone significantly outperforms prior auxiliary-free matting methods, particularly in complex or cluttered backgrounds, and is backed by an accompanying dataset and open-source implementation ."
  },
  {
    "Model": null,
    "Model Unique Name": "FoundationStereo: Zero-Shot Stereo Matching",
    "Category": "stereo matching",
    "Detailed Category": null,
    "Dataset": "FoundationStereo Dataset (1M synthetic stereo pairs), Scene Flow, Middlebury, ETH3D, KITTI",
    "Paper": "https://arxiv.org/pdf/2501.09898",
    "GitHub": "https://github.com/NVlabs/FoundationStereo",
    "HuggingFace": null,
    "Summary": "FoundationStereo presents a foundation-model approach for stereo depth estimation that achieves strong zero-shot generalization across diverse real-world domains without requiring per-domain fine-tuning. The model is trained on a curated 1 million synthetic stereo image pair dataset and enhanced with architectural components like a side-tuning feature backbone to adapt monocular priors, attentive long-range cost-volume filtering, and iterative refinement. These strategies together close the sim-to-real gap and deliver state-of-the-art robustness and accuracy on challenging scenarios—such as reflective surfaces, thin structures, and extreme lighting—establishing a new zero-shot benchmark for stereo matching. Though not yet real-time (≈0.7 s per image on A100), it sets the stage for deployable, generalizable stereo depth models ."
  },
  {
    "Model": null,
    "Model Unique Name": "Towards Universal Soccer Video Understanding",
    "Category": "video understanding",
    "Detailed Category": null,
    "Dataset": "SoccerReplay-1988, SoccerNet-v2, SoccerNet",
    "Paper": "https://arxiv.org/pdf/2412.01820",
    "GitHub": "https://github.com/jyrao/UniSoccer",
    "HuggingFace": null,
    "Summary": "This work introduces SoccerReplay-1988, the largest multi-modal soccer dataset comprising nearly 2,000 full matches with rich annotations—including timestamped events and match commentary—generated via an automated pipeline. The authors also present MatchVision, the first soccer-specific visual-language foundation model that extends visual backbones with spatiotemporal attention to excel across tasks like event classification, commentary generation, and multi-view foul recognition. MatchVision achieves state-of-the-art performance on both existing benchmarks and newly introduced evaluation tasks, demonstrating the effectiveness of large-scale data collection and unified modeling for comprehensive sports video understanding ."
  },
  {
    "Model": null,
    "Model Unique Name": "Magma: A Foundation Model for Multimodal AI Agents",
    "Category": "visual agents",
    "Detailed Category": null,
    "Dataset": "SeeClick, Vision2UI, Ego4D, EpicKitchen, Something‑Something v2, Open‑X‑Embodiment",
    "Paper": "https://arxiv.org/pdf/2502.13130",
    "GitHub": "https://github.com/microsoft/Magma",
    "HuggingFace": null,
    "Summary": "Magma is a unified multimodal AI foundation model that bridges verbal intelligence (understanding visual-language inputs) and spatial-temporal intelligence (planning and executing actions in digital and physical environments). Through joint pretraining on heterogeneous datasets—including labeled UIs with actionable “Set-of-Mark” annotations and videos with “Trace-of-Mark” motion traces—it learns to ground actions and plan multi-step sequences across tasks like GUI navigation and robotic manipulation. Magma achieves state-of-the-art performance on both specialized agentic benchmarks and general vision-language tasks, despite not using larger datasets, and offers unified, zero-shot adaptability across modalities. Code and model checkpoints are publicly available."
  },
  {
    "Model": null,
    "Model Unique Name": "Semantic-SAM: Segment and Recognize Anything at Any Granularity",
    "Category": "segmentation",
    "Detailed Category": null,
    "Dataset": "SA-1B, Objects365, COCO panoptic, ADE20k panoptic, PASCAL Part, PACO, PartImageNet",
    "Paper": "https://arxiv.org/pdf/2307.04767",
    "GitHub": "https://github.com/UX-Decoder/Semantic-SAM?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "Semantic‑SAM extends the Segment‑Anything paradigm by jointly learning semantic awareness and multi-granularity mask generation in an interactive model. It consolidates generic object, part, and SA‑1B segmentation datasets into a unified training pipeline with decoupled classifiers that distinguish both object-level and part-level semantics. By integrating a multi-choice learning strategy, the model produces multiple valid masks for each user click, spanning from entire objects down to fine-grained parts. This first attempt at combined training on SA‑1B, generic, and part datasets yields a model capable of interactive segmentation at any desired level of granularity, and training with SA‑1B further boosts its performance on panoptic and part segmentation tasks  ￼. The project is open-sourced with demos and full code available .\n"
  },
  {
    "Model": null,
    "Model Unique Name": "Segment Everything Everywhere All at Once",
    "Category": "segmentation",
    "Detailed Category": null,
    "Dataset": "MS COCO, DAVIS",
    "Paper": "https://arxiv.org/pdf/2304.06718",
    "GitHub": "https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "SEEM presents a promptable, interactive segmentation model that supports diverse user inputs—including points, boxes, scribbles, masks, text prompts, and even referring regions from other images—by encoding them into a unified visual-semantic space. It employs a Transformer encoder-decoder architecture with memory prompts to retain segmentation history, enabling efficient multi-round interaction without re-running the image encoder. Designed for versatility, compositionality, interactivity, and open-vocabulary segmentation, SEEM delivers competitive performance across nine segmentation tasks—interactive, generic, referring, and video segmentation—often with minimal supervision (as little as 1%)  ￼. Its model-agnostic design and compositional prompting allow zero-shot generalization to novel combinations of prompts and modalities."
  },
  {
    "Model": null,
    "Model Unique Name": "SAM 2: Segment Anything in Images and Videos",
    "Category": "segmentation",
    "Detailed Category": null,
    "Dataset": "SA-1B, SA-V",
    "Paper": "https://arxiv.org/pdf/2408.00714",
    "GitHub": "https://github.com/facebookresearch/sam2?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "SAM 2 is a unified transformer-based foundation model that enables promptable segmentation for both images and videos by introducing streaming memory to carry prompt context across frames. The authors also developed a data engine to create SA‑V, the largest video segmentation dataset to date, collecting 35.5 million masks across 50,900 videos. Compared to its predecessor, SAM 2 achieves around 3× fewer user interactions for video segmentation and is 6× faster on image tasks, while maintaining or improving accuracy. It offers state-of-the-art performance in interactive video segmentation benchmarks and strong zero-shot generalization across 17 video and 37 image datasets. All model weights, dataset, and code are publicly released  ￼."
  },
  {
    "Model": null,
    "Model Unique Name": "Grounding DINO",
    "Category": "Zero-Shot Object Detection",
    "Detailed Category": null,
    "Dataset": "OpenImages v5 subset, Localized Narratives (OpenImages), LVIS subset, RefCOCO, RefCOCO+, RefCOCOg, gRefCOCO, COCO, Objects365 v1 & v2, V3Det, GRIT, GQA, Flickr30K Entities, ODinW13/35",
    "Paper": "https://arxiv.org/pdf/2303.05499",
    "GitHub": "https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "Grounding DINO integrates language understanding into the transformer-based DINO object detector, enabling detection of any text-specified concept—be it category names or complex referring expressions—without additional model modifications. It introduces a tight cross-modal fusion design with language-aware feature enhancements, language-guided query selection, and a cross-modality decoder, and is pre-trained across detection, grounding, and caption datasets. The model achieves state-of-the-art zero-shot open-vocabulary performance: 52.5 AP on COCO and 26.1 AP on the ODinW benchmark without seeing COCO labels during pre-training. It also demonstrates strong referring-expression comprehension and supports diverse downstream applications like grounding-aware image editing ."
  },
  {
    "Model": null,
    "Model Unique Name": "One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer",
    "Category": "3D Reconstruction",
    "Detailed Category": null,
    "Dataset": "COCO-WholeBody, MPII, Human3.6M, UBody, AGORA, EHF",
    "Paper": "https://arxiv.org/pdf/2303.16160",
    "GitHub": "https://github.com/IDEA-Research/OSX",
    "HuggingFace": null,
    "Summary": "OSX introduces a single-stage transformer-based pipeline that recovers detailed whole-body 3D meshes—including body, hands, and face—from a single image, without separate networks or manual post-processing. The model uses a Component-Aware Transformer, where a global body encoder provides a shared feature map for local face and hand decoders that apply feature-level upsample-crop and keypoint-guided deformable attention. This joint design preserves inter-part coherence, avoids noticeable artifacts in skeletal structure, and achieves top performance (first place) on the AGORA SMPL-X benchmark. The authors also contribute a new UBody dataset containing high-quality 2D and 3D annotations for partially visible upper-body images collected “in the wild.”"
  },
  {
    "Model": null,
    "Model Unique Name": "Recognize Anything Model",
    "Category": "Image Tagging",
    "Detailed Category": null,
    "Dataset": "COCO, Visual Genome, Conceptual Captions, SBU Captions, Conceptual 12M",
    "Paper": "https://arxiv.org/pdf/2306.03514",
    "GitHub": "https://github.com/OPPOMKLab/recognize-anything?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "Recognize Anything (RAM) is a foundation-level model for general-purpose image tagging that achieves strong zero-shot performance by training on 14 million image–text pairs without manual labels. RAM’s pipeline uses automatic text semantic parsing to generate large-scale tag annotations, a unified model architecture that combines captioning and tagging, and a data engine that refines tags through expansion and cleaning. The model outperforms CLIP, BLIP, and even fully supervised tagging models—including commercial APIs—across multiple benchmarks, recognizing over 6,400 common categories, and is released with open-source weights and code ."
  },
  {
    "Model": null,
    "Model Unique Name": "VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking",
    "Category": "3D detection & tracking",
    "Detailed Category": null,
    "Dataset": "nuScenes, Waymo, Argoverse2, KITTI",
    "Paper": "https://arxiv.org/pdf/2303.11301",
    "GitHub": "https://github.com/dvlab-research/VoxelNeXt?tab=readme-ov-file",
    "HuggingFace": null,
    "Summary": "VoxelNeXt introduces a clean, fully sparse 3D object detection and tracking framework based entirely on sparse voxel features—eliminating anchors, center proxies, sparse-to-dense conversion, and NMS. Built on sparse convolutional networks, it achieves top-tier speed–accuracy trade-offs on benchmarks like nuScenes, Waymo, and Argoverse2, and outperforms all previous LiDAR-based methods in nuScenes tracking—it even set the leaderboard’s best result  ￼."
  }
]